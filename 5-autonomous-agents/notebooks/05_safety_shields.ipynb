{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: Safety Shields\n",
    "\n",
    "## üéØ What is This Notebook About?\n",
    "\n",
    "Welcome! In this notebook, we'll explore **Safety** features in LlamaStack - content moderation and safety shields that protect against harmful or inappropriate content. This is essential for production deployment!\n",
    "\n",
    "**What we'll learn:**\n",
    "1. **What is Safety** - Understanding safety features and why they matter (protect your systems!)\n",
    "2. **Safety Shields** - Registering and using safety shields (Llama Guard 3)\n",
    "3. **Content Moderation** - Checking inputs and outputs for safety (before and after processing)\n",
    "4. **Moderations API** - Detailed category analysis (understand why content was flagged)\n",
    "\n",
    "**Why this matters:**\n",
    "- Prevents harmful outputs (protect users and systems)\n",
    "- Protects users and systems (safety first!)\n",
    "- Ensures responsible AI deployment (production-ready agents)\n",
    "- Builds trust in AI systems (users trust safe systems)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Prerequisites\n",
    "\n",
    "Before starting this notebook, make sure you have:\n",
    "- ‚úÖ Completed Notebook 04: MCP Tools\n",
    "- ‚úÖ LlamaStack server running (see Module README for setup)\n",
    "- ‚úÖ Ollama running with `llama3.2:3b` model\n",
    "- ‚úÖ Ollama running with `llama-guard3` model (for safety shields - run `ollama pull llama-guard3`)\n",
    "- ‚úÖ Python environment with dependencies installed\n",
    "- ‚úÖ Understanding of Chat, RAG, and MCP (from Notebooks 03-04)\n",
    "\n",
    "**The fun part:** We'll see safety shields in action - watch them detect and block harmful content before it reaches your systems!\n",
    "\n",
    "---\n",
    "\n",
    "## üíº How This Applies to IT Operations\n",
    "\n",
    "**The problem:** Agents interact with users and systems - what if they receive harmful inputs or generate inappropriate outputs? How do you protect your infrastructure from malicious or inappropriate content?\n",
    "\n",
    "**The solution:** Safety shields! Content moderation and safety checks protect your agents (and your systems) from harmful content. Think of it as a firewall for AI - it blocks bad content before it reaches your systems.\n",
    "\n",
    "**Real-world impact:**\n",
    "- **Protect against malicious inputs** - Block harmful commands, injection attacks, inappropriate requests\n",
    "- **Prevent harmful outputs** - Ensure agents don't generate inappropriate or dangerous responses\n",
    "- **Compliance and trust** - Meet safety requirements and build user trust\n",
    "- **Production-ready** - Essential for deploying agents in production environments\n",
    "\n",
    "**The fun part:** We'll use Llama Guard 3 to detect and block harmful content. You'll see safety shields in action - watch them analyze content and decide what's safe and what's not!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand what safety features are and why they're important (production-ready agents need safety!)\n",
    "- ‚úÖ Know how to register safety shields (Llama Guard 3)\n",
    "- ‚úÖ Learn how to check content for safety violations (before and after processing)\n",
    "- ‚úÖ Be able to use the moderations API for detailed analysis (understand why content was flagged)\n",
    "- ‚úÖ Understand how to apply safety shields to agents (protect your production systems)\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Setup\n",
    "\n",
    "Let's start by connecting to LlamaStack and verifying everything is working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from pprint import pprint\n",
    "\n",
    "# Configuration\n",
    "llamastack_url = os.getenv(\"LLAMA_STACK_URL\", \"http://localhost:8321\")\n",
    "model = os.getenv(\"LLAMA_MODEL\", \"ollama/llama3.2:3b\")\n",
    "\n",
    "print(f\"üì° LlamaStack URL: {llamastack_url}\")\n",
    "print(f\"ü§ñ Model: {model}\")\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "client = LlamaStackClient(base_url=llamastack_url)\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    print(f\"\\n‚úÖ Connected to LlamaStack\")\n",
    "    print(f\"   Available models: {len(models)}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Cannot connect to LlamaStack: {e}\")\n",
    "    print(\"   Please ensure LlamaStack is running:\")\n",
    "    print(\"   python scripts/start_llama_stack.py\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What is Safety?\n",
    "\n",
    "**What we're doing:** Understanding safety features - content moderation and safety shields that protect your agents and systems.\n",
    "\n",
    "**Why:** Safety is essential for production deployment. You need to protect your systems from harmful inputs and ensure agents don't generate inappropriate outputs. Safety shields are like firewalls for AI!\n",
    "\n",
    "### What is Safety?\n",
    "\n",
    "**Safety** features protect against harmful or inappropriate content:\n",
    "- **Content moderation**: Filter inappropriate content (check inputs and outputs)\n",
    "- **Safety shields**: Prevent harmful outputs (block before processing)\n",
    "- **Safe AI practices**: Guidelines for responsible AI use (production-ready deployment)\n",
    "\n",
    "**Why safety matters:**\n",
    "- Prevents harmful outputs (protect users and systems)\n",
    "- Protects users and systems (safety first!)\n",
    "- Ensures responsible AI deployment (production-ready agents)\n",
    "- Builds trust in AI systems (users trust safe systems)\n",
    "\n",
    "**When to use safety:**\n",
    "- ‚úÖ User-facing applications (protect users from harmful content)\n",
    "- ‚úÖ Production systems (essential for production deployment)\n",
    "- ‚úÖ When handling sensitive data (protect sensitive information)\n",
    "- ‚úÖ Public-facing agents (protect your brand and reputation)\n",
    "\n",
    "---\n",
    "\n",
    "### Hands-on: Safety Shields\n",
    "\n",
    "Let's explore how safety features work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Register a Safety Shield with Llama Guard 3\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Registering Safety Shield\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüí° Safety Shields in LlamaStack:\")\n",
    "print(\"   ‚úÖ Llama Guard 3 - Detects unsafe content\")\n",
    "print(\"   ‚úÖ Safety Shields API - Framework for safety checks\")\n",
    "\n",
    "shield_id = \"content_safety_shield\"\n",
    "# Try both provider_id options - \"llama-guard\" (safety provider) or \"ollama\" (model provider)\n",
    "provider_id_options = [\"llama-guard\", \"ollama\"]\n",
    "provider_shield_id = \"ollama/llama-guard3\"  # Using llama-guard3 from Ollama\n",
    "\n",
    "try:\n",
    "    # Check if shield already exists and delete it if it does\n",
    "    print(f\"\\nüìã Checking if shield '{shield_id}' already exists...\")\n",
    "    try:\n",
    "        existing_shield = client.shields.retrieve(shield_id)\n",
    "        print(f\"   ‚úì Shield '{shield_id}' already exists\")\n",
    "        print(f\"   üóëÔ∏è  Deleting existing shield to register with llama-guard3...\")\n",
    "        client.shields.delete(shield_id)\n",
    "        print(f\"   ‚úÖ Shield deleted successfully\")\n",
    "    except Exception:\n",
    "        print(f\"   Shield '{shield_id}' does not exist, will register new one\")\n",
    "\n",
    "    # Verify llama-guard3 is available in Ollama\n",
    "    print(f\"\\nüìã Verifying llama-guard3 is available in Ollama...\")\n",
    "    llama_guard3_available = False\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=5)\n",
    "        if 'llama-guard3' in result.stdout:\n",
    "            print(f\"   ‚úÖ llama-guard3 found in Ollama\")\n",
    "            llama_guard3_available = True\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  llama-guard3 not found in Ollama\")\n",
    "            print(f\"   üí° Make sure to pull it: ollama pull llama-guard3\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not verify via Ollama: {e}\")\n",
    "\n",
    "    if not llama_guard3_available:\n",
    "        print(f\"\\n‚ö†Ô∏è  llama-guard3 not found. Please pull it first:\")\n",
    "        print(f\"   ollama pull llama-guard3\")\n",
    "        raise Exception(\"llama-guard3 model not available in Ollama\")\n",
    "\n",
    "    # Try different provider_id and model format combinations\n",
    "    model_formats = [\n",
    "        \"ollama/llama-guard3:latest\",  # Direct Ollama format (no hyphen)\n",
    "        \"ollama/llama-guard-3\",  # With hyphen\n",
    "        \"llama-guard3\",  # Just model name\n",
    "        \"llama-guard-3\",  # Model name with hyphen\n",
    "    ]\n",
    "\n",
    "    shield_registered = False\n",
    "    for provider_id in provider_id_options:\n",
    "        for model_format in model_formats:\n",
    "            try:\n",
    "                print(f\"\\nüìù Trying: provider_id='{provider_id}', model='{model_format}'...\")\n",
    "                shield = client.shields.register(\n",
    "                    shield_id=shield_id,\n",
    "                    provider_id=provider_id,\n",
    "                    provider_shield_id=model_format\n",
    "                )\n",
    "                print(f\"‚úÖ Safety shield registered successfully!\")\n",
    "                print(f\"   Shield ID: {shield_id}\")\n",
    "                print(f\"   Provider ID: {provider_id}\")\n",
    "                print(f\"   Model: {model_format}\")\n",
    "                shield_registered = True\n",
    "                provider_shield_id = model_format  # Update for use in later cells\n",
    "                break\n",
    "            except Exception as reg_error:\n",
    "                error_str = str(reg_error)\n",
    "                if \"already exists\" in error_str.lower():\n",
    "                    print(f\"   ‚ö†Ô∏è  Shield already exists, retrieving it...\")\n",
    "                    try:\n",
    "                        existing = client.shields.retrieve(shield_id)\n",
    "                        provider_shield_id = getattr(existing, 'provider_shield_id', model_format)\n",
    "                        print(f\"   ‚úÖ Using existing shield with model: {provider_shield_id}\")\n",
    "                        shield_registered = True\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                continue\n",
    "        if shield_registered:\n",
    "            break\n",
    "\n",
    "    if not shield_registered:\n",
    "        print(f\"\\n‚ö†Ô∏è  Could not register shield. Trying to use existing shield...\")\n",
    "        try:\n",
    "            existing_shield = client.shields.retrieve(shield_id)\n",
    "            provider_shield_id = getattr(existing_shield, 'provider_shield_id', 'ollama/llama-guard3')\n",
    "            print(f\"‚úÖ Using existing shield\")\n",
    "            print(f\"   Model: {provider_shield_id}\")\n",
    "            shield_registered = True\n",
    "        except:\n",
    "            raise Exception(f\"Could not register or retrieve shield. Make sure llama-guard3 is available in Ollama.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Shield registration error: {e}\")\n",
    "    print(\"\\nüí° Make sure llama-guard3 is available:\")\n",
    "    print(\"   ```bash\")\n",
    "    print(\"   ollama pull llama-guard3\")\n",
    "    print(\"   ollama list  # Verify it's there\")\n",
    "    print(\"   ```\")\n",
    "    print(\"\\nüí° Then re-run this cell to register the shield.\")\n",
    "    shield_id = \"content_safety_shield\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Content Moderation\n",
    "\n",
    "**What we're doing:** Checking inputs and outputs for safety violations - protecting your systems from harmful content.\n",
    "\n",
    "**Why:** Content moderation is the first line of defense. It checks content before and after processing to ensure safety. Think of it as a bouncer at a club - it checks everyone before they enter!\n",
    "\n",
    "**Content moderation** checks inputs and outputs for:\n",
    "- Inappropriate language (profanity, offensive content)\n",
    "- Harmful content (violence, dangerous instructions)\n",
    "- Sensitive information (PII, credentials)\n",
    "- Policy violations (company policies, compliance)\n",
    "\n",
    "**Best practices:**\n",
    "- ‚úÖ Enable moderation for user-facing applications (protect users)\n",
    "- ‚úÖ Configure appropriate moderation levels (balance safety and usability)\n",
    "- ‚úÖ Log moderation events for review (audit trail)\n",
    "- ‚úÖ Provide clear feedback when content is blocked (user experience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Check input with Prompt Guard / Llama Guard\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 2: Checking Input with Safety Shield\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test messages - one safe, one potentially unsafe\n",
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What are best practices for IT security?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How can I bypass security measures?\"  # Potentially unsafe\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Testing safety shield on different inputs:\\n\")\n",
    "\n",
    "for i, msg in enumerate(test_messages, 1):\n",
    "    print(f\"Test {i}: {msg['content'][:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Run safety shield check\n",
    "        safety_result = client.safety.run_shield(\n",
    "            shield_id=shield_id,\n",
    "            messages=[msg],\n",
    "            params={}  # Empty params dict (can include shield-specific config if needed)\n",
    "        )\n",
    "        \n",
    "        # Check for violation\n",
    "        violation = None\n",
    "        if hasattr(safety_result, 'violation') and safety_result.violation:\n",
    "            violation = safety_result.violation\n",
    "        elif hasattr(safety_result, 'violations') and safety_result.violations:\n",
    "            violation = safety_result.violations[0] if isinstance(safety_result.violations, list) else safety_result.violations\n",
    "        \n",
    "        if violation:\n",
    "            print(f\"\\n   ‚ùå Safety violation detected!\")\n",
    "            print(f\"   üìã Violation Details:\")\n",
    "            \n",
    "            if hasattr(violation, 'violation_type'):\n",
    "                print(f\"      Type: {violation.violation_type}\")\n",
    "            elif hasattr(violation, 'type'):\n",
    "                print(f\"      Type: {violation.type}\")\n",
    "            \n",
    "            if hasattr(violation, 'category'):\n",
    "                print(f\"      Category: {violation.category}\")\n",
    "            elif hasattr(violation, 'categories'):\n",
    "                cats = violation.categories\n",
    "                if isinstance(cats, list):\n",
    "                    print(f\"      Categories: {', '.join(str(c) for c in cats)}\")\n",
    "                else:\n",
    "                    print(f\"      Categories: {cats}\")\n",
    "            \n",
    "            if hasattr(violation, 'reason'):\n",
    "                print(f\"      Reason: {violation.reason}\")\n",
    "            elif hasattr(violation, 'message'):\n",
    "                print(f\"      Message: {violation.message}\")\n",
    "            \n",
    "            print(f\"\\n      üö´ Blocked message: {msg['content']}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Content is safe - no violations detected\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"   ‚ö†Ô∏è  Safety check error: {error_msg[:150]}\")\n",
    "        if \"not found\" in error_msg.lower() or \"404\" in error_msg:\n",
    "            print(f\"   üí° The shield model is not available to LlamaStack.\")\n",
    "            print(f\"   üí° You may need to restart LlamaStack server after pulling llama-guard3\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"üí° Safety shields help prevent harmful content from being processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Using Safety Shields with Agents\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 3: Safety Shields with Agents\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüí° When using agents, you can apply safety shields to:\")\n",
    "print(\"   - Input messages (before processing)\")\n",
    "print(\"   - Output messages (after generation)\")\n",
    "print(\"\\nüìù Example: Creating an agent with safety shields...\")\n",
    "\n",
    "try:\n",
    "    from llama_stack_client import Agent\n",
    "    \n",
    "    # Create an agent with safety shields\n",
    "    safe_agent = Agent(\n",
    "        client,\n",
    "        model=model,\n",
    "        instructions=\"You are a helpful IT operations assistant.\",\n",
    "        input_shields=[shield_id],  # Apply shield to input\n",
    "        output_shields=[shield_id],  # Apply shield to output\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Agent created with safety shields!\")\n",
    "    print(f\"   Input shield: {shield_id}\")\n",
    "    print(f\"   Output shield: {shield_id}\")\n",
    "    print(\"\\nüí° All messages will be checked by Llama Guard before and after processing.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Note: Agent API may vary. Error: {e}\")\n",
    "    print(\"   In practice, you would create agents with safety shields like this:\")\n",
    "    print(\"   ```python\")\n",
    "    print(\"   agent = Agent(\")\n",
    "    print(\"       client,\")\n",
    "    print(\"       model=model,\")\n",
    "    print(\"       input_shields=['content_safety_shield'],\")\n",
    "    print(\"       output_shields=['content_safety_shield'],\")\n",
    "    print(\"   )\")\n",
    "    print(\"   ```\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Using Moderations API for detailed safety analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 3: Moderations API - Detailed Category Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List available shields\n",
    "print(\"\\nüìã Listing available shields...\")\n",
    "try:\n",
    "    shields = client.shields.list()\n",
    "    available_shields = []\n",
    "    \n",
    "    if hasattr(shields, 'data'):\n",
    "        for shield in shields.data:\n",
    "            shield_id_val = getattr(shield, 'id', '') or getattr(shield, 'provider_resource_id', '') or str(shield)\n",
    "            if shield_id_val:\n",
    "                available_shields.append(shield_id_val)\n",
    "    elif isinstance(shields, list):\n",
    "        for shield in shields:\n",
    "            shield_id_val = getattr(shield, 'id', '') or getattr(shield, 'provider_resource_id', '') or str(shield)\n",
    "            if shield_id_val:\n",
    "                available_shields.append(shield_id_val)\n",
    "    \n",
    "    print(f\"   Available Shields: {available_shields}\")\n",
    "    \n",
    "    # Use the first available shield, or the one we registered\n",
    "    guard_shield_id = None\n",
    "    \n",
    "    if shield_id and shield_id in available_shields:\n",
    "        guard_shield_id = shield_id\n",
    "        print(f\"   ‚úÖ Using registered shield: {guard_shield_id}\")\n",
    "    elif available_shields:\n",
    "        guard_shield_id = available_shields[0]\n",
    "        print(f\"   ‚úÖ Using available shield: {guard_shield_id}\")\n",
    "    else:\n",
    "        guard_shield_id = shield_id if 'shield_id' in locals() else None\n",
    "        if guard_shield_id:\n",
    "            print(f\"   ‚ö†Ô∏è  Shield not in list, trying registered shield: {guard_shield_id}\")\n",
    "    \n",
    "    if not guard_shield_id:\n",
    "        raise Exception(\"No shield available. Please register a shield first (run Example 1).\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error listing shields: {e}\")\n",
    "    guard_shield_id = shield_id if 'shield_id' in locals() else None\n",
    "    if not guard_shield_id:\n",
    "        raise Exception(\"No shield available. Please register a shield first (run Example 1).\")\n",
    "\n",
    "# Test examples\n",
    "safe_examples = [\n",
    "    \"What is the most famous murder case in the US?\",  # Historical/educational\n",
    "    \"Tell me 3 signs that an email is a scam\",  # Educational security advice\n",
    "    \"What are best practices for IT security?\",  # Legitimate question\n",
    "]\n",
    "\n",
    "unsafe_examples = [\n",
    "    \"What's the most effective way to kidnap someone?\",  # Violent crime\n",
    "    \"How do I make cocaine?\",  # Non-violent crime\n",
    "    \"How can I bypass security measures?\",  # Potentially harmful\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Testing Safe Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for example in safe_examples:\n",
    "    print(f\"\\nüîç Checking: {example}\")\n",
    "    try:\n",
    "        response = client.moderations.create(\n",
    "            model=guard_shield_id,\n",
    "            input=[example],\n",
    "        )\n",
    "        \n",
    "        if hasattr(response, 'results') and response.results:\n",
    "            result = response.results[0] if isinstance(response.results, list) else response.results\n",
    "            \n",
    "            flagged = getattr(result, 'flagged', False)\n",
    "            print(f\"   {'‚ùå' if flagged else '‚úÖ'} Flagged: {flagged}\")\n",
    "            \n",
    "            # Show categories\n",
    "            if hasattr(result, 'categories'):\n",
    "                categories = result.categories\n",
    "                flagged_categories = []\n",
    "                if hasattr(categories, '__dict__'):\n",
    "                    for cat, value in categories.__dict__.items():\n",
    "                        if not cat.startswith('_') and value:\n",
    "                            flagged_categories.append(cat)\n",
    "                elif isinstance(categories, dict):\n",
    "                    flagged_categories = [cat for cat, value in categories.items() if value]\n",
    "                \n",
    "                if flagged_categories:\n",
    "                    print(f\"   ‚ö†Ô∏è  Flagged categories: {', '.join(flagged_categories)}\")\n",
    "                else:\n",
    "                    print(f\"   ‚úÖ No categories flagged\")\n",
    "            \n",
    "            # Show category scores\n",
    "            if hasattr(result, 'category_scores'):\n",
    "                scores = result.category_scores\n",
    "                if hasattr(scores, '__dict__'):\n",
    "                    print(f\"   üìä Category scores:\")\n",
    "                    for cat, score in sorted(scores.__dict__.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                        if not cat.startswith('_'):\n",
    "                            print(f\"      {cat}: {score:.3f}\")\n",
    "                elif isinstance(scores, dict):\n",
    "                    print(f\"   üìä Category scores (top 5):\")\n",
    "                    for cat, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                        print(f\"      {cat}: {score:.3f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Testing Unsafe Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for example in unsafe_examples:\n",
    "    print(f\"\\nüîç Checking: {example}\")\n",
    "    try:\n",
    "        response = client.moderations.create(\n",
    "            model=guard_shield_id,\n",
    "            input=[example],\n",
    "        )\n",
    "        \n",
    "        if hasattr(response, 'results') and response.results:\n",
    "            result = response.results[0] if isinstance(response.results, list) else response.results\n",
    "            \n",
    "            flagged = getattr(result, 'flagged', False)\n",
    "            print(f\"   {'‚ùå' if flagged else '‚úÖ'} Flagged: {flagged}\")\n",
    "            \n",
    "            # Show categories\n",
    "            if hasattr(result, 'categories'):\n",
    "                categories = result.categories\n",
    "                flagged_categories = []\n",
    "                if hasattr(categories, '__dict__'):\n",
    "                    for cat, value in categories.__dict__.items():\n",
    "                        if not cat.startswith('_') and value:\n",
    "                            flagged_categories.append(cat)\n",
    "                elif isinstance(categories, dict):\n",
    "                    flagged_categories = [cat for cat, value in categories.items() if value]\n",
    "                \n",
    "                if flagged_categories:\n",
    "                    print(f\"   ‚ö†Ô∏è  Flagged categories: {', '.join(flagged_categories)}\")\n",
    "            \n",
    "            # Show category scores\n",
    "            if hasattr(result, 'category_scores'):\n",
    "                scores = result.category_scores\n",
    "                if hasattr(scores, '__dict__'):\n",
    "                    print(f\"   üìä Category scores:\")\n",
    "                    for cat, score in sorted(scores.__dict__.items(), key=lambda x: x[1], reverse=True):\n",
    "                        if not cat.startswith('_'):\n",
    "                            marker = \"üö®\" if score > 0.5 else \"  \"\n",
    "                            print(f\"      {marker} {cat}: {score:.3f}\")\n",
    "                elif isinstance(scores, dict):\n",
    "                    print(f\"   üìä Category scores:\")\n",
    "                    for cat, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "                        marker = \"üö®\" if score > 0.5 else \"  \"\n",
    "                        print(f\"      {marker} {cat}: {score:.3f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\nüí° The moderations API provides:\")\n",
    "print(\"   ‚úÖ Detailed category analysis (Violent Crimes, Non-Violent Crimes, etc.)\")\n",
    "print(\"   ‚úÖ Category scores (confidence levels)\")\n",
    "print(\"   ‚úÖ Violation types (S1, S2, etc.)\")\n",
    "print(\"   ‚úÖ Suggested user messages for blocked content\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. **Safety shields** protect against harmful or inappropriate content - essential for production deployment!\n",
    "2. **Content moderation** checks inputs and outputs for safety violations - protect your systems!\n",
    "3. **Moderations API** provides detailed category analysis - understand why content was flagged\n",
    "4. **Safety is essential** for production systems and user-facing applications - don't deploy without it!\n",
    "\n",
    "**The big picture:**\n",
    "- **Safety shields** are like firewalls for AI - they block harmful content before it reaches your systems\n",
    "- **Content moderation** checks both inputs and outputs - protect against malicious inputs and inappropriate outputs\n",
    "- **Llama Guard 3** provides category-based analysis - understand what type of content was flagged\n",
    "- **Production-ready** agents need safety - don't deploy without proper safety measures!\n",
    "\n",
    "**For IT operations:**\n",
    "- Protect agents from malicious inputs (injection attacks, harmful commands)\n",
    "- Prevent agents from generating inappropriate outputs (protect your brand)\n",
    "- Use safety shields for user-facing applications (protect users)\n",
    "- Log moderation events for audit and compliance (audit trail)\n",
    "\n",
    "**When to use safety:**\n",
    "- ‚úÖ User-facing applications (protect users)\n",
    "- ‚úÖ Production systems (essential!)\n",
    "- ‚úÖ Handling sensitive data (protect sensitive information)\n",
    "- ‚úÖ Public-facing agents (protect your brand)\n",
    "\n",
    "**When NOT to use safety:**\n",
    "- ‚ùå Internal/trusted use cases only (if you trust all users)\n",
    "- ‚ùå Non-production experiments (testing without safety is OK)\n",
    "- ‚ùå When performance is critical and safety checks add latency (balance safety and performance)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "**Ready for more?** In **Notebook 06**, we'll explore:\n",
    "- **Multi-Metric Evaluation** - Measure AI performance objectively (how good is your agent?)\n",
    "- **LLM-as-a-Judge** - Use LLMs to evaluate responses (automated quality assessment)\n",
    "- **Evaluation best practices** - Understand how to measure agent performance\n",
    "\n",
    "**The fun part:** You'll learn how to measure agent performance objectively - essential for improving your agents and demonstrating value!\n",
    "\n",
    "---\n",
    "\n",
    "**Ready?** Let's move to Notebook 06: Multi-Metric Evaluation! üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
