{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: LlamaStack Core Features\n",
    "\n",
    "## üéØ What is This Notebook About?\n",
    "\n",
    "Welcome to Notebook 03! In this notebook, we'll explore **LlamaStack's core capabilities** - the building blocks that make powerful agents possible.\n",
    "\n",
    "**What we'll learn:**\n",
    "1. **Simple Chat** - Basic LLM interactions\n",
    "2. **RAG (Retrieval Augmented Generation)** - Enhancing LLMs with external knowledge\n",
    "3. **MCP (Model Context Protocol)** - External tool integration\n",
    "4. **Safety** - Content moderation and safety shields\n",
    "5. **Evaluation** - Measuring AI performance\n",
    "\n",
    "**Why this matters:**\n",
    "- Understanding these features helps you build better agents\n",
    "- Each feature solves a specific problem\n",
    "- Combining features creates powerful solutions\n",
    "- This knowledge prepares you for advanced agent development\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand LlamaStack's core capabilities\n",
    "- ‚úÖ Know when to use each feature\n",
    "- ‚úÖ See how features work independently\n",
    "- ‚úÖ Be ready to combine features in agents (Notebook 04)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Prerequisites\n",
    "\n",
    "- LlamaStack server running (see Module README)\n",
    "- Ollama running with llama3.2:3b model\n",
    "- Python environment with dependencies installed\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Setup\n",
    "\n",
    "Let's start by connecting to LlamaStack and verifying everything is working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "# Configuration\n",
    "llamastack_url = os.getenv(\"LLAMA_STACK_URL\", \"http://localhost:8321\")\n",
    "model = os.getenv(\"LLAMA_MODEL\", \"ollama/llama3.2:3b\")\n",
    "\n",
    "print(f\"üì° LlamaStack URL: {llamastack_url}\")\n",
    "print(f\"ü§ñ Model: {model}\")\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "client = LlamaStackClient(base_url=llamastack_url)\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    print(f\"\\n‚úÖ Connected to LlamaStack\")\n",
    "    print(f\"   Available models: {len(models)}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Cannot connect to LlamaStack: {e}\")\n",
    "    print(\"   Please ensure LlamaStack is running:\")\n",
    "    print(\"   python scripts/start_llama_stack.py\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Simple Chat\n",
    "\n",
    "### What is Chat?\n",
    "\n",
    "**Chat** is the most basic way to interact with an LLM. It's a conversation where you send messages and receive responses.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Message Types**: System (instructions), User (questions), Assistant (responses)\n",
    "- **Streaming vs Non-streaming**: Get responses as they're generated or wait for complete response\n",
    "- **Conversation Context**: LLM remembers previous messages in the conversation\n",
    "\n",
    "**When to use Chat:**\n",
    "- Simple Q&A\n",
    "- Text generation\n",
    "- Basic reasoning tasks\n",
    "- When you don't need external knowledge or tools\n",
    "\n",
    "---\n",
    "\n",
    "### Hands-on: Basic Chat Completion\n",
    "\n",
    "Let's start with the simplest example - a single question and answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic chat completion\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Basic Chat Completion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is artificial intelligence in one sentence?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract and display the response\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"\\nüìù Question: What is artificial intelligence in one sentence?\")\n",
    "print(f\"\\nü§ñ Answer:\\n{answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompts\n",
    "\n",
    "**System prompts** are instructions that guide the LLM's behavior. They set the \"personality\" and \"role\" of the assistant.\n",
    "\n",
    "**Why use system prompts:**\n",
    "- Define the assistant's role (e.g., \"You are a helpful IT operations assistant\")\n",
    "- Set behavior guidelines\n",
    "- Provide context about the domain\n",
    "- Ensure consistent responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Chat with system prompt\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 2: Chat with System Prompt\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful IT operations assistant. You provide clear, concise answers about IT infrastructure and operations.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What should I check if a web server is not responding?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"\\nüìù Question: What should I check if a web server is not responding?\")\n",
    "print(f\"\\nü§ñ Answer (with IT operations context):\\n{answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-turn Conversations\n",
    "\n",
    "**Multi-turn conversations** maintain context across multiple exchanges. The LLM remembers previous messages in the conversation.\n",
    "\n",
    "**Why this matters:**\n",
    "- Natural conversation flow\n",
    "- Can refer back to earlier topics\n",
    "- Builds on previous context\n",
    "- More human-like interaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Multi-turn conversation\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 3: Multi-turn Conversation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First turn\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm setting up a new database server. What should I consider?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer1 = response1.choices[0].message.content\n",
    "print(f\"\\nüìù Turn 1 - Question: I'm setting up a new database server. What should I consider?\")\n",
    "print(f\"\\nü§ñ Answer:\\n{answer1[:200]}...\\n\")\n",
    "\n",
    "# Second turn - add previous messages to maintain context\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": answer1\n",
    "})\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What about security specifically?\"\n",
    "})\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer2 = response2.choices[0].message.content\n",
    "print(f\"\\nüìù Turn 2 - Question: What about security specifically?\")\n",
    "print(f\"   (Note: The assistant knows we're talking about database servers)\\n\")\n",
    "print(f\"ü§ñ Answer:\\n{answer2[:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Responses\n",
    "\n",
    "**Streaming** allows you to receive the response as it's being generated, token by token. This provides:\n",
    "- Faster perceived response time\n",
    "- Real-time feedback\n",
    "- Better user experience\n",
    "\n",
    "**When to use streaming:**\n",
    "- Long responses\n",
    "- Interactive applications\n",
    "- When you want immediate feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Streaming response\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 4: Streaming Response\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìù Question: Explain what RAG (Retrieval Augmented Generation) is.\\n\")\n",
    "print(\"ü§ñ Answer (streaming):\\n\")\n",
    "\n",
    "# Create streaming completion\n",
    "stream = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain what RAG (Retrieval Augmented Generation) is in 2-3 sentences.\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True  # Enable streaming\n",
    ")\n",
    "\n",
    "# Process stream chunk by chunk\n",
    "full_response = \"\"\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        print(content, end=\"\", flush=True)\n",
    "        full_response += content\n",
    "\n",
    "print(\"\\n\\n‚úÖ Streaming complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: RAG (Retrieval Augmented Generation)\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "**RAG** enhances LLMs with external knowledge by:\n",
    "1. **Storing documents** in a vector database (vector store)\n",
    "2. **Searching** for relevant context when answering questions\n",
    "3. **Augmenting** the LLM's prompt with retrieved context\n",
    "\n",
    "**Why RAG matters:**\n",
    "- LLMs have training data cutoff dates\n",
    "- Can't access private/internal documents\n",
    "- RAG provides up-to-date, domain-specific knowledge\n",
    "- Improves accuracy for specialized topics\n",
    "\n",
    "**When to use RAG:**\n",
    "- Need access to specific documents\n",
    "- Domain-specific knowledge required\n",
    "- Private/internal information\n",
    "- Up-to-date information needed\n",
    "\n",
    "---\n",
    "\n",
    "### Hands-on: Creating a Vector Store\n",
    "\n",
    "Let's create a vector store and add some IT operations documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Create a vector store\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Creating a Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample IT operations documentation\n",
    "it_docs = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"content\": \"To restart a web server, use: systemctl restart nginx. Check status with: systemctl status nginx.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"content\": \"High CPU usage troubleshooting: 1) Check top processes with 'top' or 'htop', 2) Identify CPU-intensive processes, 3) Check for runaway processes or infinite loops.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"content\": \"Database connection issues: Check firewall rules, verify credentials, ensure database service is running, check network connectivity with 'telnet hostname port'.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc4\",\n",
    "        \"content\": \"Disk space issues: Use 'df -h' to check disk usage, find large files with 'du -sh /*', clean logs with 'journalctl --vacuum-time=7d'.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc5\",\n",
    "        \"content\": \"Service monitoring: Use 'systemctl list-units --type=service' to list all services, 'systemctl is-active servicename' to check status, set up monitoring with Prometheus or Nagios.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\nüìö Sample IT Operations Documentation:\")\n",
    "for doc in it_docs:\n",
    "    print(f\"   - {doc['id']}: {doc['content'][:60]}...\")\n",
    "\n",
    "print(\"\\nüí° These documents will be stored in a vector store for retrieval.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store using LlamaStack\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Creating Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Create a vector store\n",
    "    vector_store = client.vector_stores.create(\n",
    "        name=\"it-operations-docs\",\n",
    "        description=\"IT operations documentation and troubleshooting guides\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Vector store created!\")\n",
    "    print(f\"   Name: {vector_store.name}\")\n",
    "    print(f\"   ID: {vector_store.id}\")\n",
    "    \n",
    "    # Add documents to the vector store\n",
    "    print(f\"\\nüìù Adding {len(it_docs)} documents to vector store...\")\n",
    "    \n",
    "    for doc in it_docs:\n",
    "        client.vector_stores.documents.create(\n",
    "            vector_store_id=vector_store.id,\n",
    "            content=doc[\"content\"],\n",
    "            metadata={\"doc_id\": doc[\"id\"]}\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ All documents added successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Note: Vector store API may vary. Error: {e}\")\n",
    "    print(\"   This is a demonstration of the concept.\")\n",
    "    print(\"   In practice, you would use the appropriate LlamaStack vector store API.\")\n",
    "    \n",
    "    # Store for later use\n",
    "    vector_store_id = \"demo_vector_store\"\n",
    "    print(f\"\\nüí° For this demo, we'll use a simulated vector store ID: {vector_store_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for Relevant Context\n",
    "\n",
    "Once documents are in the vector store, we can search for relevant context based on semantic similarity (meaning, not just keywords).\n",
    "\n",
    "**How it works:**\n",
    "1. Convert query to embedding (vector representation)\n",
    "2. Compare with document embeddings\n",
    "3. Return most similar documents\n",
    "4. Use retrieved documents as context for LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Search for relevant context\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 2: Searching Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate vector store search (in practice, use LlamaStack API)\n",
    "def search_vector_store(query, documents, top_k=2):\n",
    "    \"\"\"Simulate semantic search - in practice, this uses embeddings\"\"\"\n",
    "    # Simple keyword matching for demo (real RAG uses semantic similarity)\n",
    "    query_lower = query.lower()\n",
    "    results = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        score = sum(1 for word in query_lower.split() if word in doc[\"content\"].lower())\n",
    "        if score > 0:\n",
    "            results.append((doc, score))\n",
    "    \n",
    "    # Sort by score and return top_k\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in results[:top_k]]\n",
    "\n",
    "query = \"How do I restart a web server?\"\n",
    "print(f\"\\nüîç Query: {query}\\n\")\n",
    "\n",
    "# Search for relevant documents\n",
    "relevant_docs = search_vector_store(query, it_docs, top_k=2)\n",
    "\n",
    "print(\"üìö Retrieved Documents:\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"\\n   {i}. {doc['id']}:\")\n",
    "    print(f\"      {doc['content']}\")\n",
    "\n",
    "print(\"\\nüí° These documents will be used as context for the LLM.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Retrieved Context in Chat\n",
    "\n",
    "Now let's use the retrieved documents as context for the LLM. This is the \"Augmented Generation\" part of RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: RAG - Using retrieved context in chat\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 3: RAG - Chat with Retrieved Context\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"How do I restart a web server?\"\n",
    "relevant_docs = search_vector_store(query, it_docs, top_k=2)\n",
    "\n",
    "# Build context from retrieved documents\n",
    "context = \"\\n\\n\".join([f\"Document {doc['id']}: {doc['content']}\" for doc in relevant_docs])\n",
    "\n",
    "# Create prompt with context\n",
    "prompt = f\"\"\"Use the following IT operations documentation to answer the question.\n",
    "\n",
    "Documentation:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the documentation provided:\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Question: {query}\\n\")\n",
    "print(f\"üìö Context Retrieved:\\n{context}\\n\")\n",
    "\n",
    "# Get response with context\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful IT operations assistant. Answer questions based on the provided documentation.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"ü§ñ Answer (with RAG context):\\n{answer}\\n\")\n",
    "print(\"‚úÖ Notice how the answer uses the specific documentation provided!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: MCP (Model Context Protocol)\n",
    "\n",
    "### What is MCP?\n",
    "\n",
    "**MCP (Model Context Protocol)** is a protocol for integrating external tools and services with LLMs. It allows agents to:\n",
    "- **Call external APIs** (e.g., check service status, restart services)\n",
    "- **Access databases** (e.g., query incident logs)\n",
    "- **Execute commands** (e.g., run system commands)\n",
    "- **Integrate with other systems** (e.g., monitoring tools, ticketing systems)\n",
    "\n",
    "**Why MCP matters:**\n",
    "- LLMs can't directly interact with systems\n",
    "- MCP provides a standardized way to connect tools\n",
    "- Enables agents to take real actions\n",
    "- Makes agents more powerful and useful\n",
    "\n",
    "**When to use MCP:**\n",
    "- Need to interact with external systems\n",
    "- Want agents to take actions (not just answer questions)\n",
    "- Need real-time data from APIs\n",
    "- Want to integrate with existing tools\n",
    "\n",
    "---\n",
    "\n",
    "### Hands-on: Exploring Tool Runtime\n",
    "\n",
    "Let's explore what tools are available and how they work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: List available tools\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Exploring Tool Runtime\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # List available tool runtimes\n",
    "    tool_runtimes = client.tool_runtimes.list()\n",
    "    print(f\"\\n‚úÖ Found {len(tool_runtimes)} tool runtime(s)\")\n",
    "    \n",
    "    for runtime in tool_runtimes:\n",
    "        print(f\"\\n   Runtime: {runtime.name}\")\n",
    "        print(f\"   Type: {runtime.type}\")\n",
    "        \n",
    "        # List tools in this runtime\n",
    "        tools = client.tools.list(runtime_id=runtime.id)\n",
    "        print(f\"   Available tools: {len(tools)}\")\n",
    "        \n",
    "        for tool in tools[:5]:  # Show first 5 tools\n",
    "            print(f\"      - {tool.name}: {tool.description[:60]}...\")\n",
    "        \n",
    "        if len(tools) > 5:\n",
    "            print(f\"      ... and {len(tools) - 5} more\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Note: Tool runtime API may vary. Error: {e}\")\n",
    "    print(\"   This is a demonstration of the concept.\")\n",
    "    print(\"\\nüí° In practice, MCP tools allow agents to:\")\n",
    "    print(\"   - Call external APIs\")\n",
    "    print(\"   - Execute system commands\")\n",
    "    print(\"   - Query databases\")\n",
    "    print(\"   - Integrate with monitoring systems\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Tool Execution\n",
    "\n",
    "Tools are functions that agents can call. When an agent needs to perform an action, it:\n",
    "1. **Decides** which tool to use\n",
    "2. **Calls** the tool with appropriate parameters\n",
    "3. **Receives** the result\n",
    "4. **Uses** the result to continue reasoning\n",
    "\n",
    "**Tool Structure:**\n",
    "- **Name**: Identifies the tool\n",
    "- **Description**: Tells the LLM what the tool does\n",
    "- **Parameters**: What inputs the tool needs\n",
    "- **Returns**: What the tool outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Create a simple custom tool\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 2: Creating a Custom Tool\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define a simple tool function\n",
    "def check_service_status(service_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Check the status of a system service.\n",
    "    \n",
    "    Args:\n",
    "        service_name: Name of the service to check (e.g., 'nginx', 'mysql')\n",
    "    \n",
    "    Returns:\n",
    "        Status of the service: 'running', 'stopped', or 'not found'\n",
    "    \"\"\"\n",
    "    # Simulate service check (in practice, this would call systemctl or similar)\n",
    "    import random\n",
    "    statuses = ['running', 'stopped', 'not found']\n",
    "    status = random.choice(statuses)\n",
    "    \n",
    "    return f\"Service '{service_name}' is {status}.\"\n",
    "\n",
    "# Test the tool\n",
    "print(\"\\nüîß Custom Tool: check_service_status\")\n",
    "print(\"   Description: Check the status of a system service\")\n",
    "print(\"   Parameters: service_name (str)\")\n",
    "print(\"\\nüìù Testing tool:\")\n",
    "result = check_service_status(\"nginx\")\n",
    "print(f\"   check_service_status('nginx') ‚Üí {result}\")\n",
    "\n",
    "print(\"\\nüí° In Notebook 02, we saw how to use tools with agents.\")\n",
    "print(\"   Tools enable agents to take actions, not just answer questions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Integration Patterns\n",
    "\n",
    "**Common patterns for tool integration:**\n",
    "1. **Client-side tools**: Python functions that run in your process\n",
    "2. **Server-side tools**: Tools registered with LlamaStack server\n",
    "3. **MCP tools**: Tools accessed via Model Context Protocol\n",
    "4. **API tools**: Tools that call external REST APIs\n",
    "\n",
    "**Best practices:**\n",
    "- Provide clear descriptions so LLM knows when to use tools\n",
    "- Handle errors gracefully\n",
    "- Return structured data when possible\n",
    "- Log tool calls for debugging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Safety\n",
    "\n",
    "### What is Safety?\n",
    "\n",
    "**Safety** features protect against harmful or inappropriate content:\n",
    "- **Content moderation**: Filter inappropriate content\n",
    "- **Safety shields**: Prevent harmful outputs\n",
    "- **Safe AI practices**: Guidelines for responsible AI use\n",
    "\n",
    "**Why safety matters:**\n",
    "- Prevents harmful outputs\n",
    "- Protects users and systems\n",
    "- Ensures responsible AI deployment\n",
    "- Builds trust in AI systems\n",
    "\n",
    "**When to use safety:**\n",
    "- User-facing applications\n",
    "- Production systems\n",
    "- When handling sensitive data\n",
    "- Public-facing agents\n",
    "\n",
    "---\n",
    "\n",
    "### Hands-on: Safety Shields\n",
    "\n",
    "Let's explore how safety features work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Safety in chat completions\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Safety in Chat\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Note: Safety features are typically built into LlamaStack\n",
    "# The exact API may vary, but the concept is demonstrated here\n",
    "\n",
    "print(\"\\nüí° Safety features in LlamaStack:\")\n",
    "print(\"   ‚úÖ Content moderation\")\n",
    "print(\"   ‚úÖ Safety shields\")\n",
    "print(\"   ‚úÖ Harmful content detection\")\n",
    "print(\"   ‚úÖ Safe response generation\")\n",
    "\n",
    "# Example: Safe chat completion\n",
    "print(\"\\nüìù Example: Safe chat completion\")\n",
    "print(\"   LlamaStack automatically applies safety checks\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are best practices for IT security?\"\n",
    "        }\n",
    "    ],\n",
    "    # Safety settings (if available in your LlamaStack version)\n",
    "    # safety_settings={\n",
    "    #     \"enabled\": True,\n",
    "    #     \"moderation_level\": \"medium\"\n",
    "    # }\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"\\nü§ñ Safe Response:\\n{answer[:200]}...\\n\")\n",
    "\n",
    "print(\"‚úÖ Response generated with safety checks applied.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Moderation\n",
    "\n",
    "**Content moderation** checks inputs and outputs for:\n",
    "- Inappropriate language\n",
    "- Harmful content\n",
    "- Sensitive information\n",
    "- Policy violations\n",
    "\n",
    "**Best practices:**\n",
    "- Enable moderation for user-facing applications\n",
    "- Configure appropriate moderation levels\n",
    "- Log moderation events for review\n",
    "- Provide clear feedback when content is blocked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Evaluation\n",
    "\n",
    "### What is Evaluation?\n",
    "\n",
    "**Evaluation** measures how well your AI system performs. It helps you:\n",
    "- **Measure performance**: How accurate are responses?\n",
    "- **Compare models**: Which model works best?\n",
    "- **Track improvements**: Are changes making things better?\n",
    "- **Identify issues**: What needs to be fixed?\n",
    "\n",
    "**Why evaluation matters:**\n",
    "- Ensures quality before deployment\n",
    "- Helps choose the right model\n",
    "- Tracks performance over time\n",
    "- Builds confidence in AI systems\n",
    "\n",
    "**When to use evaluation:**\n",
    "- Before deploying to production\n",
    "- When comparing different models\n",
    "- After making changes\n",
    "- Regular quality checks\n",
    "\n",
    "---\n",
    "\n",
    "### Hands-on: Creating an Evaluation Dataset\n",
    "\n",
    "Let's create a simple evaluation dataset and run evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Create evaluation dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Creating Evaluation Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample evaluation dataset for IT operations Q&A\n",
    "evaluation_dataset = [\n",
    "    {\n",
    "        \"question\": \"How do I restart a web server?\",\n",
    "        \"expected_topics\": [\"systemctl\", \"restart\", \"nginx\", \"apache\"],\n",
    "        \"category\": \"troubleshooting\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What causes high CPU usage?\",\n",
    "        \"expected_topics\": [\"processes\", \"top\", \"htop\", \"monitoring\"],\n",
    "        \"category\": \"diagnostics\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I check disk space?\",\n",
    "        \"expected_topics\": [\"df\", \"du\", \"disk\", \"storage\"],\n",
    "        \"category\": \"monitoring\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä Evaluation Dataset:\")\n",
    "for i, item in enumerate(evaluation_dataset, 1):\n",
    "    print(f\"\\n   {i}. Question: {item['question']}\")\n",
    "    print(f\"      Category: {item['category']}\")\n",
    "    print(f\"      Expected topics: {', '.join(item['expected_topics'])}\")\n",
    "\n",
    "print(\"\\nüí° This dataset can be used to evaluate model performance.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Run evaluations\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 2: Running Evaluations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüîç Evaluating model responses...\\n\")\n",
    "\n",
    "results = []\n",
    "for item in evaluation_dataset:\n",
    "    # Get model response\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": item[\"question\"]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    # Simple evaluation: Check if expected topics appear in answer\n",
    "    answer_lower = answer.lower()\n",
    "    found_topics = [topic for topic in item[\"expected_topics\"] if topic.lower() in answer_lower]\n",
    "    score = len(found_topics) / len(item[\"expected_topics\"])\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": item[\"question\"],\n",
    "        \"answer\": answer[:100] + \"...\",\n",
    "        \"found_topics\": found_topics,\n",
    "        \"score\": score\n",
    "    })\n",
    "    \n",
    "    print(f\"üìù Q: {item['question']}\")\n",
    "    print(f\"   Found topics: {', '.join(found_topics) if found_topics else 'None'}\")\n",
    "    print(f\"   Score: {score:.2%}\")\n",
    "    print()\n",
    "\n",
    "# Calculate average score\n",
    "avg_score = sum(r[\"score\"] for r in results) / len(results)\n",
    "print(f\"‚úÖ Average Score: {avg_score:.2%}\")\n",
    "print(\"\\nüí° In practice, you would use more sophisticated evaluation metrics.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Evaluation Metrics\n",
    "\n",
    "**Common evaluation metrics:**\n",
    "- **Accuracy**: How often is the answer correct?\n",
    "- **Relevance**: Does the answer address the question?\n",
    "- **Completeness**: Does the answer cover all aspects?\n",
    "- **Latency**: How fast is the response?\n",
    "\n",
    "**Evaluation workflows:**\n",
    "1. Create evaluation dataset\n",
    "2. Run model on dataset\n",
    "3. Compare outputs to expected results\n",
    "4. Calculate metrics\n",
    "5. Analyze results and improve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### When to Use Each Feature\n",
    "\n",
    "**Simple Chat:**\n",
    "- ‚úÖ Basic Q&A\n",
    "- ‚úÖ Text generation\n",
    "- ‚úÖ Simple reasoning\n",
    "- ‚ùå Don't use when you need external knowledge or tools\n",
    "\n",
    "**RAG:**\n",
    "- ‚úÖ Need access to specific documents\n",
    "- ‚úÖ Domain-specific knowledge required\n",
    "- ‚úÖ Private/internal information\n",
    "- ‚ùå Don't use for general knowledge questions\n",
    "\n",
    "**MCP Tools:**\n",
    "- ‚úÖ Need to interact with external systems\n",
    "- ‚úÖ Want agents to take actions\n",
    "- ‚úÖ Need real-time data\n",
    "- ‚ùå Don't use for pure text generation\n",
    "\n",
    "**Safety:**\n",
    "- ‚úÖ User-facing applications\n",
    "- ‚úÖ Production systems\n",
    "- ‚úÖ Handling sensitive data\n",
    "- ‚ùå Not needed for internal/trusted use cases\n",
    "\n",
    "**Evaluation:**\n",
    "- ‚úÖ Before deploying to production\n",
    "- ‚úÖ Comparing different models\n",
    "- ‚úÖ Tracking performance over time\n",
    "- ‚ùå Not needed for one-off experiments\n",
    "\n",
    "---\n",
    "\n",
    "### How Features Complement Each Other\n",
    "\n",
    "**Powerful combinations:**\n",
    "- **Chat + RAG**: Answer questions with domain knowledge\n",
    "- **Chat + MCP**: Answer questions and take actions\n",
    "- **RAG + MCP**: Use knowledge to make informed actions\n",
    "- **All + Safety**: Production-ready agent with safety checks\n",
    "- **All + Evaluation**: Measured, safe, powerful agent\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps: Combining in Agents\n",
    "\n",
    "In **Notebook 04**, we'll combine these features to build:\n",
    "- **Knowledge-augmented agents** (Chat + RAG)\n",
    "- **Action-taking agents** (Chat + MCP)\n",
    "- **Safe agents** (All + Safety)\n",
    "- **Evaluated agents** (All + Evaluation)\n",
    "\n",
    "**Ready to build powerful agents?** Let's move to Notebook 04!\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "1. **Chat** is the foundation - basic LLM interaction\n",
    "2. **RAG** adds knowledge - access to documents\n",
    "3. **MCP** adds actions - interact with systems\n",
    "4. **Safety** adds protection - responsible AI\n",
    "5. **Evaluation** adds measurement - ensure quality\n",
    "\n",
    "**Remember:** Each feature solves a specific problem. Combining them creates powerful solutions!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
