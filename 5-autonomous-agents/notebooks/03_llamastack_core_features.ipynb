{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: LlamaStack Core Features\n",
    "\n",
    "## üéØ What is This Notebook About?\n",
    "\n",
    "Welcome! In this notebook, we'll explore **LlamaStack's core capabilities** - the building blocks that make powerful agents possible. Think of these as the agent's \"superpowers\"!\n",
    "\n",
    "**What we'll learn:**\n",
    "1. **Simple Chat** - Basic LLM interactions (the foundation)\n",
    "2. **RAG (Retrieval Augmented Generation)** - Enhancing LLMs with external knowledge (giving agents access to your docs!)\n",
    "\n",
    "**Why this matters:**\n",
    "- Understanding these features helps you build better agents\n",
    "- Each feature solves a specific problem (chat for Q&A, RAG for knowledge)\n",
    "- Combining features creates powerful solutions (chat + RAG = smart assistant with your docs!)\n",
    "- This knowledge prepares you for advanced agent development\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Prerequisites\n",
    "\n",
    "Before starting this notebook, make sure you have:\n",
    "- ‚úÖ Completed Notebook 02: Building a Simple Agent\n",
    "- ‚úÖ LlamaStack server running (see Module README for setup)\n",
    "- ‚úÖ Ollama running with `llama3.2:3b` model\n",
    "- ‚úÖ Python environment with dependencies installed\n",
    "\n",
    "**The fun part:** We'll start with simple chat (easy!) and then add RAG so agents can answer questions about your specific IT operations documentation!\n",
    "\n",
    "---\n",
    "\n",
    "## üíº How This Applies to IT Operations\n",
    "\n",
    "**The problem:** LLMs are trained on general knowledge, but they don't know about YOUR infrastructure, YOUR procedures, YOUR specific configurations. How do you give agents access to your internal knowledge?\n",
    "\n",
    "**The solution:** RAG (Retrieval Augmented Generation)! You store your IT operations documentation in a vector store, and the agent retrieves relevant context when answering questions. It's like giving the agent access to your internal wiki!\n",
    "\n",
    "**Real-world impact:**\n",
    "- **Chat** for general Q&A - \"What is a load balancer?\" (general knowledge)\n",
    "- **RAG** for specific knowledge - \"How do we restart services in our infrastructure?\" (your docs!)\n",
    "- **Combine both** - Agents can answer general questions AND questions about your specific setup\n",
    "- **Production-ready** - Give agents access to your runbooks, troubleshooting guides, and documentation\n",
    "\n",
    "**The fun part:** We'll build a vector store with IT operations documentation and watch the agent retrieve relevant context to answer questions. It's like having an assistant that actually reads your documentation!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand LlamaStack's core capabilities (chat and RAG)\n",
    "- ‚úÖ Know when to use each feature (chat for general Q&A, RAG for domain-specific knowledge)\n",
    "- ‚úÖ See how features work independently (each solves a different problem)\n",
    "- ‚úÖ Be ready to explore MCP tools (Notebook 04), Safety (Notebook 05), and Evaluation (Notebook 06)\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Setup\n",
    "\n",
    "Let's start by connecting to LlamaStack and verifying everything is working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "# Configuration\n",
    "llamastack_url = os.getenv(\"LLAMA_STACK_URL\", \"http://localhost:8321\")\n",
    "model = os.getenv(\"LLAMA_MODEL\", \"ollama/llama3.2:3b\")\n",
    "\n",
    "print(f\"üì° LlamaStack URL: {llamastack_url}\")\n",
    "print(f\"ü§ñ Model: {model}\")\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "client = LlamaStackClient(base_url=llamastack_url)\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    print(f\"\\n‚úÖ Connected to LlamaStack\")\n",
    "    print(f\"   Available models: {len(models)}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Cannot connect to LlamaStack: {e}\")\n",
    "    print(\"   Please ensure LlamaStack is running:\")\n",
    "    print(\"   python scripts/start_llama_stack.py\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Simple Chat\n",
    "\n",
    "**What we're doing:** Learning the basics of LLM interactions - sending messages and getting responses.\n",
    "\n",
    "**Why:** Chat is the foundation. Everything else builds on this. It's like learning to walk before you run!\n",
    "\n",
    "### What is Chat?\n",
    "\n",
    "**Chat** is the most basic way to interact with an LLM. It's a conversation where you send messages and receive responses - simple as that!\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Message Types**: System (instructions), User (questions), Assistant (responses)\n",
    "- **Streaming vs Non-streaming**: Get responses as they're generated (streaming) or wait for complete response (non-streaming)\n",
    "- **Conversation Context**: LLM remembers previous messages in the conversation (like a real conversation!)\n",
    "\n",
    "**When to use Chat:**\n",
    "- ‚úÖ Simple Q&A (\"What is a load balancer?\")\n",
    "- ‚úÖ Text generation (summaries, explanations)\n",
    "- ‚úÖ Basic reasoning tasks (troubleshooting steps)\n",
    "- ‚ùå Don't use when you need external knowledge or tools (use RAG or MCP instead)\n",
    "\n",
    "---\n",
    "\n",
    "### Hands-on: Basic Chat Completion\n",
    "\n",
    "Let's start with the simplest example - a single question and answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic chat completion\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Basic Chat Completion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is artificial intelligence in one sentence?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract and display the response\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"\\nüìù Question: What is artificial intelligence in one sentence?\")\n",
    "print(f\"\\nü§ñ Answer:\\n{answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompts\n",
    "\n",
    "**System prompts** are instructions that guide the LLM's behavior. They set the \"personality\" and \"role\" of the assistant.\n",
    "\n",
    "**Why use system prompts:**\n",
    "- Define the assistant's role (e.g., \"You are a helpful IT operations assistant\")\n",
    "- Set behavior guidelines\n",
    "- Provide context about the domain\n",
    "- Ensure consistent responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Chat with system prompt\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 2: Chat with System Prompt\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful IT operations assistant. You provide clear, concise answers about IT infrastructure and operations.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What should I check if a web server is not responding?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"\\nüìù Question: What should I check if a web server is not responding?\")\n",
    "print(f\"\\nü§ñ Answer (with IT operations context):\\n{answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-turn Conversations\n",
    "\n",
    "**Multi-turn conversations** maintain context across multiple exchanges. The LLM remembers previous messages in the conversation.\n",
    "\n",
    "**Why this matters:**\n",
    "- Natural conversation flow\n",
    "- Can refer back to earlier topics\n",
    "- Builds on previous context\n",
    "- More human-like interaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Multi-turn conversation\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 3: Multi-turn Conversation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First turn\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm setting up a new database server. What should I consider?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer1 = response1.choices[0].message.content\n",
    "print(f\"\\nüìù Turn 1 - Question: I'm setting up a new database server. What should I consider?\")\n",
    "print(f\"\\nü§ñ Answer:\\n{answer1[:200]}...\\n\")\n",
    "\n",
    "# Second turn - add previous messages to maintain context\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": answer1\n",
    "})\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What about security specifically?\"\n",
    "})\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer2 = response2.choices[0].message.content\n",
    "print(f\"\\nüìù Turn 2 - Question: What about security specifically?\")\n",
    "print(f\"   (Note: The assistant knows we're talking about database servers)\\n\")\n",
    "print(f\"ü§ñ Answer:\\n{answer2[:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Responses\n",
    "\n",
    "**Streaming** allows you to receive the response as it's being generated, token by token. This provides:\n",
    "- Faster perceived response time\n",
    "- Real-time feedback\n",
    "- Better user experience\n",
    "\n",
    "**When to use streaming:**\n",
    "- Long responses\n",
    "- Interactive applications\n",
    "- When you want immediate feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Streaming response\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 4: Streaming Response\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìù Question: Explain what RAG (Retrieval Augmented Generation) is.\\n\")\n",
    "print(\"ü§ñ Answer (streaming):\\n\")\n",
    "\n",
    "# Create streaming completion\n",
    "stream = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain what RAG (Retrieval Augmented Generation) is in 2-3 sentences.\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True  # Enable streaming\n",
    ")\n",
    "\n",
    "# Process stream chunk by chunk\n",
    "full_response = \"\"\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        print(content, end=\"\", flush=True)\n",
    "        full_response += content\n",
    "\n",
    "print(\"\\n\\n‚úÖ Streaming complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: RAG (Retrieval Augmented Generation)\n",
    "\n",
    "**What we're doing:** Giving agents access to YOUR documentation - runbooks, troubleshooting guides, procedures, anything!\n",
    "\n",
    "**Why:** LLMs don't know about your specific infrastructure. RAG lets you store your docs in a vector store, and the agent retrieves relevant context when answering questions. It's like giving the agent access to your internal wiki!\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "**RAG** enhances LLMs with external knowledge by:\n",
    "1. **Storing documents** in a vector database (vector store) - your docs go here!\n",
    "2. **Searching** for relevant context when answering questions - semantic search finds the right docs\n",
    "3. **Augmenting** the LLM's prompt with retrieved context - the agent uses YOUR docs to answer!\n",
    "\n",
    "**Why RAG matters:**\n",
    "- LLMs have training data cutoff dates (they don't know about your new systems!)\n",
    "- Can't access private/internal documents (your runbooks aren't on the internet!)\n",
    "- RAG provides up-to-date, domain-specific knowledge (your specific procedures!)\n",
    "- Improves accuracy for specialized topics (your infrastructure, your way!)\n",
    "\n",
    "**When to use RAG:**\n",
    "- ‚úÖ Need access to specific documents (runbooks, procedures)\n",
    "- ‚úÖ Domain-specific knowledge required (your infrastructure)\n",
    "- ‚úÖ Private/internal information (internal docs, configurations)\n",
    "- ‚úÖ Up-to-date information needed (current procedures, recent changes)\n",
    "\n",
    "---\n",
    "\n",
    "### Hands-on: Creating a Vector Store\n",
    "\n",
    "Let's create a vector store and add some IT operations documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Create a vector store\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Creating a Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample IT operations documentation\n",
    "it_docs = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"content\": \"To restart a web server, use: systemctl restart nginx. Check status with: systemctl status nginx.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"content\": \"High CPU usage troubleshooting: 1) Check top processes with 'top' or 'htop', 2) Identify CPU-intensive processes, 3) Check for runaway processes or infinite loops.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"content\": \"Database connection issues: Check firewall rules, verify credentials, ensure database service is running, check network connectivity with 'telnet hostname port'.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc4\",\n",
    "        \"content\": \"Disk space issues: Use 'df -h' to check disk usage, find large files with 'du -sh /*', clean logs with 'journalctl --vacuum-time=7d'.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc5\",\n",
    "        \"content\": \"Service monitoring: Use 'systemctl list-units --type=service' to list all services, 'systemctl is-active servicename' to check status, set up monitoring with Prometheus or Nagios.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\nüìö Sample IT Operations Documentation:\")\n",
    "for doc in it_docs:\n",
    "    print(f\"   - {doc['id']}: {doc['content'][:60]}...\")\n",
    "\n",
    "print(\"\\nüí° These documents will be stored in a vector store for retrieval.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store using LlamaStack\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Creating Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "# Step 1: Create files from text content\n",
    "print(f\"\\nüìù Creating files from {len(it_docs)} documents...\")\n",
    "file_ids = []\n",
    "\n",
    "for i, doc in enumerate(it_docs, 1):\n",
    "    # Create a file-like object from the document content\n",
    "    file_content = BytesIO(doc[\"content\"].encode('utf-8'))\n",
    "    file_name = f\"doc_{i}.txt\"\n",
    "    \n",
    "    # Upload file to LlamaStack\n",
    "    # The API expects a tuple: (filename, file_content, content_type)\n",
    "    file_obj = (file_name, file_content, 'text/plain')\n",
    "    \n",
    "    uploaded_file = client.files.create(\n",
    "        file=file_obj,\n",
    "        purpose=\"assistants\"\n",
    "    )\n",
    "    file_ids.append(uploaded_file.id)\n",
    "    print(f\"   ‚úÖ Uploaded {file_name} (ID: {uploaded_file.id})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(file_ids)} files\")\n",
    "\n",
    "# Step 2: Create vector store with files\n",
    "print(f\"\\nüì¶ Creating vector store...\")\n",
    "vector_store = client.vector_stores.create(\n",
    "    name=\"it-operations-docs\",\n",
    "    file_ids=file_ids,\n",
    "    metadata={\"description\": \"IT operations documentation and troubleshooting guides\"}\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Vector store created!\")\n",
    "print(f\"   Name: {vector_store.name}\")\n",
    "print(f\"   ID: {vector_store.id}\")\n",
    "print(f\"   Files: {len(file_ids)}\")\n",
    "\n",
    "# Step 3: Wait for files to be processed (vector stores need time to index files)\n",
    "print(f\"\\n‚è≥ Waiting for files to be processed and indexed...\")\n",
    "import time\n",
    "\n",
    "max_wait = 30  # Maximum wait time in seconds\n",
    "wait_interval = 2  # Check every 2 seconds\n",
    "elapsed = 0\n",
    "\n",
    "while elapsed < max_wait:\n",
    "    # Check vector store status\n",
    "    vs_status = client.vector_stores.retrieve(vector_store.id)\n",
    "    \n",
    "    # Check if files are processed (status might be in file_counts or similar)\n",
    "    if hasattr(vs_status, 'file_counts'):\n",
    "        file_counts = vs_status.file_counts\n",
    "        if hasattr(file_counts, 'in_progress') and file_counts.in_progress == 0:\n",
    "            print(f\"   ‚úÖ All files processed!\")\n",
    "            break\n",
    "    elif hasattr(vs_status, 'status'):\n",
    "        if vs_status.status == 'completed':\n",
    "            print(f\"   ‚úÖ Vector store ready!\")\n",
    "            break\n",
    "    \n",
    "    # Check file status directly\n",
    "    vs_files = client.vector_stores.files.list(vector_store.id)\n",
    "    if hasattr(vs_files, 'data'):\n",
    "        processed = sum(1 for f in vs_files.data if hasattr(f, 'status') and f.status == 'completed')\n",
    "        if processed == len(file_ids):\n",
    "            print(f\"   ‚úÖ All {processed} files processed!\")\n",
    "            break\n",
    "    \n",
    "    print(f\"   ‚è≥ Waiting... ({elapsed}s/{max_wait}s)\", end='\\r')\n",
    "    time.sleep(wait_interval)\n",
    "    elapsed += wait_interval\n",
    "\n",
    "if elapsed >= max_wait:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Timeout waiting for processing. Files may still be indexing.\")\n",
    "    print(f\"   üí° You can proceed, but search results may be incomplete initially.\")\n",
    "\n",
    "print(f\"\\nüí° The vector store is ready for semantic search!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for Relevant Context\n",
    "\n",
    "Once documents are in the vector store, we can search for relevant context based on semantic similarity (meaning, not just keywords).\n",
    "\n",
    "**How it works:**\n",
    "1. Convert query to embedding (vector representation)\n",
    "2. Compare with document embeddings\n",
    "3. Return most similar documents\n",
    "4. Use retrieved documents as context for LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Search for relevant context using LlamaStack API\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 2: Searching Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"How do I restart a web server?\"\n",
    "print(f\"\\nüîç Query: {query}\\n\")\n",
    "\n",
    "# Search the vector store using LlamaStack API\n",
    "search_results = client.vector_stores.search(\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    max_num_results=2\n",
    ")\n",
    "\n",
    "print(\"üìö Retrieved Documents (from vector store):\")\n",
    "print(f\"   Found {len(search_results.data)} results\\n\")\n",
    "\n",
    "if len(search_results.data) == 0:\n",
    "    print(\"   ‚ö†Ô∏è  No results found. This might mean:\")\n",
    "    print(\"      - Files are still being processed/indexed\")\n",
    "    print(\"      - Try waiting a few seconds and searching again\")\n",
    "    print(\"      - Or check if files were added correctly to the vector store\")\n",
    "    print(\"\\n   üí° For demonstration, we'll use the original documents:\")\n",
    "    # Fallback to original documents for demonstration\n",
    "    for i, doc in enumerate(it_docs[:2], 1):\n",
    "        if \"restart\" in doc[\"content\"].lower() or \"web server\" in doc[\"content\"].lower():\n",
    "            print(f\"\\n   {i}. {doc['id']}:\")\n",
    "            print(f\"      {doc['content']}\")\n",
    "else:\n",
    "    for i, result in enumerate(search_results.data, 1):\n",
    "        print(f\"   {i}. \", end=\"\")\n",
    "        # The result contains the document content and score\n",
    "        if hasattr(result, 'score'):\n",
    "            print(f\"Score: {result.score:.3f}\")\n",
    "        if hasattr(result, 'content') and result.content:\n",
    "            print(f\"      Content: {result.content[:150]}...\")\n",
    "        elif hasattr(result, 'text') and result.text:\n",
    "            print(f\"      Text: {result.text[:150]}...\")\n",
    "        elif hasattr(result, 'document') and result.document:\n",
    "            print(f\"      Document: {str(result.document)[:150]}...\")\n",
    "        else:\n",
    "            # Try to get any text-like attribute\n",
    "            result_str = str(result)\n",
    "            print(f\"      Result: {result_str[:150]}...\")\n",
    "        print()\n",
    "\n",
    "print(\"\\nüí° These documents were retrieved using semantic search (embeddings).\")\n",
    "print(\"   They will be used as context for the LLM.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Retrieved Context in Chat\n",
    "\n",
    "Now let's use the retrieved documents as context for the LLM. This is the \"Augmented Generation\" part of RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: RAG - Using retrieved context in chat\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 3: RAG - Chat with Retrieved Context\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"How do I restart a web server?\"\n",
    "print(f\"\\nüìù Question: {query}\\n\")\n",
    "\n",
    "# Search the vector store for relevant context\n",
    "search_results = client.vector_stores.search(\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    max_num_results=2\n",
    ")\n",
    "\n",
    "# Build context from retrieved documents\n",
    "context_parts = []\n",
    "for i, result in enumerate(search_results.data, 1):\n",
    "    # Extract content from result\n",
    "    if hasattr(result, 'content') and result.content:\n",
    "        content = result.content\n",
    "    elif hasattr(result, 'text') and result.text:\n",
    "        content = result.text\n",
    "    else:\n",
    "        # Try to get content from file if available\n",
    "        content = f\"Document {i} (score: {result.score:.3f})\"\n",
    "    \n",
    "    context_parts.append(f\"Document {i}:\\n{content}\")\n",
    "\n",
    "context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "# Create prompt with context\n",
    "prompt = f\"\"\"Use the following IT operations documentation to answer the question.\n",
    "\n",
    "Documentation:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the documentation provided:\"\"\"\n",
    "\n",
    "print(f\"üìö Context Retrieved from Vector Store:\\n{context[:300]}...\\n\")\n",
    "\n",
    "# Get response with context\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful IT operations assistant. Answer questions based on the provided documentation.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"ü§ñ Answer (with RAG context):\\n{answer}\\n\")\n",
    "print(\"‚úÖ Notice how the answer uses the specific documentation retrieved from the vector store!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. **Simple Chat** is the foundation - basic LLM interactions for Q&A and text generation\n",
    "2. **RAG (Retrieval Augmented Generation)** gives agents access to YOUR documentation - store docs, retrieve context, answer questions!\n",
    "3. **System prompts** guide the LLM's behavior - set the role, personality, and domain\n",
    "4. **Multi-turn conversations** maintain context - agents remember what you talked about\n",
    "5. **Streaming** provides real-time feedback - see responses as they're generated\n",
    "\n",
    "**The big picture:**\n",
    "- **Chat** for general Q&A - when you need general knowledge\n",
    "- **RAG** for domain-specific knowledge - when you need YOUR docs\n",
    "- **Combine both** - agents can answer general questions AND questions about your specific setup\n",
    "\n",
    "**For IT operations:**\n",
    "- Use **Chat** for general IT questions (\"What is a load balancer?\")\n",
    "- Use **RAG** for your specific procedures (\"How do we restart services in our infrastructure?\")\n",
    "- Store your runbooks, troubleshooting guides, and documentation in vector stores\n",
    "- Give agents access to your internal knowledge base\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "**Ready for more?** In **Notebook 04**, we'll explore:\n",
    "- **MCP (Model Context Protocol)** - External tool integration (give agents access to APIs, databases, commands!)\n",
    "- **How to integrate tools** with agents (connect to your monitoring systems, ticketing systems, etc.)\n",
    "- **Building production-ready agents** that can both answer questions AND take actions\n",
    "\n",
    "**The fun part:** You'll learn how to give agents access to your IT infrastructure tools - monitoring APIs, service management, databases, anything!\n",
    "\n",
    "---\n",
    "\n",
    "**Ready?** Let's move to Notebook 04: MCP Tools! üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
