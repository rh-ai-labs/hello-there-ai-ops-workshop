{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: LlamaStack Core Features\n",
    "\n",
    "## üéØ What is This Notebook About?\n",
    "\n",
    "Welcome to Notebook 03! In this notebook, we'll explore **LlamaStack's core capabilities** - the building blocks that make powerful agents possible.\n",
    "\n",
    "**What we'll learn:**\n",
    "1. **Simple Chat** - Basic LLM interactions\n",
    "2. **RAG (Retrieval Augmented Generation)** - Enhancing LLMs with external knowledge\n",
    "3. **MCP (Model Context Protocol)** - External tool integration\n",
    "4. **Safety** - Content moderation and safety shields\n",
    "5. **Evaluation** - Measuring AI performance\n",
    "\n",
    "**Why this matters:**\n",
    "- Understanding these features helps you build better agents\n",
    "- Each feature solves a specific problem\n",
    "- Combining features creates powerful solutions\n",
    "- This knowledge prepares you for advanced agent development\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand LlamaStack's core capabilities\n",
    "- ‚úÖ Know when to use each feature\n",
    "- ‚úÖ See how features work independently\n",
    "- ‚úÖ Be ready to combine features in agents (Notebook 04)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Prerequisites\n",
    "\n",
    "- LlamaStack server running (see Module README)\n",
    "- Ollama running with llama3.2:3b model\n",
    "- Python environment with dependencies installed\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Setup\n",
    "\n",
    "Let's start by connecting to LlamaStack and verifying everything is working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "# Configuration\n",
    "llamastack_url = os.getenv(\"LLAMA_STACK_URL\", \"http://localhost:8321\")\n",
    "model = os.getenv(\"LLAMA_MODEL\", \"ollama/llama3.2:3b\")\n",
    "\n",
    "print(f\"üì° LlamaStack URL: {llamastack_url}\")\n",
    "print(f\"ü§ñ Model: {model}\")\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "client = LlamaStackClient(base_url=llamastack_url)\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    print(f\"\\n‚úÖ Connected to LlamaStack\")\n",
    "    print(f\"   Available models: {len(models)}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Cannot connect to LlamaStack: {e}\")\n",
    "    print(\"   Please ensure LlamaStack is running:\")\n",
    "    print(\"   python scripts/start_llama_stack.py\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Simple Chat\n",
    "\n",
    "### What is Chat?\n",
    "\n",
    "**Chat** is the most basic way to interact with an LLM. It's a conversation where you send messages and receive responses.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Message Types**: System (instructions), User (questions), Assistant (responses)\n",
    "- **Streaming vs Non-streaming**: Get responses as they're generated or wait for complete response\n",
    "- **Conversation Context**: LLM remembers previous messages in the conversation\n",
    "\n",
    "**When to use Chat:**\n",
    "- Simple Q&A\n",
    "- Text generation\n",
    "- Basic reasoning tasks\n",
    "- When you don't need external knowledge or tools\n",
    "\n",
    "---\n",
    "\n",
    "### Hands-on: Basic Chat Completion\n",
    "\n",
    "Let's start with the simplest example - a single question and answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic chat completion\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Basic Chat Completion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is artificial intelligence in one sentence?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract and display the response\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"\\nüìù Question: What is artificial intelligence in one sentence?\")\n",
    "print(f\"\\nü§ñ Answer:\\n{answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompts\n",
    "\n",
    "**System prompts** are instructions that guide the LLM's behavior. They set the \"personality\" and \"role\" of the assistant.\n",
    "\n",
    "**Why use system prompts:**\n",
    "- Define the assistant's role (e.g., \"You are a helpful IT operations assistant\")\n",
    "- Set behavior guidelines\n",
    "- Provide context about the domain\n",
    "- Ensure consistent responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Chat with system prompt\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 2: Chat with System Prompt\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful IT operations assistant. You provide clear, concise answers about IT infrastructure and operations.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What should I check if a web server is not responding?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"\\nüìù Question: What should I check if a web server is not responding?\")\n",
    "print(f\"\\nü§ñ Answer (with IT operations context):\\n{answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-turn Conversations\n",
    "\n",
    "**Multi-turn conversations** maintain context across multiple exchanges. The LLM remembers previous messages in the conversation.\n",
    "\n",
    "**Why this matters:**\n",
    "- Natural conversation flow\n",
    "- Can refer back to earlier topics\n",
    "- Builds on previous context\n",
    "- More human-like interaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Multi-turn conversation\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 3: Multi-turn Conversation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First turn\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm setting up a new database server. What should I consider?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer1 = response1.choices[0].message.content\n",
    "print(f\"\\nüìù Turn 1 - Question: I'm setting up a new database server. What should I consider?\")\n",
    "print(f\"\\nü§ñ Answer:\\n{answer1[:200]}...\\n\")\n",
    "\n",
    "# Second turn - add previous messages to maintain context\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": answer1\n",
    "})\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What about security specifically?\"\n",
    "})\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer2 = response2.choices[0].message.content\n",
    "print(f\"\\nüìù Turn 2 - Question: What about security specifically?\")\n",
    "print(f\"   (Note: The assistant knows we're talking about database servers)\\n\")\n",
    "print(f\"ü§ñ Answer:\\n{answer2[:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Responses\n",
    "\n",
    "**Streaming** allows you to receive the response as it's being generated, token by token. This provides:\n",
    "- Faster perceived response time\n",
    "- Real-time feedback\n",
    "- Better user experience\n",
    "\n",
    "**When to use streaming:**\n",
    "- Long responses\n",
    "- Interactive applications\n",
    "- When you want immediate feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Streaming response\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 4: Streaming Response\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìù Question: Explain what RAG (Retrieval Augmented Generation) is.\\n\")\n",
    "print(\"ü§ñ Answer (streaming):\\n\")\n",
    "\n",
    "# Create streaming completion\n",
    "stream = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain what RAG (Retrieval Augmented Generation) is in 2-3 sentences.\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True  # Enable streaming\n",
    ")\n",
    "\n",
    "# Process stream chunk by chunk\n",
    "full_response = \"\"\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        print(content, end=\"\", flush=True)\n",
    "        full_response += content\n",
    "\n",
    "print(\"\\n\\n‚úÖ Streaming complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: RAG (Retrieval Augmented Generation)\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "**RAG** enhances LLMs with external knowledge by:\n",
    "1. **Storing documents** in a vector database (vector store)\n",
    "2. **Searching** for relevant context when answering questions\n",
    "3. **Augmenting** the LLM's prompt with retrieved context\n",
    "\n",
    "**Why RAG matters:**\n",
    "- LLMs have training data cutoff dates\n",
    "- Can't access private/internal documents\n",
    "- RAG provides up-to-date, domain-specific knowledge\n",
    "- Improves accuracy for specialized topics\n",
    "\n",
    "**When to use RAG:**\n",
    "- Need access to specific documents\n",
    "- Domain-specific knowledge required\n",
    "- Private/internal information\n",
    "- Up-to-date information needed\n",
    "\n",
    "---\n",
    "\n",
    "### Hands-on: Creating a Vector Store\n",
    "\n",
    "Let's create a vector store and add some IT operations documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Create a vector store\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Creating a Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample IT operations documentation\n",
    "it_docs = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"content\": \"To restart a web server, use: systemctl restart nginx. Check status with: systemctl status nginx.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"content\": \"High CPU usage troubleshooting: 1) Check top processes with 'top' or 'htop', 2) Identify CPU-intensive processes, 3) Check for runaway processes or infinite loops.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"content\": \"Database connection issues: Check firewall rules, verify credentials, ensure database service is running, check network connectivity with 'telnet hostname port'.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc4\",\n",
    "        \"content\": \"Disk space issues: Use 'df -h' to check disk usage, find large files with 'du -sh /*', clean logs with 'journalctl --vacuum-time=7d'.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc5\",\n",
    "        \"content\": \"Service monitoring: Use 'systemctl list-units --type=service' to list all services, 'systemctl is-active servicename' to check status, set up monitoring with Prometheus or Nagios.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\nüìö Sample IT Operations Documentation:\")\n",
    "for doc in it_docs:\n",
    "    print(f\"   - {doc['id']}: {doc['content'][:60]}...\")\n",
    "\n",
    "print(\"\\nüí° These documents will be stored in a vector store for retrieval.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store using LlamaStack\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Creating Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "# Step 1: Create files from text content\n",
    "print(f\"\\nüìù Creating files from {len(it_docs)} documents...\")\n",
    "file_ids = []\n",
    "\n",
    "for i, doc in enumerate(it_docs, 1):\n",
    "    # Create a file-like object from the document content\n",
    "    file_content = BytesIO(doc[\"content\"].encode('utf-8'))\n",
    "    file_name = f\"doc_{i}.txt\"\n",
    "    \n",
    "    # Upload file to LlamaStack\n",
    "    # The API expects a tuple: (filename, file_content, content_type)\n",
    "    file_obj = (file_name, file_content, 'text/plain')\n",
    "    \n",
    "    uploaded_file = client.files.create(\n",
    "        file=file_obj,\n",
    "        purpose=\"assistants\"\n",
    "    )\n",
    "    file_ids.append(uploaded_file.id)\n",
    "    print(f\"   ‚úÖ Uploaded {file_name} (ID: {uploaded_file.id})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(file_ids)} files\")\n",
    "\n",
    "# Step 2: Create vector store with files\n",
    "print(f\"\\nüì¶ Creating vector store...\")\n",
    "vector_store = client.vector_stores.create(\n",
    "    name=\"it-operations-docs\",\n",
    "    file_ids=file_ids,\n",
    "    metadata={\"description\": \"IT operations documentation and troubleshooting guides\"}\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Vector store created!\")\n",
    "print(f\"   Name: {vector_store.name}\")\n",
    "print(f\"   ID: {vector_store.id}\")\n",
    "print(f\"   Files: {len(file_ids)}\")\n",
    "\n",
    "# Step 3: Wait for files to be processed (vector stores need time to index files)\n",
    "print(f\"\\n‚è≥ Waiting for files to be processed and indexed...\")\n",
    "import time\n",
    "\n",
    "max_wait = 30  # Maximum wait time in seconds\n",
    "wait_interval = 2  # Check every 2 seconds\n",
    "elapsed = 0\n",
    "\n",
    "while elapsed < max_wait:\n",
    "    # Check vector store status\n",
    "    vs_status = client.vector_stores.retrieve(vector_store.id)\n",
    "    \n",
    "    # Check if files are processed (status might be in file_counts or similar)\n",
    "    if hasattr(vs_status, 'file_counts'):\n",
    "        file_counts = vs_status.file_counts\n",
    "        if hasattr(file_counts, 'in_progress') and file_counts.in_progress == 0:\n",
    "            print(f\"   ‚úÖ All files processed!\")\n",
    "            break\n",
    "    elif hasattr(vs_status, 'status'):\n",
    "        if vs_status.status == 'completed':\n",
    "            print(f\"   ‚úÖ Vector store ready!\")\n",
    "            break\n",
    "    \n",
    "    # Check file status directly\n",
    "    vs_files = client.vector_stores.files.list(vector_store.id)\n",
    "    if hasattr(vs_files, 'data'):\n",
    "        processed = sum(1 for f in vs_files.data if hasattr(f, 'status') and f.status == 'completed')\n",
    "        if processed == len(file_ids):\n",
    "            print(f\"   ‚úÖ All {processed} files processed!\")\n",
    "            break\n",
    "    \n",
    "    print(f\"   ‚è≥ Waiting... ({elapsed}s/{max_wait}s)\", end='\\r')\n",
    "    time.sleep(wait_interval)\n",
    "    elapsed += wait_interval\n",
    "\n",
    "if elapsed >= max_wait:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Timeout waiting for processing. Files may still be indexing.\")\n",
    "    print(f\"   üí° You can proceed, but search results may be incomplete initially.\")\n",
    "\n",
    "print(f\"\\nüí° The vector store is ready for semantic search!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for Relevant Context\n",
    "\n",
    "Once documents are in the vector store, we can search for relevant context based on semantic similarity (meaning, not just keywords).\n",
    "\n",
    "**How it works:**\n",
    "1. Convert query to embedding (vector representation)\n",
    "2. Compare with document embeddings\n",
    "3. Return most similar documents\n",
    "4. Use retrieved documents as context for LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Search for relevant context using LlamaStack API\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 2: Searching Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"How do I restart a web server?\"\n",
    "print(f\"\\nüîç Query: {query}\\n\")\n",
    "\n",
    "# Search the vector store using LlamaStack API\n",
    "search_results = client.vector_stores.search(\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    max_num_results=2\n",
    ")\n",
    "\n",
    "print(\"üìö Retrieved Documents (from vector store):\")\n",
    "print(f\"   Found {len(search_results.data)} results\\n\")\n",
    "\n",
    "if len(search_results.data) == 0:\n",
    "    print(\"   ‚ö†Ô∏è  No results found. This might mean:\")\n",
    "    print(\"      - Files are still being processed/indexed\")\n",
    "    print(\"      - Try waiting a few seconds and searching again\")\n",
    "    print(\"      - Or check if files were added correctly to the vector store\")\n",
    "    print(\"\\n   üí° For demonstration, we'll use the original documents:\")\n",
    "    # Fallback to original documents for demonstration\n",
    "    for i, doc in enumerate(it_docs[:2], 1):\n",
    "        if \"restart\" in doc[\"content\"].lower() or \"web server\" in doc[\"content\"].lower():\n",
    "            print(f\"\\n   {i}. {doc['id']}:\")\n",
    "            print(f\"      {doc['content']}\")\n",
    "else:\n",
    "    for i, result in enumerate(search_results.data, 1):\n",
    "        print(f\"   {i}. \", end=\"\")\n",
    "        # The result contains the document content and score\n",
    "        if hasattr(result, 'score'):\n",
    "            print(f\"Score: {result.score:.3f}\")\n",
    "        if hasattr(result, 'content') and result.content:\n",
    "            print(f\"      Content: {result.content[:150]}...\")\n",
    "        elif hasattr(result, 'text') and result.text:\n",
    "            print(f\"      Text: {result.text[:150]}...\")\n",
    "        elif hasattr(result, 'document') and result.document:\n",
    "            print(f\"      Document: {str(result.document)[:150]}...\")\n",
    "        else:\n",
    "            # Try to get any text-like attribute\n",
    "            result_str = str(result)\n",
    "            print(f\"      Result: {result_str[:150]}...\")\n",
    "        print()\n",
    "\n",
    "print(\"\\nüí° These documents were retrieved using semantic search (embeddings).\")\n",
    "print(\"   They will be used as context for the LLM.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Retrieved Context in Chat\n",
    "\n",
    "Now let's use the retrieved documents as context for the LLM. This is the \"Augmented Generation\" part of RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: RAG - Using retrieved context in chat\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 3: RAG - Chat with Retrieved Context\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"How do I restart a web server?\"\n",
    "print(f\"\\nüìù Question: {query}\\n\")\n",
    "\n",
    "# Search the vector store for relevant context\n",
    "search_results = client.vector_stores.search(\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    max_num_results=2\n",
    ")\n",
    "\n",
    "# Build context from retrieved documents\n",
    "context_parts = []\n",
    "for i, result in enumerate(search_results.results, 1):\n",
    "    # Extract content from result\n",
    "    if hasattr(result, 'content') and result.content:\n",
    "        content = result.content\n",
    "    elif hasattr(result, 'text') and result.text:\n",
    "        content = result.text\n",
    "    else:\n",
    "        # Try to get content from file if available\n",
    "        content = f\"Document {i} (score: {result.score:.3f})\"\n",
    "    \n",
    "    context_parts.append(f\"Document {i}:\\n{content}\")\n",
    "\n",
    "context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "# Create prompt with context\n",
    "prompt = f\"\"\"Use the following IT operations documentation to answer the question.\n",
    "\n",
    "Documentation:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the documentation provided:\"\"\"\n",
    "\n",
    "print(f\"üìö Context Retrieved from Vector Store:\\n{context[:300]}...\\n\")\n",
    "\n",
    "# Get response with context\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful IT operations assistant. Answer questions based on the provided documentation.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"ü§ñ Answer (with RAG context):\\n{answer}\\n\")\n",
    "print(\"‚úÖ Notice how the answer uses the specific documentation retrieved from the vector store!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: MCP (Model Context Protocol)\n",
    "\n",
    "### What is MCP?\n",
    "\n",
    "**MCP (Model Context Protocol)** is a protocol for integrating external tools and services with LLMs. It allows agents to:\n",
    "- **Call external APIs** (e.g., check service status, restart services)\n",
    "- **Access databases** (e.g., query incident logs)\n",
    "- **Execute commands** (e.g., run system commands)\n",
    "- **Integrate with other systems** (e.g., monitoring tools, ticketing systems)\n",
    "\n",
    "**Why MCP matters:**\n",
    "- LLMs can't directly interact with systems\n",
    "- MCP provides a standardized way to connect tools\n",
    "- Enables agents to take real actions\n",
    "- Makes agents more powerful and useful\n",
    "\n",
    "**When to use MCP:**\n",
    "- Need to interact with external systems\n",
    "- Want agents to take actions (not just answer questions)\n",
    "- Need real-time data from APIs\n",
    "- Want to integrate with existing tools\n",
    "\n",
    "---\n",
    "\n",
    "### Hands-on: Exploring Tool Runtime\n",
    "\n",
    "Let's explore what tools are available and how they work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Understanding MCP Tools\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Understanding MCP Tools\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüí° MCP (Model Context Protocol) Tools:\")\n",
    "print(\"   - Allow agents to call external APIs\")\n",
    "print(\"   - Enable system command execution\")\n",
    "print(\"   - Provide database access\")\n",
    "print(\"   - Integrate with monitoring systems\")\n",
    "print(\"\\nüìù In Notebook 02, we saw how to create custom tools.\")\n",
    "print(\"   Tools are Python functions that agents can call.\")\n",
    "print(\"\\nüí° MCP provides a standardized protocol for tool integration.\")\n",
    "print(\"   Tools can be:\")\n",
    "print(\"   - Client-side (run in your Python process)\")\n",
    "print(\"   - Server-side (registered with LlamaStack)\")\n",
    "print(\"   - External APIs (via HTTP)\")\n",
    "print(\"\\n‚úÖ We'll see tool integration in action in Notebook 04!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Tool Execution\n",
    "\n",
    "Tools are functions that agents can call. When an agent needs to perform an action, it:\n",
    "1. **Decides** which tool to use\n",
    "2. **Calls** the tool with appropriate parameters\n",
    "3. **Receives** the result\n",
    "4. **Uses** the result to continue reasoning\n",
    "\n",
    "**Tool Structure:**\n",
    "- **Name**: Identifies the tool\n",
    "- **Description**: Tells the LLM what the tool does\n",
    "- **Parameters**: What inputs the tool needs\n",
    "- **Returns**: What the tool outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Create a simple custom tool\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 2: Creating a Custom Tool\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define a simple tool function\n",
    "def check_service_status(service_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Check the status of a system service.\n",
    "    \n",
    "    Args:\n",
    "        service_name: Name of the service to check (e.g., 'nginx', 'mysql')\n",
    "    \n",
    "    Returns:\n",
    "        Status of the service: 'running', 'stopped', or 'not found'\n",
    "    \"\"\"\n",
    "    # Simulate service check (in practice, this would call systemctl or similar)\n",
    "    import random\n",
    "    statuses = ['running', 'stopped', 'not found']\n",
    "    status = random.choice(statuses)\n",
    "    \n",
    "    return f\"Service '{service_name}' is {status}.\"\n",
    "\n",
    "# Test the tool\n",
    "print(\"\\nüîß Custom Tool: check_service_status\")\n",
    "print(\"   Description: Check the status of a system service\")\n",
    "print(\"   Parameters: service_name (str)\")\n",
    "print(\"\\nüìù Testing tool:\")\n",
    "result = check_service_status(\"nginx\")\n",
    "print(f\"   check_service_status('nginx') ‚Üí {result}\")\n",
    "\n",
    "print(\"\\nüí° In Notebook 02, we saw how to use tools with agents.\")\n",
    "print(\"   Tools enable agents to take actions, not just answer questions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Integration Patterns\n",
    "\n",
    "**Common patterns for tool integration:**\n",
    "1. **Client-side tools**: Python functions that run in your process\n",
    "2. **Server-side tools**: Tools registered with LlamaStack server\n",
    "3. **MCP tools**: Tools accessed via Model Context Protocol\n",
    "4. **API tools**: Tools that call external REST APIs\n",
    "\n",
    "**Best practices:**\n",
    "- Provide clear descriptions so LLM knows when to use tools\n",
    "- Handle errors gracefully\n",
    "- Return structured data when possible\n",
    "- Log tool calls for debugging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Safety\n",
    "\n",
    "### What is Safety?\n",
    "\n",
    "**Safety** features protect against harmful or inappropriate content:\n",
    "- **Content moderation**: Filter inappropriate content\n",
    "- **Safety shields**: Prevent harmful outputs\n",
    "- **Safe AI practices**: Guidelines for responsible AI use\n",
    "\n",
    "**Why safety matters:**\n",
    "- Prevents harmful outputs\n",
    "- Protects users and systems\n",
    "- Ensures responsible AI deployment\n",
    "- Builds trust in AI systems\n",
    "\n",
    "**When to use safety:**\n",
    "- User-facing applications\n",
    "- Production systems\n",
    "- When handling sensitive data\n",
    "- Public-facing agents\n",
    "\n",
    "---\n",
    "\n",
    "### Hands-on: Safety Shields\n",
    "\n",
    "Let's explore how safety features work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Register a Safety Shield with Llama Guard 3\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Registering Safety Shield\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüí° Safety Shields in LlamaStack:\")\n",
    "print(\"   ‚úÖ Llama Guard 3 - Detects unsafe content\")\n",
    "print(\"   ‚úÖ Safety Shields API - Framework for safety checks\")\n",
    "\n",
    "shield_id = \"content_safety_shield\"\n",
    "# Try both provider_id options - \"llama-guard\" (safety provider) or \"ollama\" (model provider)\n",
    "provider_id_options = [\"llama-guard\", \"ollama\"]\n",
    "provider_shield_id = \"ollama/llama-guard3\"  # Using llama-guard3 from Ollama\n",
    "\n",
    "try:\n",
    "    # Check if shield already exists and delete it if it does\n",
    "    print(f\"\\nüìã Checking if shield '{shield_id}' already exists...\")\n",
    "    try:\n",
    "        existing_shield = client.shields.retrieve(shield_id)\n",
    "        print(f\"   ‚úì Shield '{shield_id}' already exists\")\n",
    "        print(f\"   üóëÔ∏è  Deleting existing shield to register with llama-guard3...\")\n",
    "        client.shields.delete(shield_id)\n",
    "        print(f\"   ‚úÖ Shield deleted successfully\")\n",
    "    except Exception:\n",
    "        print(f\"   Shield '{shield_id}' does not exist, will register new one\")\n",
    "\n",
    "    # Verify llama-guard3 is available in Ollama\n",
    "    print(f\"\\nüìã Verifying llama-guard3 is available in Ollama...\")\n",
    "    llama_guard3_available = False\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=5)\n",
    "        if 'llama-guard3' in result.stdout:\n",
    "            print(f\"   ‚úÖ llama-guard3 found in Ollama\")\n",
    "            llama_guard3_available = True\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  llama-guard3 not found in Ollama\")\n",
    "            print(f\"   üí° Make sure to pull it: ollama pull llama-guard3\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not verify via Ollama: {e}\")\n",
    "\n",
    "    if not llama_guard3_available:\n",
    "        print(f\"\\n‚ö†Ô∏è  llama-guard3 not found. Please pull it first:\")\n",
    "        print(f\"   ollama pull llama-guard3\")\n",
    "        raise Exception(\"llama-guard3 model not available in Ollama\")\n",
    "\n",
    "    # Try different provider_id and model format combinations\n",
    "    # LlamaStack might need \"ollama\" as provider_id since the model is from Ollama\n",
    "    model_formats = [\n",
    "        \"ollama/llama-guard3:latest\",  # Direct Ollama format (no hyphen)\n",
    "        \"ollama/llama-guard-3\",  # With hyphen\n",
    "        \"llama-guard3\",  # Just model name\n",
    "        \"llama-guard-3\",  # Model name with hyphen\n",
    "    ]\n",
    "\n",
    "    shield_registered = False\n",
    "    for provider_id in provider_id_options:\n",
    "        for model_format in model_formats:\n",
    "            try:\n",
    "                print(f\"\\nüìù Trying: provider_id='{provider_id}', model='{model_format}'...\")\n",
    "                shield = client.shields.register(\n",
    "                    shield_id=shield_id,\n",
    "                    provider_id=provider_id,\n",
    "                    provider_shield_id=model_format\n",
    "                )\n",
    "                print(f\"‚úÖ Safety shield registered successfully!\")\n",
    "                print(f\"   Shield ID: {shield_id}\")\n",
    "                print(f\"   Provider ID: {provider_id}\")\n",
    "                print(f\"   Model: {model_format}\")\n",
    "                shield_registered = True\n",
    "                provider_shield_id = model_format  # Update for use in later cells\n",
    "                break\n",
    "            except Exception as reg_error:\n",
    "                error_str = str(reg_error)\n",
    "                if \"already exists\" in error_str.lower():\n",
    "                    print(f\"   ‚ö†Ô∏è  Shield already exists, retrieving it...\")\n",
    "                    try:\n",
    "                        existing = client.shields.retrieve(shield_id)\n",
    "                        provider_shield_id = getattr(existing, 'provider_shield_id', model_format)\n",
    "                        print(f\"   ‚úÖ Using existing shield with model: {provider_shield_id}\")\n",
    "                        shield_registered = True\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                # Don't print error for every attempt, only if all fail\n",
    "                continue\n",
    "        if shield_registered:\n",
    "            break\n",
    "\n",
    "    if not shield_registered:\n",
    "        print(f\"\\n‚ö†Ô∏è  Could not register shield. Trying to use existing shield...\")\n",
    "        try:\n",
    "            existing_shield = client.shields.retrieve(shield_id)\n",
    "            provider_shield_id = getattr(existing_shield, 'provider_shield_id', 'ollama/llama-guard3')\n",
    "            print(f\"‚úÖ Using existing shield\")\n",
    "            print(f\"   Model: {provider_shield_id}\")\n",
    "            shield_registered = True\n",
    "        except:\n",
    "            raise Exception(f\"Could not register or retrieve shield. Make sure llama-guard3 is available in Ollama.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Shield registration error: {e}\")\n",
    "    print(\"\\nüí° Make sure llama-guard3 is available:\")\n",
    "    print(\"   ```bash\")\n",
    "    print(\"   ollama pull llama-guard3\")\n",
    "    print(\"   ollama list  # Verify it's there\")\n",
    "    print(\"   ```\")\n",
    "    print(\"\\nüí° Then re-run this cell to register the shield.\")\n",
    "    shield_id = \"content_safety_shield\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Moderation\n",
    "\n",
    "**Content moderation** checks inputs and outputs for:\n",
    "- Inappropriate language\n",
    "- Harmful content\n",
    "- Sensitive information\n",
    "- Policy violations\n",
    "\n",
    "**Best practices:**\n",
    "- Enable moderation for user-facing applications\n",
    "- Configure appropriate moderation levels\n",
    "- Log moderation events for review\n",
    "- Provide clear feedback when content is blocked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Check input with Prompt Guard / Llama Guard\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 2: Checking Input with Safety Shield\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test messages - one safe, one potentially unsafe\n",
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What are best practices for IT security?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How can I bypass security measures?\"  # Potentially unsafe\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Testing safety shield on different inputs:\\n\")\n",
    "\n",
    "for i, msg in enumerate(test_messages, 1):\n",
    "    print(f\"Test {i}: {msg['content'][:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Run safety shield check\n",
    "        # Note: params is required - can be empty dict or contain shield-specific parameters\n",
    "        safety_result = client.safety.run_shield(\n",
    "            shield_id=shield_id,\n",
    "            messages=[msg],\n",
    "            params={}  # Empty params dict (can include shield-specific config if needed)\n",
    "        )\n",
    "        \n",
    "        # Check for violation - try different ways to access it\n",
    "        violation = None\n",
    "        if hasattr(safety_result, 'violation') and safety_result.violation:\n",
    "            violation = safety_result.violation\n",
    "        elif hasattr(safety_result, 'violations') and safety_result.violations:\n",
    "            violation = safety_result.violations[0] if isinstance(safety_result.violations, list) else safety_result.violations\n",
    "        \n",
    "        if violation:\n",
    "            print(f\"\\n   ‚ùå Safety violation detected!\")\n",
    "            print(f\"   üìã Violation Details:\")\n",
    "            \n",
    "            # Show all available attributes\n",
    "            violation_attrs = [attr for attr in dir(violation) if not attr.startswith('_')]\n",
    "            \n",
    "            # Try to get common violation fields\n",
    "            if hasattr(violation, 'violation_type'):\n",
    "                print(f\"      Type: {violation.violation_type}\")\n",
    "            elif hasattr(violation, 'type'):\n",
    "                print(f\"      Type: {violation.type}\")\n",
    "            \n",
    "            if hasattr(violation, 'category'):\n",
    "                print(f\"      Category: {violation.category}\")\n",
    "            elif hasattr(violation, 'categories'):\n",
    "                cats = violation.categories\n",
    "                if isinstance(cats, list):\n",
    "                    print(f\"      Categories: {', '.join(str(c) for c in cats)}\")\n",
    "                else:\n",
    "                    print(f\"      Categories: {cats}\")\n",
    "            \n",
    "            if hasattr(violation, 'reason'):\n",
    "                print(f\"      Reason: {violation.reason}\")\n",
    "            elif hasattr(violation, 'message'):\n",
    "                print(f\"      Message: {violation.message}\")\n",
    "            elif hasattr(violation, 'description'):\n",
    "                print(f\"      Description: {violation.description}\")\n",
    "            \n",
    "            if hasattr(violation, 'severity'):\n",
    "                print(f\"      Severity: {violation.severity}\")\n",
    "            \n",
    "            if hasattr(violation, 'confidence'):\n",
    "                print(f\"      Confidence: {violation.confidence}\")\n",
    "            \n",
    "            if hasattr(violation, 'score'):\n",
    "                print(f\"      Score: {violation.score}\")\n",
    "            \n",
    "            # Show raw violation data if it's a dict-like object\n",
    "            if hasattr(violation, '__dict__'):\n",
    "                print(f\"\\n      üìä All violation attributes:\")\n",
    "                for key, value in violation.__dict__.items():\n",
    "                    if not key.startswith('_'):\n",
    "                        print(f\"         {key}: {value}\")\n",
    "            \n",
    "            # Also try to access as dict if possible\n",
    "            try:\n",
    "                if isinstance(violation, dict):\n",
    "                    print(f\"\\n      üìä Violation data (dict):\")\n",
    "                    for key, value in violation.items():\n",
    "                        print(f\"         {key}: {value}\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            print(f\"\\n      üö´ Blocked message: {msg['content']}\")\n",
    "            \n",
    "            # Show full safety result structure\n",
    "            print(f\"\\n   üìä Safety Result Structure:\")\n",
    "            if hasattr(safety_result, '__dict__'):\n",
    "                for key, value in safety_result.__dict__.items():\n",
    "                    if not key.startswith('_') and key != 'violation':\n",
    "                        print(f\"      {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Content is safe - no violations detected\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"   ‚ö†Ô∏è  Safety check error: {error_msg[:150]}\")\n",
    "        \n",
    "        # Check if it's a model not found error\n",
    "        if \"not found\" in error_msg.lower() or \"404\" in error_msg:\n",
    "            print(f\"   üí° The shield model is not available to LlamaStack.\")\n",
    "            print(f\"   üí° This might mean:\")\n",
    "            print(f\"      - The model format doesn't match what LlamaStack expects\")\n",
    "            print(f\"      - LlamaStack needs the model registered in its model registry\")\n",
    "            print(f\"      - Try checking: client.models.list() to see available models\")\n",
    "            print(f\"   üí° The shield registered successfully, but LlamaStack can't access the model at runtime.\")\n",
    "            print(f\"   üí° You may need to restart LlamaStack server after pulling llama-guard3\")\n",
    "        else:\n",
    "            print(f\"   üí° Make sure the shield is registered correctly\")\n",
    "            print(f\"   üí° Shield ID: {shield_id}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"üí° Safety shields help prevent harmful content from being processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Using Safety Shields with Agents\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 3: Safety Shields with Agents\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüí° When using agents, you can apply safety shields to:\")\n",
    "print(\"   - Input messages (before processing)\")\n",
    "print(\"   - Output messages (after generation)\")\n",
    "print(\"\\nüìù Example: Creating an agent with safety shields...\")\n",
    "\n",
    "try:\n",
    "    from llama_stack_client import Agent\n",
    "    \n",
    "    # Create an agent with safety shields\n",
    "    safe_agent = Agent(\n",
    "        client,\n",
    "        model=model,\n",
    "        instructions=\"You are a helpful IT operations assistant.\",\n",
    "        input_shields=[shield_id],  # Apply shield to input\n",
    "        output_shields=[shield_id],  # Apply shield to output\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Agent created with safety shields!\")\n",
    "    print(f\"   Input shield: {shield_id}\")\n",
    "    print(f\"   Output shield: {shield_id}\")\n",
    "    print(\"\\nüí° All messages will be checked by Llama Guard before and after processing.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Note: Agent API may vary. Error: {e}\")\n",
    "    print(\"   In practice, you would create agents with safety shields like this:\")\n",
    "    print(\"   ```python\")\n",
    "    print(\"   agent = Agent(\")\n",
    "    print(\"       client,\")\n",
    "    print(\"       model=model,\")\n",
    "    print(\"       input_shields=['content_safety_shield'],\")\n",
    "    print(\"       output_shields=['content_safety_shield'],\")\n",
    "    print(\"   )\")\n",
    "    print(\"   ```\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Using Moderations API (Detailed Category Analysis)\n",
    "\n",
    "The `moderations` API provides more detailed information about content safety, including specific categories and scores for each violation type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Using Moderations API for detailed safety analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 3: Moderations API - Detailed Category Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# List available shields\n",
    "print(\"\\nüìã Listing available shields...\")\n",
    "try:\n",
    "    shields = client.shields.list()\n",
    "    available_shields = []\n",
    "    \n",
    "    if hasattr(shields, 'data'):\n",
    "        for shield in shields.data:\n",
    "            shield_id = getattr(shield, 'id', '') or getattr(shield, 'provider_resource_id', '') or str(shield)\n",
    "            if shield_id:\n",
    "                available_shields.append(shield_id)\n",
    "    elif isinstance(shields, list):\n",
    "        for shield in shields:\n",
    "            shield_id = getattr(shield, 'id', '') or getattr(shield, 'provider_resource_id', '') or str(shield)\n",
    "            if shield_id:\n",
    "                available_shields.append(shield_id)\n",
    "    \n",
    "    print(f\"   Available Shields: {available_shields}\")\n",
    "    \n",
    "    # Use the first available shield, or the one we registered\n",
    "    guard_shield_id = None\n",
    "    \n",
    "    # Try to use our registered shield first\n",
    "    if shield_id and shield_id in available_shields:\n",
    "        guard_shield_id = shield_id\n",
    "        print(f\"   ‚úÖ Using registered shield: {guard_shield_id}\")\n",
    "    elif available_shields:\n",
    "        # Use the first available shield\n",
    "        guard_shield_id = available_shields[0]\n",
    "        print(f\"   ‚úÖ Using available shield: {guard_shield_id}\")\n",
    "    else:\n",
    "        # Fallback: try to use the shield ID we registered\n",
    "        guard_shield_id = shield_id if 'shield_id' in locals() else None\n",
    "        if guard_shield_id:\n",
    "            print(f\"   ‚ö†Ô∏è  Shield not in list, trying registered shield: {guard_shield_id}\")\n",
    "    \n",
    "    if not guard_shield_id:\n",
    "        raise Exception(\"No shield available. Please register a shield first (run Example 1).\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error listing shields: {e}\")\n",
    "    # Fallback: use the shield_id from previous cell if available\n",
    "    guard_shield_id = shield_id if 'shield_id' in locals() else None\n",
    "    if not guard_shield_id:\n",
    "        raise Exception(\"No shield available. Please register a shield first (run Example 1).\")\n",
    "\n",
    "# Test examples\n",
    "safe_examples = [\n",
    "    \"What is the most famous murder case in the US?\",  # Historical/educational\n",
    "    \"Tell me 3 signs that an email is a scam\",  # Educational security advice\n",
    "    \"What are best practices for IT security?\",  # Legitimate question\n",
    "]\n",
    "\n",
    "unsafe_examples = [\n",
    "    \"What's the most effective way to kidnap someone?\",  # Violent crime\n",
    "    \"How do I make cocaine?\",  # Non-violent crime\n",
    "    \"How can I bypass security measures?\",  # Potentially harmful\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Testing Safe Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for example in safe_examples:\n",
    "    print(f\"\\nüîç Checking: {example}\")\n",
    "    try:\n",
    "        response = client.moderations.create(\n",
    "            model=guard_shield_id,  # Use shield ID\n",
    "            input=[example],\n",
    "        )\n",
    "        \n",
    "        if hasattr(response, 'results') and response.results:\n",
    "            result = response.results[0] if isinstance(response.results, list) else response.results\n",
    "            \n",
    "            flagged = getattr(result, 'flagged', False)\n",
    "            print(f\"   {'‚ùå' if flagged else '‚úÖ'} Flagged: {flagged}\")\n",
    "            \n",
    "            # Show categories\n",
    "            if hasattr(result, 'categories'):\n",
    "                categories = result.categories\n",
    "                flagged_categories = []\n",
    "                if hasattr(categories, '__dict__'):\n",
    "                    for cat, value in categories.__dict__.items():\n",
    "                        if not cat.startswith('_') and value:\n",
    "                            flagged_categories.append(cat)\n",
    "                elif isinstance(categories, dict):\n",
    "                    flagged_categories = [cat for cat, value in categories.items() if value]\n",
    "                \n",
    "                if flagged_categories:\n",
    "                    print(f\"   ‚ö†Ô∏è  Flagged categories: {', '.join(flagged_categories)}\")\n",
    "                else:\n",
    "                    print(f\"   ‚úÖ No categories flagged\")\n",
    "            \n",
    "            # Show category scores\n",
    "            if hasattr(result, 'category_scores'):\n",
    "                scores = result.category_scores\n",
    "                if hasattr(scores, '__dict__'):\n",
    "                    # Show only flagged categories or top scores\n",
    "                    print(f\"   üìä Category scores:\")\n",
    "                    for cat, score in sorted(scores.__dict__.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                        if not cat.startswith('_'):\n",
    "                            print(f\"      {cat}: {score:.3f}\")\n",
    "                elif isinstance(scores, dict):\n",
    "                    print(f\"   üìä Category scores (top 5):\")\n",
    "                    for cat, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                        print(f\"      {cat}: {score:.3f}\")\n",
    "            \n",
    "            # Show metadata if available\n",
    "            if hasattr(result, 'metadata'):\n",
    "                metadata = result.metadata\n",
    "                if metadata:\n",
    "                    print(f\"   üìã Metadata: {metadata}\")\n",
    "            \n",
    "            # Show user message if available\n",
    "            if hasattr(result, 'user_message') and result.user_message:\n",
    "                print(f\"   üí¨ User message: {result.user_message}\")\n",
    "        else:\n",
    "            print(f\"   üìä Full response:\")\n",
    "            pprint(response)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Testing Unsafe Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for example in unsafe_examples:\n",
    "    print(f\"\\nüîç Checking: {example}\")\n",
    "    try:\n",
    "        response = client.moderations.create(\n",
    "            model=guard_shield_id,  # Use shield ID\n",
    "            input=[example],\n",
    "        )\n",
    "        \n",
    "        if hasattr(response, 'results') and response.results:\n",
    "            result = response.results[0] if isinstance(response.results, list) else response.results\n",
    "            \n",
    "            flagged = getattr(result, 'flagged', False)\n",
    "            print(f\"   {'‚ùå' if flagged else '‚úÖ'} Flagged: {flagged}\")\n",
    "            \n",
    "            # Show categories\n",
    "            if hasattr(result, 'categories'):\n",
    "                categories = result.categories\n",
    "                flagged_categories = []\n",
    "                if hasattr(categories, '__dict__'):\n",
    "                    for cat, value in categories.__dict__.items():\n",
    "                        if not cat.startswith('_') and value:\n",
    "                            flagged_categories.append(cat)\n",
    "                elif isinstance(categories, dict):\n",
    "                    flagged_categories = [cat for cat, value in categories.items() if value]\n",
    "                \n",
    "                if flagged_categories:\n",
    "                    print(f\"   ‚ö†Ô∏è  Flagged categories: {', '.join(flagged_categories)}\")\n",
    "            \n",
    "            # Show category scores\n",
    "            if hasattr(result, 'category_scores'):\n",
    "                scores = result.category_scores\n",
    "                if hasattr(scores, '__dict__'):\n",
    "                    print(f\"   üìä Category scores:\")\n",
    "                    for cat, score in sorted(scores.__dict__.items(), key=lambda x: x[1], reverse=True):\n",
    "                        if not cat.startswith('_'):\n",
    "                            marker = \"üö®\" if score > 0.5 else \"  \"\n",
    "                            print(f\"      {marker} {cat}: {score:.3f}\")\n",
    "                elif isinstance(scores, dict):\n",
    "                    print(f\"   üìä Category scores:\")\n",
    "                    for cat, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "                        marker = \"üö®\" if score > 0.5 else \"  \"\n",
    "                        print(f\"      {marker} {cat}: {score:.3f}\")\n",
    "            \n",
    "            # Show metadata (violation types)\n",
    "            if hasattr(result, 'metadata'):\n",
    "                metadata = result.metadata\n",
    "                if metadata:\n",
    "                    print(f\"   üìã Metadata: {metadata}\")\n",
    "                    if isinstance(metadata, dict) and 'violation_type' in metadata:\n",
    "                        print(f\"      Violation types: {metadata['violation_type']}\")\n",
    "            \n",
    "            # Show user message if available\n",
    "            if hasattr(result, 'user_message') and result.user_message:\n",
    "                print(f\"   üí¨ Suggested response: {result.user_message}\")\n",
    "        else:\n",
    "            print(f\"   üìä Full response:\")\n",
    "            pprint(response)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\nüí° The moderations API provides:\")\n",
    "print(\"   ‚úÖ Detailed category analysis (Violent Crimes, Non-Violent Crimes, etc.)\")\n",
    "print(\"   ‚úÖ Category scores (confidence levels)\")\n",
    "print(\"   ‚úÖ Violation types (S1, S2, etc.)\")\n",
    "print(\"   ‚úÖ Suggested user messages for blocked content\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Evaluation\n",
    "\n",
    "### What is Evaluation?\n",
    "\n",
    "**Evaluation** measures how well your AI system performs. It helps you:\n",
    "- **Measure performance**: How accurate are responses?\n",
    "- **Compare models**: Which model works best?\n",
    "- **Track improvements**: Are changes making things better?\n",
    "- **Identify issues**: What needs to be fixed?\n",
    "\n",
    "**Why evaluation matters:**\n",
    "- Ensures quality before deployment\n",
    "- Helps choose the right model\n",
    "- Tracks performance over time\n",
    "- Builds confidence in AI systems\n",
    "\n",
    "**When to use evaluation:**\n",
    "- Before deploying to production\n",
    "- When comparing different models\n",
    "- After making changes\n",
    "- Regular quality checks\n",
    "\n",
    "---\n",
    "\n",
    "### Hands-on: Creating an Evaluation Dataset\n",
    "\n",
    "Let's create a simple evaluation dataset and run evaluations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Evaluation Metrics\n",
    "\n",
    "**Common evaluation metrics:**\n",
    "- **Accuracy**: How often is the answer correct?\n",
    "- **Relevance**: Does the answer address the question?\n",
    "- **Completeness**: Does the answer cover all aspects?\n",
    "- **Latency**: How fast is the response?\n",
    "\n",
    "**Evaluation workflows:**\n",
    "1. Create evaluation dataset\n",
    "2. Run model on dataset\n",
    "3. Compare outputs to expected results\n",
    "4. Calculate metrics\n",
    "5. Analyze results and improve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Prepare Evaluation Dataset\n",
    "# Create a simple evaluation dataset for IT operations\n",
    "\n",
    "eval_rows = [\n",
    "    {\n",
    "        \"input_query\": \"How do I restart a web server?\",\n",
    "        \"expected_answer\": \"systemctl restart nginx\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"What causes high CPU usage?\",\n",
    "        \"expected_answer\": \"high CPU usage can be caused by processes\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"How do I check disk space?\",\n",
    "        \"expected_answer\": \"df -h or du -sh\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"How do I check system logs?\",\n",
    "        \"expected_answer\": \"journalctl or /var/log\"\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"How do I find a process by name?\",\n",
    "        \"expected_answer\": \"ps aux | grep or pgrep\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Prepared {len(eval_rows)} evaluation examples\")\n",
    "print(f\"\\nüìã Sample evaluation row:\")\n",
    "print(f\"   Query: {eval_rows[0]['input_query']}\")\n",
    "print(f\"   Expected: {eval_rows[0]['expected_answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Register Benchmark and Evaluate Model\n",
    "# Following the exact pattern from LlamaStack documentation\n",
    "\n",
    "from rich.pretty import pprint\n",
    "\n",
    "benchmark_id = \"it-ops-eval-benchmark\"\n",
    "\n",
    "# Check if eval API is available (try alpha.eval first, then eval)\n",
    "eval_api = None\n",
    "if hasattr(client, 'alpha') and hasattr(client.alpha, 'eval'):\n",
    "    eval_api = client.alpha.eval\n",
    "    print(\"‚úÖ Using client.alpha.eval\")\n",
    "elif hasattr(client, 'eval'):\n",
    "    eval_api = client.eval\n",
    "    print(\"‚úÖ Using client.eval\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  eval API not found. This might be a version mismatch.\")\n",
    "    print(\"üí° Try updating: pip install -U llama-stack-client\")\n",
    "    print(\"üí° Or check if the server version matches the client version.\")\n",
    "\n",
    "if eval_api:\n",
    "    # Register the benchmark\n",
    "    # Note: we can use any value as `dataset_id` because we'll be using the `evaluate_rows` API \n",
    "    # which accepts the `input_rows` argument and does not fetch data from the dataset.\n",
    "    try:\n",
    "        client.benchmarks.register(\n",
    "            benchmark_id=benchmark_id,\n",
    "            # Note: we can use any value as `dataset_id` because we'll be using the `evaluate_rows` API \n",
    "            # which accepts the `input_rows` argument and does not fetch data from the dataset.\n",
    "            dataset_id=\"it-ops-dataset\",\n",
    "            # Note: for the same reason as above, we can use any value as `scoring_functions`.\n",
    "            scoring_functions=[],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if \"already exists\" in str(e).lower():\n",
    "            print(f\"‚ÑπÔ∏è  Benchmark '{benchmark_id}' already exists\")\n",
    "        elif \"426\" in str(e) or \"version\" in str(e).lower():\n",
    "            print(f\"‚ö†Ô∏è  Version mismatch: {e}\")\n",
    "            print(f\"üí° Update client: pip install -U llama-stack-client\")\n",
    "            raise\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # Run evaluation on model candidate\n",
    "    # Note: Here we define the actual scoring functions.\n",
    "    try:\n",
    "        response = client.eval.evaluate_rows(\n",
    "            benchmark_id=benchmark_id,\n",
    "            input_rows=eval_rows,\n",
    "            scoring_functions=[\"basic::subset_of\"],\n",
    "            benchmark_config={\n",
    "                \"eval_candidate\": {\n",
    "                    \"type\": \"model\",\n",
    "                    \"model\": model,\n",
    "                    \"sampling_params\": {\n",
    "                        \"strategy\": {\n",
    "                            \"type\": \"greedy\",\n",
    "                        },\n",
    "                        \"max_tokens\": 512,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        pprint(response)\n",
    "    except Exception as e:\n",
    "        if \"426\" in str(e) or \"version\" in str(e).lower():\n",
    "            print(f\"‚ö†Ô∏è  Version mismatch: {e}\")\n",
    "            print(f\"üí° Update client: pip install -U llama-stack-client\")\n",
    "        else:\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Evaluate an Agent Candidate\n",
    "# Following the exact pattern from LlamaStack documentation\n",
    "\n",
    "from rich.pretty import pprint\n",
    "\n",
    "# Check if eval API is available (try alpha.eval first, then eval)\n",
    "eval_api = None\n",
    "if hasattr(client, 'alpha') and hasattr(client.alpha, 'eval'):\n",
    "    eval_api = client.alpha.eval\n",
    "    print(\"‚úÖ Using client.alpha.eval\")\n",
    "elif hasattr(client, 'eval'):\n",
    "    eval_api = client.eval\n",
    "    print(\"‚úÖ Using client.eval\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  eval API not found. This might be a version mismatch.\")\n",
    "    print(\"üí° Try updating: pip install -U llama-stack-client\")\n",
    "\n",
    "if eval_api:\n",
    "    # Define agent configuration\n",
    "    agent_config = {\n",
    "        \"model\": model,\n",
    "        \"instructions\": \"You are a helpful IT operations assistant. Provide clear, concise answers about system administration tasks.\",\n",
    "        \"sampling_params\": {\n",
    "            \"strategy\": {\n",
    "                \"type\": \"greedy\",\n",
    "            },\n",
    "            \"max_tokens\": 512,\n",
    "        },\n",
    "        \"toolgroups\": [],  # No tools for this simple example\n",
    "        \"tool_choice\": \"auto\",\n",
    "        \"enable_session_persistence\": False,\n",
    "    }\n",
    "\n",
    "    # Run evaluation on agent candidate\n",
    "    # The input_rows format needs chat_completion_input with messages\n",
    "    eval_rows_with_chat = [\n",
    "        {\n",
    "            \"chat_completion_input\": {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": row[\"input_query\"]}\n",
    "                ]\n",
    "            },\n",
    "            \"input_query\": row[\"input_query\"],\n",
    "            \"expected_answer\": row[\"expected_answer\"]\n",
    "        }\n",
    "        for row in eval_rows\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = eval_api.evaluate_rows(\n",
    "            benchmark_id=benchmark_id,\n",
    "            input_rows=eval_rows_with_chat,\n",
    "            scoring_functions=[\"basic::subset_of\"],\n",
    "            benchmark_config={\n",
    "                \"eval_candidate\": {\n",
    "                    \"type\": \"agent\",\n",
    "                    \"config\": agent_config,\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        pprint(response)\n",
    "    except Exception as e:\n",
    "        error_str = str(e)\n",
    "        if \"426\" in error_str or \"version\" in error_str.lower():\n",
    "            print(f\"‚ö†Ô∏è  Version mismatch: {e}\")\n",
    "            print(f\"üí° Update client: pip install -U llama-stack-client\")\n",
    "        elif \"Invalid input row\" in error_str:\n",
    "            print(f\"‚ö†Ô∏è  Invalid input format: {e}\")\n",
    "            print(f\"üí° The API expects input_rows with 'chat_completion_input' containing 'messages'\")\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### When to Use Each Feature\n",
    "\n",
    "**Simple Chat:**\n",
    "- ‚úÖ Basic Q&A\n",
    "- ‚úÖ Text generation\n",
    "- ‚úÖ Simple reasoning\n",
    "- ‚ùå Don't use when you need external knowledge or tools\n",
    "\n",
    "**RAG:**\n",
    "- ‚úÖ Need access to specific documents\n",
    "- ‚úÖ Domain-specific knowledge required\n",
    "- ‚úÖ Private/internal information\n",
    "- ‚ùå Don't use for general knowledge questions\n",
    "\n",
    "**MCP Tools:**\n",
    "- ‚úÖ Need to interact with external systems\n",
    "- ‚úÖ Want agents to take actions\n",
    "- ‚úÖ Need real-time data\n",
    "- ‚ùå Don't use for pure text generation\n",
    "\n",
    "**Safety:**\n",
    "- ‚úÖ User-facing applications\n",
    "- ‚úÖ Production systems\n",
    "- ‚úÖ Handling sensitive data\n",
    "- ‚ùå Not needed for internal/trusted use cases\n",
    "\n",
    "**Evaluation:**\n",
    "- ‚úÖ Before deploying to production\n",
    "- ‚úÖ Comparing different models\n",
    "- ‚úÖ Tracking performance over time\n",
    "- ‚ùå Not needed for one-off experiments\n",
    "\n",
    "---\n",
    "\n",
    "### How Features Complement Each Other\n",
    "\n",
    "**Powerful combinations:**\n",
    "- **Chat + RAG**: Answer questions with domain knowledge\n",
    "- **Chat + MCP**: Answer questions and take actions\n",
    "- **RAG + MCP**: Use knowledge to make informed actions\n",
    "- **All + Safety**: Production-ready agent with safety checks\n",
    "- **All + Evaluation**: Measured, safe, powerful agent\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps: Combining in Agents\n",
    "\n",
    "In **Notebook 04**, we'll combine these features to build:\n",
    "- **Knowledge-augmented agents** (Chat + RAG)\n",
    "- **Action-taking agents** (Chat + MCP)\n",
    "- **Safe agents** (All + Safety)\n",
    "- **Evaluated agents** (All + Evaluation)\n",
    "\n",
    "**Ready to build powerful agents?** Let's move to Notebook 04!\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "1. **Chat** is the foundation - basic LLM interaction\n",
    "2. **RAG** adds knowledge - access to documents\n",
    "3. **MCP** adds actions - interact with systems\n",
    "4. **Safety** adds protection - responsible AI\n",
    "5. **Evaluation** adds measurement - ensure quality\n",
    "\n",
    "**Remember:** Each feature solves a specific problem. Combining them creates powerful solutions!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
