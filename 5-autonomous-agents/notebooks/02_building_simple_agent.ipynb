{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Building a Simple Agent with Tools\n",
    "\n",
    "## üéØ What is This Notebook About?\n",
    "\n",
    "In this notebook, we'll build a simple autonomous agent **step by step** using the LlamaStack SDK directly. You'll see exactly how each component works.\n",
    "\n",
    "**What we'll learn:**\n",
    "1. How to connect to LlamaStack using the Python SDK\n",
    "2. How to define tools in the correct format (OpenAI function calling)\n",
    "3. How to create an agent with tools using the SDK\n",
    "4. How to create agent sessions\n",
    "5. How to execute turns and see the agent's reasoning\n",
    "6. How to process streaming responses from the agent\n",
    "\n",
    "**Why this matters:**\n",
    "- You'll understand the **actual API calls** and responses\n",
    "- You'll see **raw LlamaStack outputs** to understand how it works\n",
    "- You'll learn the **fundamentals** before using abstractions\n",
    "- This foundation prepares you for more advanced agents\n",
    "\n",
    "**Note:** We'll use the SDK directly here. In later notebooks, we'll show how abstractions can simplify this.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Know how to use LlamaStack SDK directly\n",
    "- ‚úÖ Understand the exact format for tools (OpenAI function calling)\n",
    "- ‚úÖ Be able to create agents, sessions, and turns manually\n",
    "- ‚úÖ See and understand agent streaming responses\n",
    "- ‚úÖ Know how to process tool calls and responses\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Prerequisites\n",
    "\n",
    "- LlamaStack server running (see Module README)\n",
    "- Ollama running with llama3.2:3b model\n",
    "- Python environment with dependencies installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for imports (we'll use some helper functions)\n",
    "notebook_dir = Path().resolve()\n",
    "src_path = notebook_dir.parent / 'src'\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import LlamaStack SDK - this is what we'll use directly!\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# Import helper modules for environment and tools\n",
    "from environment import SimulatedEnvironment\n",
    "from tools import ToolRegistry\n",
    "\n",
    "# Configuration\n",
    "llamastack_url = os.getenv(\"LLAMA_STACK_URL\", \"http://localhost:8321\")\n",
    "model = os.getenv(\"LLAMA_MODEL\", \"ollama/llama3.2:3b\")\n",
    "\n",
    "print(f\"üì° LlamaStack URL: {llamastack_url}\")\n",
    "print(f\"ü§ñ Model: {model}\")\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "client = LlamaStackClient(base_url=llamastack_url)\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    print(f\"\\n‚úÖ Connected to LlamaStack\")\n",
    "    print(f\"   Available models: {len(models)}\")\n",
    "    if models:\n",
    "        print(f\"   Using model: {model}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Cannot connect to LlamaStack: {e}\")\n",
    "    print(\"   Please ensure LlamaStack is running:\")\n",
    "    print(\"   python scripts/start_llama_stack.py\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Tools\n",
    "\n",
    "### What are Tools?\n",
    "\n",
    "**Tools** are the actions an agent can take. They're defined in a specific format that LlamaStack understands.\n",
    "\n",
    "**Key Points:**\n",
    "- Tools use **OpenAI function calling format**\n",
    "- Each tool has: `type`, `function` (with `name`, `description`, `parameters`)\n",
    "- Tools are passed to the agent when creating it\n",
    "- The agent uses LLM reasoning to decide which tool to call\n",
    "\n",
    "Let's see how tools are structured!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a simulated environment\n",
    "# This gives us a safe place to test our agent\n",
    "env = SimulatedEnvironment()\n",
    "print(\"‚úÖ Created simulated environment\")\n",
    "print(f\"   Services: {list(env.services.keys())}\")\n",
    "\n",
    "# Step 2: Create tools using our helper\n",
    "# We'll use ToolRegistry to help us create tools, but we'll see the actual format\n",
    "tool_registry = ToolRegistry(env)\n",
    "print(f\"\\n‚úÖ Created tool registry\")\n",
    "print(f\"   Tools available: {len(tool_registry.list_tools())}\")\n",
    "\n",
    "# Step 3: Get tools in LlamaStack format\n",
    "# This is the IMPORTANT part - see the exact format!\n",
    "tools = tool_registry.get_tools_for_llamastack()\n",
    "\n",
    "print(f\"\\nüìã Tools in LlamaStack format (OpenAI function calling):\")\n",
    "print(f\"   Number of tools: {len(tools)}\")\n",
    "print(f\"\\n   First tool structure:\")\n",
    "print(json.dumps(tools[0], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Tool Format\n",
    "\n",
    "Notice the structure:\n",
    "- **`type: \"function\"`** - This tells LlamaStack it's a function tool\n",
    "- **`function`** - Contains the tool definition\n",
    "  - **`name`** - The tool identifier\n",
    "  - **`description`** - What the tool does (the LLM uses this to decide when to call it)\n",
    "  - **`parameters`** - JSON Schema defining the tool's inputs\n",
    "\n",
    "This is the **OpenAI function calling format** that LlamaStack uses. The agent's LLM reads these descriptions and decides which tools to call based on the task.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Creating an Agent\n",
    "\n",
    "Now let's create an agent using the LlamaStack SDK directly. We'll see the exact API call and response!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define agent instructions\n",
    "# These tell the agent how to behave and what to do\n",
    "instructions = \"\"\"You are an autonomous IT operations agent. Your job is to monitor IT services, identify problems, and take corrective actions.\n",
    "\n",
    "When analyzing IT services:\n",
    "1. First, check the status of services to understand the current state\n",
    "2. Identify any problems (failed services, high CPU/memory, degraded performance)\n",
    "3. Take appropriate corrective actions (restart failed services, scale overloaded services)\n",
    "4. Verify that actions were successful\n",
    "5. Provide a clear summary of what was done\n",
    "\n",
    "Always be careful and thoughtful. Only take actions that are necessary and safe.\n",
    "If you're unsure about an action, explain your reasoning.\"\"\"\n",
    "\n",
    "print(\"üìù Agent Instructions:\")\n",
    "print(instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create the agent using LlamaStack SDK\n",
    "# This is the ACTUAL API call - see what happens!\n",
    "print(\"=\" * 60)\n",
    "print(\"Creating Agent with LlamaStack SDK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì§ Sending request to: {llamastack_url}/v1alpha/agents\")\n",
    "print(f\"   Model: {model}\")\n",
    "print(f\"   Tools: {len(tools)}\")\n",
    "print(f\"\\n   Payload structure:\")\n",
    "print(f\"   {{\")\n",
    "print(f\"     'agent_config': {{\")\n",
    "print(f\"       'model': '{model}',\")\n",
    "print(f\"       'instructions': '...',\")\n",
    "print(f\"       'tools': [list of {len(tools)} tools]\")\n",
    "print(f\"     }}\")\n",
    "print(f\"   }}\")\n",
    "\n",
    "# Create the agent - this is the actual SDK call\n",
    "agent_response = client.alpha.agents.create(\n",
    "    agent_config={\n",
    "        \"model\": model,\n",
    "        \"instructions\": instructions,\n",
    "        \"tools\": tools\n",
    "    }\n",
    ")\n",
    "\n",
    "# See the response!\n",
    "print(f\"\\nüì• Response received:\")\n",
    "print(f\"   Type: {type(agent_response)}\")\n",
    "print(f\"   Agent ID: {agent_response.agent_id}\")\n",
    "\n",
    "# Store the agent_id - we'll need it!\n",
    "agent_id = agent_response.agent_id\n",
    "print(f\"\\n‚úÖ Agent created successfully!\")\n",
    "print(f\"   Agent ID: {agent_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "1. **We called `client.alpha.agents.create()`** - This is the SDK method for creating agents\n",
    "2. **We passed `agent_config`** with:\n",
    "   - `model`: Which LLM to use\n",
    "   - `instructions`: How the agent should behave\n",
    "   - `tools`: What actions the agent can take\n",
    "3. **LlamaStack created the agent** and returned an `agent_id`\n",
    "4. **The agent is now ready** to receive tasks!\n",
    "\n",
    "**Important:** The agent exists on the LlamaStack server. We'll use the `agent_id` to interact with it.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Creating a Session\n",
    "\n",
    "Before we can give the agent a task, we need to create a **session**. A session is like a conversation thread - it maintains context for the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent session\n",
    "# Sessions maintain conversation context\n",
    "print(\"=\" * 60)\n",
    "print(\"Creating Agent Session\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "session_name = f\"session-{int(time.time())}\"\n",
    "print(f\"\\nüì§ Creating session: {session_name}\")\n",
    "\n",
    "# Create session using SDK\n",
    "session_response = client.alpha.agents.session.create(\n",
    "    agent_id=agent_id,\n",
    "    session_name=session_name\n",
    ")\n",
    "\n",
    "# See the response\n",
    "print(f\"\\nüì• Session response:\")\n",
    "print(f\"   Type: {type(session_response)}\")\n",
    "print(f\"   Session ID: {session_response.session_id}\")\n",
    "\n",
    "session_id = session_response.session_id\n",
    "print(f\"\\n‚úÖ Session created!\")\n",
    "print(f\"   Session ID: {session_id}\")\n",
    "print(f\"\\nüí° Note: Sessions are different from regular conversations.\")\n",
    "print(f\"   Agent sessions maintain agent-specific context and tool state.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Executing a Turn\n",
    "\n",
    "Now let's give the agent a task and see it work! A **turn** is one interaction with the agent.\n",
    "\n",
    "**What happens:**\n",
    "1. We send a message to the agent\n",
    "2. The agent reasons about what to do\n",
    "3. The agent may call tools\n",
    "4. The agent responds with results\n",
    "5. We see the streaming response in real-time\n",
    "\n",
    "Let's watch it happen!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the task\n",
    "task = \"Check the status of all services\"\n",
    "print(\"=\" * 60)\n",
    "print(\"Executing Agent Turn\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìã Task: {task}\")\n",
    "\n",
    "# Step 2: Prepare messages\n",
    "# Messages are how we communicate with the agent\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": task\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\nüì§ Sending messages:\")\n",
    "print(json.dumps(messages, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a turn and process streaming response\n",
    "# This is where the magic happens - the agent reasons and acts!\n",
    "print(f\"\\nüîÑ Creating turn...\")\n",
    "print(f\"   Agent ID: {agent_id}\")\n",
    "print(f\"   Session ID: {session_id}\")\n",
    "print(f\"   Streaming: True (we'll see responses in real-time)\")\n",
    "\n",
    "# Create turn - this returns a stream of events\n",
    "turn_stream = client.alpha.agents.turn.create(\n",
    "    agent_id=agent_id,\n",
    "    session_id=session_id,\n",
    "    messages=messages,\n",
    "    stream=True  # We want to see the response as it's generated\n",
    ")\n",
    "\n",
    "print(f\"\\nüì• Streaming response (showing first few chunks):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Process the stream - this is what the agent is doing!\n",
    "result = \"\"\n",
    "turn_id = None\n",
    "chunk_count = 0\n",
    "\n",
    "for chunk in turn_stream:\n",
    "    chunk_count += 1\n",
    "    \n",
    "    # Show first few chunks to understand the structure\n",
    "    if chunk_count <= 3:\n",
    "        print(f\"\\n[Chunk {chunk_count}]\")\n",
    "        print(f\"  Type: {type(chunk).__name__}\")\n",
    "        if hasattr(chunk, 'event') and chunk.event:\n",
    "            event = chunk.event\n",
    "            if hasattr(event, 'payload') and event.payload:\n",
    "                payload = event.payload\n",
    "                # Try to convert to dict to see structure\n",
    "                if hasattr(payload, 'dict'):\n",
    "                    payload_dict = payload.model_dump()\n",
    "                    print(f\"  Event type: {payload_dict.get('event_type', 'N/A')}\")\n",
    "                    if 'delta' in payload_dict:\n",
    "                        delta = payload_dict['delta']\n",
    "                        if isinstance(delta, dict) and 'content' in delta:\n",
    "                            print(f\"  Text: {delta['text'][:100]}...\")\n",
    "    \n",
    "    # Extract content from chunks\n",
    "    if hasattr(chunk, 'event') and chunk.event:\n",
    "        event = chunk.event\n",
    "        if hasattr(event, 'payload') and event.payload:\n",
    "            payload = event.payload\n",
    "            payload_dict = payload.model_dump() if hasattr(payload, 'dict') else {}\n",
    "            \n",
    "            # Get content from delta\n",
    "            if 'delta' in payload_dict:\n",
    "                delta = payload_dict['delta']\n",
    "                delta_dict = delta.model_dump() if hasattr(delta, 'dict') else (delta if isinstance(delta, dict) else {})\n",
    "                if 'text' in delta_dict and delta_dict['text']:\n",
    "                    result += str(delta_dict['text'])\n",
    "            \n",
    "            # Get turn_id\n",
    "            if 'turn_id' in payload_dict and not turn_id:\n",
    "                turn_id = payload_dict['turn_id']\n",
    "            \n",
    "            # Check for completion\n",
    "            if payload_dict.get('event_type') in ['turn_complete', 'turn_end', 'complete', 'done']:\n",
    "                break\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"‚úÖ Turn completed!\")\n",
    "print(f\"   Total chunks processed: {chunk_count}\")\n",
    "print(f\"   Turn ID: {turn_id}\")\n",
    "print(f\"\\nüìä Agent Response:\")\n",
    "print(\"-\" * 60)\n",
    "print(result)\n",
    "print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Streaming Response\n",
    "\n",
    "**What we saw:**\n",
    "- **Chunks** - Each chunk is a piece of the agent's response\n",
    "- **Event structure** - Each chunk has an `event` with a `payload`\n",
    "- **Delta** - The `delta` contains the actual content being generated\n",
    "- **Event types** - Different events indicate different stages (start, progress, complete)\n",
    "\n",
    "**Key insight:** The agent is reasoning and responding in real-time. We can see:\n",
    "- When the agent starts thinking\n",
    "- When the agent generates text\n",
    "- When the agent calls tools (if any)\n",
    "- When the agent finishes\n",
    "\n",
    "This streaming approach lets us see the agent's \"thought process\" as it happens!\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: Testing More Complex Tasks\n",
    "\n",
    "Let's try a more complex task that requires the agent to actually use tools!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: A task that requires tool usage\n",
    "task2 = \"Check the status of the web-server service and restart it if it's down\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Test 2: Task Requiring Tool Usage\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìã Task: {task2}\")\n",
    "\n",
    "# Create a new session for this task\n",
    "session_name2 = f\"session-{int(time.time())}-task2\"\n",
    "session_response2 = client.alpha.agents.session.create(\n",
    "    agent_id=agent_id,\n",
    "    session_name=session_name2\n",
    ")\n",
    "session_id2 = session_response2.session_id\n",
    "\n",
    "print(f\"\\nüìù Created new session: {session_id2}\")\n",
    "\n",
    "# Execute the turn\n",
    "messages2 = [{\"role\": \"user\", \"content\": task2}]\n",
    "\n",
    "turn_stream2 = client.alpha.agents.turn.create(\n",
    "    agent_id=agent_id,\n",
    "    session_id=session_id2,\n",
    "    messages=messages2,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Process the stream\n",
    "result2 = \"\"\n",
    "print(f\"\\nüîÑ Agent is working...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for chunk in turn_stream2:\n",
    "    if hasattr(chunk, 'event') and chunk.event:\n",
    "        event = chunk.event\n",
    "        if hasattr(event, 'payload') and event.payload:\n",
    "            payload = event.payload\n",
    "            payload_dict = payload.model_dump() if hasattr(payload, 'dict') else {}\n",
    "            \n",
    "            # Extract content\n",
    "            if 'delta' in payload_dict:\n",
    "                delta = payload_dict['delta']\n",
    "                delta_dict = delta.model_dump() if hasattr(delta, 'dict') else (delta if isinstance(delta, dict) else {})\n",
    "                if 'text' in delta_dict and delta_dict['text']:\n",
    "                    content = str(delta_dict['text'])\n",
    "                    result2 += content\n",
    "                    # Print as it streams\n",
    "                    print(content, end=\"\", flush=True)\n",
    "            \n",
    "            # Check for completion\n",
    "            if payload_dict.get('event_type') in ['turn_complete', 'turn_end', 'complete', 'done']:\n",
    "                break\n",
    "\n",
    "print(f\"\\n\\n{'=' * 60}\")\n",
    "print(f\"‚úÖ Task completed!\")\n",
    "print(f\"\\nüìä Full Response:\")\n",
    "print(\"-\" * 60)\n",
    "print(result2)\n",
    "print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Understanding What Happened\n",
    "\n",
    "### The Agent Execution Flow\n",
    "\n",
    "Let's break down what just happened:\n",
    "\n",
    "1. **Task Received**: \"Check the status of the web-server service and restart it if it's down\"\n",
    "2. **Agent Reasoning**: The LLM analyzed the task and decided:\n",
    "   - First, I need to check the service status (use `check_service_status` tool)\n",
    "   - Then, if it's down, restart it (use `restart_service` tool)\n",
    "3. **Tool Selection**: The agent selected appropriate tools based on the task\n",
    "4. **Tool Execution**: Tools were executed (in the simulated environment)\n",
    "5. **Response Generation**: The agent synthesized the results and responded\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- **The agent reasoned** about what to do\n",
    "- **The agent selected tools** based on the task\n",
    "- **The agent executed tools** and got results\n",
    "- **The agent provided a summary** of what was done\n",
    "\n",
    "This is the **autonomous agent loop** in action:\n",
    "- **Observe** (read the task)\n",
    "- **Think** (reason about what to do)\n",
    "- **Act** (execute tools)\n",
    "- **Respond** (provide results)\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Tool Format**: Tools use OpenAI function calling format with `type`, `function`, `name`, `description`, `parameters`\n",
    "2. **Agent Creation**: Use `client.alpha.agents.create()` with `agent_config` containing model, instructions, and tools\n",
    "3. **Session Creation**: Use `client.alpha.agents.session.create()` to create conversation sessions\n",
    "4. **Turn Execution**: Use `client.alpha.agents.turn.create()` with `stream=True` to see real-time responses\n",
    "5. **Streaming Processing**: Process chunks to extract content from `event.payload.delta.content`\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "‚úÖ We saw the **actual SDK calls** and responses  \n",
    "‚úÖ We understood the **exact format** for tools  \n",
    "‚úÖ We saw **raw streaming responses** from LlamaStack  \n",
    "‚úÖ We learned how to **process agent outputs** step by step  \n",
    "‚úÖ We saw the agent **reason and act** autonomously  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- In Notebook 03, we'll explore LlamaStack's other features (RAG, MCP, Safety, Eval)\n",
    "- In Notebook 04, we'll see how to combine features for advanced agents\n",
    "- Later, we'll see how abstractions can simplify this workflow (but you'll understand what's happening underneath!)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
