{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0775cdd1",
   "metadata": {},
   "source": [
    "# Fine-tuning de Modelo com LoRA\n",
    "\n",
    "Este notebook demonstra como realizar fine-tuning de um modelo de linguagem (Qwen2.5-3B-Instruct) usando a técnica LoRA (Low-Rank Adaptation) para uma tarefa específica de extração de campos estruturados a partir de tickets de call center.\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Treinar o modelo para extrair automaticamente campos estruturados (categoria, subcategoria, grupo de atribuição, etc.) a partir de descrições de tickets de TI, retornando os dados em formato JSON.\n",
    "\n",
    "## Tecnologias Utilizadas\n",
    "\n",
    "- **Modelo Base**: Qwen/Qwen2.5-3B-Instruct (modelo de linguagem instrucional)\n",
    "- **Técnica de Fine-tuning**: LoRA (Low-Rank Adaptation) - permite treinar apenas uma pequena parte dos parâmetros\n",
    "- **Bibliotecas**: \n",
    "  - `transformers` - Hugging Face Transformers\n",
    "  - `peft` - Parameter-Efficient Fine-Tuning\n",
    "  - `trl` - Transformer Reinforcement Learning (SFTTrainer)\n",
    "  - `datasets` - Hugging Face Datasets\n",
    "\n",
    "## Estrutura do Notebook\n",
    "\n",
    "1. **Configuração e Carregamento**: Configuração inicial e carregamento do dataset\n",
    "2. **Preparação do Modelo**: Carregamento do tokenizer e modelo base\n",
    "3. **Configuração LoRA**: Aplicação da técnica LoRA para treinamento eficiente\n",
    "4. **Preparação dos Dados**: Formatação e tokenização do dataset\n",
    "5. **Treinamento**: Execução do fine-tuning\n",
    "6. **Salvamento**: Salvamento do adaptador LoRA treinado\n",
    "\n",
    "## Formato do Dataset\n",
    "\n",
    "O dataset deve estar em formato JSONL com o seguinte formato:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"Prompt com a descrição do ticket...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"{\\\"campo1\\\": \\\"valor1\\\", ...}\"}\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf4dc85",
   "metadata": {},
   "source": [
    "## 1. Configuração e Carregamento do Dataset\n",
    "\n",
    "Primeiro, vamos configurar os parâmetros principais e carregar o dataset de treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ccfcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataset...\n",
      "Dataset carregado: 1000 exemplos\n",
      "Estrutura do dataset: {'messages': List({'role': Value('string'), 'content': Value('string')})}\n",
      "\n",
      "Primeiro exemplo do dataset:\n",
      "{'messages': [{'role': 'user', 'content': 'Short description: Request for PostgreSQL upgrade to the latest version.\\nContent: I would like to request an upgrade for our PostgreSQL to the latest version in order to utilize new features and improvements. Before proceeding, please assess the impact this upgrade may have on our current projects and confirm compatibility with our existing data. Could you escalate this request to the Software Upgrade Team for me?\\nPreencha os seguintes campos em JSON:\\n- category\\n- subcategory\\n- issue_request\\n- assignment_group\\n- software_system\\n- poor_close_notes\\n- resolution_time\\n- info_score_close_notes'}, {'role': 'assistant', 'content': '{\"category\": \"SOFTWARE\", \"subcategory\": \"INSTALLATION\", \"issue_request\": \"PostgreSQL Upgrade Request\", \"assignment_group\": \"DBTED SUPPORT GROUP\", \"software_system\": \"PostgreSQL\", \"poor_close_notes\": \"See worknotes\", \"resolution_time\": 514.97, \"info_score_close_notes\": 0.8}'}]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "DATA_FILE = \"finetune_multioutput_small.jsonl\"  # seu arquivo JSONL com formato de mensagens\n",
    "OUTPUT_DIR = \"./qwen2.5-lora\"\n",
    "\n",
    "# Carregar o dataset de treinamento\n",
    "# O dataset deve estar em formato JSONL com campo \"messages\"\n",
    "print(\"Carregando dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=DATA_FILE, split=\"train\")\n",
    "print(f\"Dataset carregado: {len(dataset)} exemplos\")\n",
    "print(f\"Estrutura do dataset: {dataset.features}\")\n",
    "\n",
    "# Verificar formato do primeiro exemplo para validação\n",
    "if len(dataset) > 0:\n",
    "    print(\"\\nPrimeiro exemplo do dataset:\")\n",
    "    print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd80796",
   "metadata": {},
   "source": [
    "## 2. Verificação de Autenticação\n",
    "\n",
    "Verifica se você está autenticado no Hugging Face Hub (necessário para baixar modelos privados ou fazer upload).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ae8d1",
   "metadata": {},
   "source": [
    "## 3. Carregamento do Tokenizer\n",
    "\n",
    "O tokenizer é responsável por converter texto em tokens (IDs numéricos) que o modelo pode processar. Configuramos o token de padding caso não exista.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a9b7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '691e3c7f623e254dabe49713', 'name': 'apappel', 'fullname': 'Ana Paula Appel', 'email': 'aappel@redhat.com', 'emailVerified': True, 'canPay': False, 'billingMode': 'prepaid', 'periodEnd': 1764547200, 'isPro': False, 'avatarUrl': '/avatars/74a3f637b5e78c5dcc48bbff36b5da55.svg', 'orgs': [{'type': 'org', 'id': '6586e2598dd42194879b59aa', 'name': 'RedHatAI', 'fullname': 'Red Hat AI', 'email': 'ap@redhat.com', 'canPay': False, 'billingMode': 'postpaid', 'periodEnd': 1764547199, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60466e4b4f40b01b66151416/cdABRow21BL0sl1vSVTPk.png', 'roleInOrg': 'contributor', 'isEnterprise': True, 'plan': 'enterprise'}], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'finetune', 'role': 'write', 'createdAt': '2025-11-19T21:58:07.735Z'}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "print(whoami())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f3a00",
   "metadata": {},
   "source": [
    "## 4. Carregamento do Modelo Base\n",
    "\n",
    "Carregamos o modelo Qwen2.5-3B-Instruct. O modelo detecta automaticamente se há GPU disponível e ajusta o tipo de dados (float16 para GPU, float32 para CPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e20def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aappel/Development/Github/hello-there-ai-ops-workshop/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1025: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 151665\n",
      "Pad token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 2. Load tokenizer\n",
    "# -------------------------------\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, use_auth_token=True)\n",
    "\n",
    "# Configurar padding token se não existir\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e9a39",
   "metadata": {},
   "source": [
    "## 5. Configuração LoRA\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** é uma técnica de fine-tuning eficiente que:\n",
    "\n",
    "- Treina apenas uma pequena fração dos parâmetros do modelo (geralmente < 1%)\n",
    "- Adiciona matrizes de baixo rank às camadas de atenção\n",
    "- Reduz drasticamente o uso de memória e tempo de treinamento\n",
    "- Mantém a qualidade do modelo original\n",
    "\n",
    "**Parâmetros LoRA**:\n",
    "- `r=32`: Rank das matrizes LoRA (maior = mais parâmetros treináveis)\n",
    "- `lora_alpha=32`: Fator de escala para os pesos LoRA\n",
    "- `target_modules`: Módulos onde LoRA será aplicado (q_proj, v_proj são camadas de atenção)\n",
    "- `lora_dropout=0.05`: Taxa de dropout para regularização\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a39669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n",
      "Loading model with dtype: torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aappel/Development/Github/hello-there-ai-ops-workshop/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea1d2d1ff6042c0b98d8c9bb340cd9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on CPU\n",
      "Model dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 3. Load base model\n",
    "# -------------------------------\n",
    "print(\"Loading base model...\")\n",
    "# Usar float16 apenas se tiver GPU, caso contrário usar float32\n",
    "model_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"Loading model with dtype: {model_dtype}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    use_auth_token=True, \n",
    "    torch_dtype=model_dtype,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Model loaded on device: {next(model.parameters()).device}\")\n",
    "else:\n",
    "    print(\"Model loaded on CPU\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8348e4",
   "metadata": {},
   "source": [
    "## 6. Configuração dos Argumentos de Treinamento\n",
    "\n",
    "Configuramos os hiperparâmetros do treinamento:\n",
    "\n",
    "- **Batch size**: Tamanho do lote (ajustado para 1 com gradient accumulation)\n",
    "- **Learning rate**: Taxa de aprendizado (2e-4 é um bom valor inicial)\n",
    "- **Epochs**: Número de épocas de treinamento\n",
    "- **Gradient accumulation**: Acumula gradientes de múltiplos batches antes de atualizar pesos\n",
    "- **FP16**: Usa precisão de 16 bits se GPU disponível (economiza memória)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eddea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando LoRA ao modelo...\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 7,372,800 || all params: 3,093,311,488 || trainable%: 0.2383\n",
      "✓ Tokenizer associado ao modelo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aappel/Development/Github/hello-there-ai-ops-workshop/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 4. LoRA configuration\n",
    "# -------------------------------\n",
    "# Para Qwen2.5, os módulos de atenção geralmente são: q_proj, k_proj, v_proj, o_proj\n",
    "# Vamos usar q_proj e v_proj para começar (mais eficiente)\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # módulos de atenção do Qwen2.5\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"Aplicando LoRA ao modelo...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Associar tokenizer ao modelo (necessário para algumas versões do TRL)\n",
    "if not hasattr(model, 'tokenizer') or model.tokenizer is None:\n",
    "    model.tokenizer = tokenizer\n",
    "    print(\"✓ Tokenizer associado ao modelo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e81a65",
   "metadata": {},
   "source": [
    "## 7. Validação do Formato do Dataset\n",
    "\n",
    "Verificamos se o dataset está no formato correto esperado pelo SFTTrainer (campo \"messages\" com formato de chat).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d9669",
   "metadata": {},
   "source": [
    "## 8. Preparação e Tokenização do Dataset\n",
    "\n",
    "Esta é uma etapa crítica do processo:\n",
    "\n",
    "1. **Formatação**: Converte as mensagens do formato chat para texto formatado no padrão do Qwen (`<|im_start|>user`, `<|im_end|>`, etc.)\n",
    "\n",
    "2. **Tokenização**: Converte o texto em tokens (IDs numéricos) que o modelo pode processar\n",
    "   - **Importante**: O DataCollator precisa de dados já tokenizados, não texto bruto\n",
    "   - Usamos `truncation=True` e `max_length=1024` para limitar o tamanho\n",
    "   - `padding=False` porque o DataCollator fará padding dinâmico durante o treinamento\n",
    "\n",
    "3. **DataCollator**: Responsável por criar batches com padding adequado durante o treinamento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "597809a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FP16: False (GPU available: False)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 5. Training arguments\n",
    "# -------------------------------\n",
    "# Verificar se há GPU disponível\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "print(f\"Using FP16: {use_fp16} (GPU available: {torch.cuda.is_available()})\")\n",
    "\n",
    "# Usar TrainingArguments do transformers (compatível com SFTTrainer)\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=100,\n",
    "    fp16=use_fp16,  # Use fp16 apenas se tiver GPU\n",
    "    bf16=False,  # Use bf16 se tiver GPU moderna (A100, H100)\n",
    "    dataloader_num_workers=0,  # Evitar problemas de multiprocessing\n",
    "    remove_unused_columns=False,  # Manter todas as colunas do dataset\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1311d78",
   "metadata": {},
   "source": [
    "## 9. Treinamento\n",
    "\n",
    "Inicia o processo de fine-tuning. O treinamento pode levar algum tempo dependendo do tamanho do dataset e dos recursos disponíveis.\n",
    "\n",
    "**Monitoramento**:\n",
    "- Os logs aparecem a cada `logging_steps` (10 steps)\n",
    "- Checkpoints são salvos a cada `save_steps` (500 steps)\n",
    "- O progresso mostra loss, learning rate e outras métricas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93246478",
   "metadata": {},
   "source": [
    "## 10. Salvamento do Modelo\n",
    "\n",
    "Salva o adaptador LoRA treinado e o tokenizer. O adaptador LoRA contém apenas os pesos treinados (muito menor que o modelo completo) e pode ser carregado junto com o modelo base para inferência.\n",
    "\n",
    "**Estrutura salva**:\n",
    "- `adapter_config.json`: Configuração do LoRA\n",
    "- `adapter_model.bin`: Pesos do adaptador LoRA\n",
    "- `tokenizer files`: Arquivos do tokenizer\n",
    "\n",
    "**Para usar o modelo treinado**:\n",
    "```python\n",
    "from peft import PeftModel\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model = PeftModel.from_pretrained(model, OUTPUT_DIR)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3253317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset tem formato de mensagens correto\n",
      "  Exemplo de mensagens: [{'role': 'user', 'content': 'Short description: Request for PostgreSQL upgrade to the latest version.\\nContent: I would like to request an upgrade for our PostgreSQL to the latest version in order to utilize new features and improvements. Before proceeding, please assess the impact this upgrade may have on our current projects and confirm compatibility with our existing data. Could you escalate this request to the Software Upgrade Team for me?\\nPreencha os seguintes campos em JSON:\\n- category\\n- subcategory\\n- issue_request\\n- assignment_group\\n- software_system\\n- poor_close_notes\\n- resolution_time\\n- info_score_close_notes'}, {'role': 'assistant', 'content': '{\"category\": \"SOFTWARE\", \"subcategory\": \"INSTALLATION\", \"issue_request\": \"PostgreSQL Upgrade Request\", \"assignment_group\": \"DBTED SUPPORT GROUP\", \"software_system\": \"PostgreSQL\", \"poor_close_notes\": \"See worknotes\", \"resolution_time\": 514.97, \"info_score_close_notes\": 0.8}'}]\n"
     ]
    }
   ],
   "source": [
    "# Verificar se o dataset tem o formato correto de mensagens\n",
    "# O SFTTrainer espera um campo \"messages\" com formato de chat\n",
    "sample = dataset[0]\n",
    "if \"messages\" in sample:\n",
    "    print(\"✓ Dataset tem formato de mensagens correto\")\n",
    "    print(f\"  Exemplo de mensagens: {sample['messages']}\")\n",
    "else:\n",
    "    print(\"✗ Dataset não tem formato de mensagens\")\n",
    "    print(f\"  Campos disponíveis: {sample.keys()}\")\n",
    "    raise ValueError(\"O dataset precisa ter o campo 'messages' no formato de chat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fb5a575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatando dataset...\n",
      "✓ Dataset formatado. Exemplo (primeiros 200 chars): {'text': '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nShort description: Request for PostgreSQL upgrade to the latest version....\n",
      "\n",
      "Tokenizando dataset...\n",
      "✓ Dataset tokenizado. Exemplo de campos: ['input_ids', 'attention_mask']\n",
      "✓ SFTTrainer criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 6. Trainer (SFT)\n",
    "# -------------------------------\n",
    "# Esta versão do TRL usa uma API diferente - precisa usar processing_class para o tokenizer\n",
    "# e formatar as mensagens em texto antes\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def format_messages_manual(messages):\n",
    "    \"\"\"Formata mensagens manualmente no formato do Qwen\"\"\"\n",
    "    formatted = \"\"\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        content = msg[\"content\"]\n",
    "        if role == \"user\":\n",
    "            formatted += f\"<|im_start|>user\\n{content}<|im_end|>\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            formatted += f\"<|im_start|>assistant\\n{content}<|im_end|>\\n\"\n",
    "    return formatted\n",
    "\n",
    "# Função para formatar as mensagens do dataset em texto\n",
    "def format_messages(examples):\n",
    "    \"\"\"Converte mensagens do formato chat para texto formatado\"\"\"\n",
    "    texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        # Verificar se o tokenizer tem apply_chat_template\n",
    "        if hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template is not None:\n",
    "            try:\n",
    "                formatted_text = tokenizer.apply_chat_template(\n",
    "                    messages, \n",
    "                    tokenize=False, \n",
    "                    add_generation_prompt=False\n",
    "                )\n",
    "            except:\n",
    "                # Fallback: formatar manualmente\n",
    "                formatted_text = format_messages_manual(messages)\n",
    "        else:\n",
    "            # Formatar manualmente se não tiver chat_template\n",
    "            formatted_text = format_messages_manual(messages)\n",
    "        texts.append(formatted_text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Aplicar formatação ao dataset\n",
    "print(\"Formatando dataset...\")\n",
    "try:\n",
    "    dataset_formatted = dataset.map(\n",
    "        format_messages,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,  # Remove colunas originais\n",
    "    )\n",
    "    print(f\"✓ Dataset formatado. Exemplo (primeiros 200 chars): {str(dataset_formatted[0])[:200]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao formatar dataset: {e}\")\n",
    "    # Tentar sem remover colunas\n",
    "    dataset_formatted = dataset.map(\n",
    "        format_messages,\n",
    "        batched=True,\n",
    "    )\n",
    "    print(f\"✓ Dataset formatado (mantendo colunas originais)\")\n",
    "\n",
    "# Tokenizar o dataset - CRÍTICO: o DataCollator precisa de dados tokenizados, não texto bruto\n",
    "print(\"\\nTokenizando dataset...\")\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokeniza os textos do dataset\"\"\"\n",
    "    # Tokenizar com padding e truncation\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # O DataCollator fará o padding\n",
    "        max_length=1024,\n",
    "        return_tensors=None,  # Retornar como listas, não tensors\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "dataset_tokenized = dataset_formatted.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],  # Remover texto bruto, manter apenas tokens\n",
    "    desc=\"Tokenizando dataset\"\n",
    ")\n",
    "print(f\"✓ Dataset tokenizado. Exemplo de campos: {list(dataset_tokenized[0].keys())}\")\n",
    "\n",
    "# Criar data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Não é masked language modeling\n",
    "    pad_to_multiple_of=8,  # Otimização para GPUs modernas\n",
    ")\n",
    "\n",
    "# Criar o trainer com os parâmetros corretos para esta versão do TRL\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset_tokenized,  # Usar dataset tokenizado, não formatado\n",
    "    processing_class=tokenizer,  # Tokenizer via processing_class\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"✓ SFTTrainer criado com sucesso!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a026fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Trainer type: <class 'trl.trainer.sft_trainer.SFTTrainer'>\n",
      "Trainer tem tokenizer: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aappel/Development/Github/hello-there-ai-ops-workshop/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 17:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.673100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.309700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.792500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.489000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.889100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.912400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.804200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.813800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.798600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.770700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.776700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.739800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.738100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.686300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.783700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.777800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.698700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 7. Train!\n",
    "# -------------------------------\n",
    "print(\"Starting training...\")\n",
    "print(f\"Trainer type: {type(trainer)}\")\n",
    "print(f\"Trainer tem tokenizer: {hasattr(trainer, 'tokenizer')}\")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"Erro durante o treinamento: {e}\")\n",
    "    print(f\"Tipo do erro: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da9eeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 8. Save LoRA adapter\n",
    "# -------------------------------\n",
    "print(\"Saving model...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94985f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
