{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Teste do Modelo com Fine-tuning\n",
        "\n",
        "Este notebook demonstra como carregar e testar o modelo que foi submetido a fine-tuning usando LoRA.\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "Testar o modelo treinado para extrair campos estruturados de tickets de call center, verificando se o fine-tuning melhorou a capacidade do modelo de retornar dados em formato JSON.\n",
        "\n",
        "## Estrutura do Notebook\n",
        "\n",
        "1. **Carregamento do Modelo**: Carrega o modelo base e o adaptador LoRA treinado\n",
        "2. **Prepara√ß√£o do Tokenizer**: Configura o tokenizer para processar as entradas\n",
        "3. **Testes com Exemplos**: Testa o modelo com diferentes exemplos de tickets\n",
        "4. **Avalia√ß√£o**: Compara as respostas do modelo com os resultados esperados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configura√ß√£o e Importa√ß√µes\n",
        "\n",
        "Configuramos os caminhos e importamos as bibliotecas necess√°rias.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configura√ß√£o carregada:\n",
            "  Modelo base: Qwen/Qwen2.5-3B-Instruct\n",
            "  Adaptador LoRA: ./qwen2.5-lora\n"
          ]
        }
      ],
      "source": [
        "# Importa√ß√µes necess√°rias\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import json\n",
        "import re\n",
        "\n",
        "# -------------------------------\n",
        "# CONFIGURA√á√ÉO\n",
        "# -------------------------------\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"  # Modelo base\n",
        "LORA_ADAPTER_PATH = \"./qwen2.5-lora\"  # Caminho do adaptador LoRA treinado\n",
        "\n",
        "print(\"Configura√ß√£o carregada:\")\n",
        "print(f\"  Modelo base: {MODEL_NAME}\")\n",
        "print(f\"  Adaptador LoRA: {LORA_ADAPTER_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carregamento do Tokenizer\n",
        "\n",
        "Carregamos o tokenizer que ser√° usado para processar as entradas e sa√≠das do modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Carregando tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aappel/Development/Github/hello-there-ai-ops-workshop/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1025: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Tokenizer carregado com sucesso\n",
            "  Tamanho do vocabul√°rio: 151665\n",
            "  Token de padding: <|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "# Carregar tokenizer\n",
        "print(\"Carregando tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "# Configurar padding token se necess√°rio\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"‚úì Tokenizer carregado com sucesso\")\n",
        "print(f\"  Tamanho do vocabul√°rio: {len(tokenizer)}\")\n",
        "print(f\"  Token de padding: {tokenizer.pad_token}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Carregamento do Modelo Base e Adaptador LoRA\n",
        "\n",
        "Carregamos o modelo base e depois aplicamos o adaptador LoRA treinado. O adaptador cont√©m apenas os pesos ajustados durante o fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Carregando modelo base...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aappel/Development/Github/hello-there-ai-ops-workshop/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe00b2d00f94438aa4b90ed373ea92ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Modelo base carregado\n",
            "\n",
            "Carregando adaptador LoRA de ./qwen2.5-lora...\n",
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aappel/Development/Github/hello-there-ai-ops-workshop/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Adaptador LoRA carregado com sucesso\n",
            "  Modelo na CPU\n"
          ]
        }
      ],
      "source": [
        "# Carregar modelo base\n",
        "print(\"Carregando modelo base...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "print(\"‚úì Modelo base carregado\")\n",
        "\n",
        "# Carregar adaptador LoRA treinado\n",
        "print(f\"\\nCarregando adaptador LoRA de {LORA_ADAPTER_PATH}...\")\n",
        "model = PeftModel.from_pretrained(model, LORA_ADAPTER_PATH)\n",
        "\n",
        "print(\"‚úì Adaptador LoRA carregado com sucesso\")\n",
        "\n",
        "# Colocar modelo em modo de avalia√ß√£o (n√£o treinamento)\n",
        "model.eval()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  Modelo no dispositivo: {next(model.parameters()).device}\")\n",
        "else:\n",
        "    print(\"  Modelo na CPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Fun√ß√£o de Infer√™ncia\n",
        "\n",
        "Criamos uma fun√ß√£o auxiliar para fazer infer√™ncia com o modelo, formatando a entrada e processando a sa√≠da.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Fun√ß√£o de infer√™ncia criada\n"
          ]
        }
      ],
      "source": [
        "def generate_response(prompt, max_new_tokens=512, temperature=0.1, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Gera uma resposta do modelo para um prompt dado.\n",
        "    \n",
        "    Args:\n",
        "        prompt: Texto de entrada (descri√ß√£o do ticket)\n",
        "        max_new_tokens: N√∫mero m√°ximo de tokens a gerar\n",
        "        temperature: Controla a aleatoriedade (menor = mais determin√≠stico)\n",
        "        top_p: Nucleus sampling (probabilidade acumulada)\n",
        "    \n",
        "    Returns:\n",
        "        Resposta gerada pelo modelo\n",
        "    \"\"\"\n",
        "    # Formatar prompt no formato esperado pelo modelo\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    \n",
        "    # Aplicar template de chat do tokenizer\n",
        "    if hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template is not None:\n",
        "        formatted_prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "    else:\n",
        "        # Fallback: formata√ß√£o manual\n",
        "        formatted_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "    \n",
        "    # Tokenizar\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
        "    \n",
        "    # Mover para o mesmo dispositivo do modelo\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "    \n",
        "    # Gerar resposta\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=temperature > 0,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decodificar resposta (remover o prompt da sa√≠da)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    \n",
        "    # Extrair apenas a parte gerada (ap√≥s o prompt)\n",
        "    if \"<|im_start|>assistant\" in generated_text:\n",
        "        response = generated_text.split(\"<|im_start|>assistant\")[-1]\n",
        "        response = response.replace(\"<|im_end|>\", \"\").strip()\n",
        "    else:\n",
        "        # Se n√£o encontrar o marcador, pegar tudo ap√≥s o prompt\n",
        "        prompt_tokens = len(tokenizer.encode(formatted_prompt))\n",
        "        response = tokenizer.decode(outputs[0][prompt_tokens:], skip_special_tokens=True)\n",
        "    \n",
        "    return response\n",
        "\n",
        "print(\"‚úì Fun√ß√£o de infer√™ncia criada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Testes com Exemplos\n",
        "\n",
        "Testamos o modelo com diferentes exemplos de tickets para verificar se ele consegue extrair os campos corretamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparados 3 exemplos para teste\n"
          ]
        }
      ],
      "source": [
        "# Exemplos de teste\n",
        "test_examples = [\n",
        "    {\n",
        "        \"name\": \"Exemplo 1: Upgrade de PostgreSQL\",\n",
        "        \"prompt\": (\n",
        "            \"Short description: Request for PostgreSQL upgrade to the latest version.\\n\"\n",
        "            \"Content: I would like to request an upgrade for our PostgreSQL to the latest version \"\n",
        "            \"in order to utilize new features and improvements. Before proceeding, please assess \"\n",
        "            \"the impact this upgrade may have on our current projects and confirm compatibility \"\n",
        "            \"with our existing data. Could you escalate this request to the Software Upgrade Team for me?\\n\"\n",
        "            \"Preencha os seguintes campos em JSON:\\n\"\n",
        "            \"- category\\n\"\n",
        "            \"- subcategory\\n\"\n",
        "            \"- issue_request\\n\"\n",
        "            \"- assignment_group\\n\"\n",
        "            \"- software_system\\n\"\n",
        "            \"- poor_close_notes\\n\"\n",
        "            \"- resolution_time\\n\"\n",
        "            \"- info_score_close_notes\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Exemplo 2: Problema de Login\",\n",
        "        \"prompt\": (\n",
        "            \"Short description: User unable to login to system\\n\"\n",
        "            \"Content: I am experiencing issues logging into the company portal. \"\n",
        "            \"I have tried resetting my password multiple times but still cannot access my account. \"\n",
        "            \"The error message says 'Invalid credentials' but I am certain I am using the correct password.\\n\"\n",
        "            \"Preencha os seguintes campos em JSON:\\n\"\n",
        "            \"- category\\n\"\n",
        "            \"- subcategory\\n\"\n",
        "            \"- issue_request\\n\"\n",
        "            \"- assignment_group\\n\"\n",
        "            \"- software_system\\n\"\n",
        "            \"- poor_close_notes\\n\"\n",
        "            \"- resolution_time\\n\"\n",
        "            \"- info_score_close_notes\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Exemplo 3: Aplica√ß√£o com Crash\",\n",
        "        \"prompt\": (\n",
        "            \"Short description: Application crashes when saving files\\n\"\n",
        "            \"Content: The application ZTrend keeps crashing whenever I try to save a file. \"\n",
        "            \"This started happening after the last system update. I have lost work multiple times. \"\n",
        "            \"Please help me resolve this issue as soon as possible.\\n\"\n",
        "            \"Preencha os seguintes campos em JSON:\\n\"\n",
        "            \"- category\\n\"\n",
        "            \"- subcategory\\n\"\n",
        "            \"- issue_request\\n\"\n",
        "            \"- assignment_group\\n\"\n",
        "            \"- software_system\\n\"\n",
        "            \"- poor_close_notes\\n\"\n",
        "            \"- resolution_time\\n\"\n",
        "            \"- info_score_close_notes\"\n",
        "        )\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Preparados {len(test_examples)} exemplos para teste\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXECUTANDO TESTES DO MODELO\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "TESTE 1: Exemplo 1: Upgrade de PostgreSQL\n",
            "================================================================================\n",
            "\n",
            "üì• Prompt:\n",
            "Short description: Request for PostgreSQL upgrade to the latest version.\n",
            "Content: I would like to request an upgrade for our PostgreSQL to the latest version in order to utilize new features and impro...\n",
            "\n",
            "ü§ñ Gerando resposta...\n",
            "\n",
            "üì§ Resposta do modelo:\n",
            "{\"category\": \"SOFTWARE\", \"subcategory\": \"INSTALLATION\", \"issue_request\": \"PostgreSQL Upgrade Request\", \"assignment_group\": \"APPLICATION SUPPORT\", \"software_system\": \"PostgreSQL\", \"poor_close_notes\": \"Issue resolved\", \"resolution_time\": 124.78, \"info_score_close_notes\": 0.9}\n",
            "\n",
            "‚úÖ Resposta v√°lida em formato JSON:\n",
            "{\n",
            "  \"category\": \"SOFTWARE\",\n",
            "  \"subcategory\": \"INSTALLATION\",\n",
            "  \"issue_request\": \"PostgreSQL Upgrade Request\",\n",
            "  \"assignment_group\": \"APPLICATION SUPPORT\",\n",
            "  \"software_system\": \"PostgreSQL\",\n",
            "  \"poor_close_notes\": \"Issue resolved\",\n",
            "  \"resolution_time\": 124.78,\n",
            "  \"info_score_close_notes\": 0.9\n",
            "}\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "TESTE 2: Exemplo 2: Problema de Login\n",
            "================================================================================\n",
            "\n",
            "üì• Prompt:\n",
            "Short description: User unable to login to system\n",
            "Content: I am experiencing issues logging into the company portal. I have tried resetting my password multiple times but still cannot access my accoun...\n",
            "\n",
            "ü§ñ Gerando resposta...\n",
            "\n",
            "üì§ Resposta do modelo:\n",
            "{\"category\": \"SOFTWARE\", \"subcategory\": \"ERROR\", \"issue_request\": \"Invalid Credentials Error\", \"assignment_group\": \"\", \"software_system\": \"Company Portal\", \"poor_close_notes\": \"Issue resolved. User can now log in.\", \"resolution_time\": 14235.89675, \"info_score_close_notes\": 0.8}\n",
            "\n",
            "‚úÖ Resposta v√°lida em formato JSON:\n",
            "{\n",
            "  \"category\": \"SOFTWARE\",\n",
            "  \"subcategory\": \"ERROR\",\n",
            "  \"issue_request\": \"Invalid Credentials Error\",\n",
            "  \"assignment_group\": \"\",\n",
            "  \"software_system\": \"Company Portal\",\n",
            "  \"poor_close_notes\": \"Issue resolved. User can now log in.\",\n",
            "  \"resolution_time\": 14235.89675,\n",
            "  \"info_score_close_notes\": 0.8\n",
            "}\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "TESTE 3: Exemplo 3: Aplica√ß√£o com Crash\n",
            "================================================================================\n",
            "\n",
            "üì• Prompt:\n",
            "Short description: Application crashes when saving files\n",
            "Content: The application ZTrend keeps crashing whenever I try to save a file. This started happening after the last system update. I have lost ...\n",
            "\n",
            "ü§ñ Gerando resposta...\n",
            "\n",
            "üì§ Resposta do modelo:\n",
            "{\"category\": \"SOFTWARE\", \"subcategory\": \"ERROR\", \"issue_request\": \"Application crashes when saving files\", \"assignment_group\": \"\", \"software_system\": \"ZTrend\", \"poor_close_notes\": \"Issue resolved. User can now save files without crashes.\", \"resolution_time\": 12486.70395, \"info_score_close_notes\": 0.9}\n",
            "\n",
            "‚úÖ Resposta v√°lida em formato JSON:\n",
            "{\n",
            "  \"category\": \"SOFTWARE\",\n",
            "  \"subcategory\": \"ERROR\",\n",
            "  \"issue_request\": \"Application crashes when saving files\",\n",
            "  \"assignment_group\": \"\",\n",
            "  \"software_system\": \"ZTrend\",\n",
            "  \"poor_close_notes\": \"Issue resolved. User can now save files without crashes.\",\n",
            "  \"resolution_time\": 12486.70395,\n",
            "  \"info_score_close_notes\": 0.9\n",
            "}\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Executar testes\n",
        "print(\"=\" * 80)\n",
        "print(\"EXECUTANDO TESTES DO MODELO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, example in enumerate(test_examples, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TESTE {i}: {example['name']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    print(\"\\nüì• Prompt:\")\n",
        "    print(example['prompt'][:200] + \"...\" if len(example['prompt']) > 200 else example['prompt'])\n",
        "    \n",
        "    print(\"\\nü§ñ Gerando resposta...\")\n",
        "    try:\n",
        "        response = generate_response(example['prompt'])\n",
        "        \n",
        "        print(\"\\nüì§ Resposta do modelo:\")\n",
        "        print(response)\n",
        "        \n",
        "        # Tentar parsear como JSON\n",
        "        try:\n",
        "            # Limpar a resposta (remover markdown code blocks se houver)\n",
        "            cleaned_response = response.strip()\n",
        "            if cleaned_response.startswith(\"```\"):\n",
        "                # Remover code blocks\n",
        "                lines = cleaned_response.split(\"\\n\")\n",
        "                cleaned_response = \"\\n\".join([l for l in lines if not l.strip().startswith(\"```\")])\n",
        "            \n",
        "            parsed_json = json.loads(cleaned_response)\n",
        "            print(\"\\n‚úÖ Resposta v√°lida em formato JSON:\")\n",
        "            print(json.dumps(parsed_json, indent=2, ensure_ascii=False))\n",
        "            \n",
        "            results.append({\n",
        "                \"test\": example['name'],\n",
        "                \"success\": True,\n",
        "                \"response\": response,\n",
        "                \"parsed_json\": parsed_json\n",
        "            })\n",
        "            \n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"\\n‚ö†Ô∏è  Resposta n√£o √© JSON v√°lido: {e}\")\n",
        "            print(f\"   Tentando extrair JSON da resposta...\")\n",
        "            \n",
        "            # Tentar encontrar JSON na resposta\n",
        "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
        "            if json_match:\n",
        "                try:\n",
        "                    parsed_json = json.loads(json_match.group())\n",
        "                    print(\"\\n‚úÖ JSON extra√≠do com sucesso:\")\n",
        "                    print(json.dumps(parsed_json, indent=2, ensure_ascii=False))\n",
        "                    results.append({\n",
        "                        \"test\": example['name'],\n",
        "                        \"success\": True,\n",
        "                        \"response\": response,\n",
        "                        \"parsed_json\": parsed_json\n",
        "                    })\n",
        "                except:\n",
        "                    results.append({\n",
        "                        \"test\": example['name'],\n",
        "                        \"success\": False,\n",
        "                        \"response\": response,\n",
        "                        \"error\": \"N√£o foi poss√≠vel parsear JSON\"\n",
        "                    })\n",
        "            else:\n",
        "                results.append({\n",
        "                    \"test\": example['name'],\n",
        "                    \"success\": False,\n",
        "                    \"response\": response,\n",
        "                    \"error\": \"Nenhum JSON encontrado na resposta\"\n",
        "                })\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Erro durante a gera√ß√£o: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        results.append({\n",
        "            \"test\": example['name'],\n",
        "            \"success\": False,\n",
        "            \"error\": str(e)\n",
        "        })\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Resumo dos Resultados\n",
        "\n",
        "Exibimos um resumo dos testes realizados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "RESUMO DOS TESTES\n",
            "================================================================================\n",
            "\n",
            "Total de testes: 3\n",
            "‚úÖ Sucessos: 3\n",
            "‚ùå Falhas: 0\n",
            "üìä Taxa de sucesso: 100.0%\n",
            "\n",
            "================================================================================\n",
            "DETALHES POR TESTE\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Teste 1: Exemplo 1: Upgrade de PostgreSQL\n",
            "   Campos extra√≠dos: ['category', 'subcategory', 'issue_request', 'assignment_group', 'software_system', 'poor_close_notes', 'resolution_time', 'info_score_close_notes']\n",
            "\n",
            "‚úÖ Teste 2: Exemplo 2: Problema de Login\n",
            "   Campos extra√≠dos: ['category', 'subcategory', 'issue_request', 'assignment_group', 'software_system', 'poor_close_notes', 'resolution_time', 'info_score_close_notes']\n",
            "\n",
            "‚úÖ Teste 3: Exemplo 3: Aplica√ß√£o com Crash\n",
            "   Campos extra√≠dos: ['category', 'subcategory', 'issue_request', 'assignment_group', 'software_system', 'poor_close_notes', 'resolution_time', 'info_score_close_notes']\n"
          ]
        }
      ],
      "source": [
        "# Resumo dos resultados\n",
        "print(\"=\" * 80)\n",
        "print(\"RESUMO DOS TESTES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "total_tests = len(results)\n",
        "successful_tests = sum(1 for r in results if r.get('success', False))\n",
        "failed_tests = total_tests - successful_tests\n",
        "\n",
        "print(f\"\\nTotal de testes: {total_tests}\")\n",
        "print(f\"‚úÖ Sucessos: {successful_tests}\")\n",
        "print(f\"‚ùå Falhas: {failed_tests}\")\n",
        "if total_tests > 0:\n",
        "    print(f\"üìä Taxa de sucesso: {successful_tests/total_tests*100:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DETALHES POR TESTE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, result in enumerate(results, 1):\n",
        "    status = \"‚úÖ\" if result.get('success', False) else \"‚ùå\"\n",
        "    print(f\"\\n{status} Teste {i}: {result['test']}\")\n",
        "    \n",
        "    if result.get('success'):\n",
        "        print(f\"   Campos extra√≠dos: {list(result.get('parsed_json', {}).keys())}\")\n",
        "    else:\n",
        "        print(f\"   Erro: {result.get('error', 'Desconhecido')}\")\n",
        "        if 'response' in result:\n",
        "            print(f\"   Resposta: {result['response'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Teste Interativo\n",
        "\n",
        "Voc√™ pode testar o modelo com seus pr√≥prios exemplos aqui. Modifique o prompt abaixo para testar diferentes casos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testando com prompt customizado...\n",
            "\n",
            "üì• Prompt:\n",
            "\n",
            "Short description: [Sua descri√ß√£o curta aqui]\n",
            "Content: [Descri√ß√£o detalhada do problema ou solicita√ß√£o]\n",
            "Preencha os seguintes campos em JSON:\n",
            "- category\n",
            "- subcategory\n",
            "- issue_request\n",
            "- assignment_group\n",
            "- software_system\n",
            "- poor_close_notes\n",
            "- resolution_time\n",
            "- info_score_close_notes\n",
            "\n",
            "\n",
            "üì§ Resposta do modelo:\n",
            "{\"category\": \"SOFTWARE\", \"subcategory\": \"ERROR\", \"issue_request\": \"Sua descri√ß√£o de problema aqui\", \"assignment_group\": \"TECH SUPPORT TEAM\", \"software_system\": \"Sua aplica√ß√£o aqui\", \"poor_close_notes\": \"Problema resolvido\", \"resolution_time\": 1208.749563, \"info_score_close_notes\": 0.8}\n",
            "\n",
            "‚úÖ JSON extra√≠do:\n",
            "{\n",
            "  \"category\": \"SOFTWARE\",\n",
            "  \"subcategory\": \"ERROR\",\n",
            "  \"issue_request\": \"Sua descri√ß√£o de problema aqui\",\n",
            "  \"assignment_group\": \"TECH SUPPORT TEAM\",\n",
            "  \"software_system\": \"Sua aplica√ß√£o aqui\",\n",
            "  \"poor_close_notes\": \"Problema resolvido\",\n",
            "  \"resolution_time\": 1208.749563,\n",
            "  \"info_score_close_notes\": 0.8\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Teste interativo - modifique o prompt abaixo\n",
        "custom_prompt = \"\"\"\n",
        "Short description: [Sua descri√ß√£o curta aqui]\n",
        "Content: [Descri√ß√£o detalhada do problema ou solicita√ß√£o]\n",
        "Preencha os seguintes campos em JSON:\n",
        "- category\n",
        "- subcategory\n",
        "- issue_request\n",
        "- assignment_group\n",
        "- software_system\n",
        "- poor_close_notes\n",
        "- resolution_time\n",
        "- info_score_close_notes\n",
        "\"\"\"\n",
        "\n",
        "print(\"Testando com prompt customizado...\")\n",
        "print(\"\\nüì• Prompt:\")\n",
        "print(custom_prompt)\n",
        "\n",
        "response = generate_response(custom_prompt)\n",
        "\n",
        "print(\"\\nüì§ Resposta do modelo:\")\n",
        "print(response)\n",
        "\n",
        "# Tentar parsear JSON\n",
        "try:\n",
        "    json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
        "    if json_match:\n",
        "        parsed = json.loads(json_match.group())\n",
        "        print(\"\\n‚úÖ JSON extra√≠do:\")\n",
        "        print(json.dumps(parsed, indent=2, ensure_ascii=False))\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  Nenhum JSON encontrado na resposta\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è  N√£o foi poss√≠vel extrair JSON: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
