{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Level 2: Hierarchical RAG\n",
    "\n",
    "This notebook will show you how to build a RAG application with Llama Stack using multi-field document indexing. You will learn how the API's provided by Llama Stack can be used to directly control and invoke all common RAG stages, including indexing, retrieval and inference. \n",
    "\n",
    "In this example, we'll work with IT call center tickets and combine multiple fields (`short_description`, `content`, and `close_notes`) to create richer document representations that improve retrieval quality and context understanding.\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers the following steps:\n",
    "1. Loading and preparing the dataset, combining multiple fields to create comprehensive document content.\n",
    "2. Indexing the enriched documents into a vector database (ChromaDB) for later retrieval.\n",
    "3. Executing the built-in RAG tool to retrieve the document chunks relevant to a given query.\n",
    "4. Using the retrieved context to answer user queries during the inference step.\n",
    "5. Understanding why multi-field RAG outperforms single-field RAG through practical query examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setting Up this Notebook\n",
    "\n",
    "First, we will start with a few imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from llama_stack_client import RAGDocument\n",
    "from llama_stack_client.types import Document\n",
    "from llama_stack_client.lib.agents.agent import Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the CSV file from the data directory\n",
    "data_dir = Path(\"../data\")\n",
    "file_path = data_dir / \"synthetic-it-call-center-tickets.csv\"\n",
    "\n",
    "# Carrega o CSV em um DataFrame do pandas\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Exibe as primeiras 5 linhas e informaÃ§Ãµes do DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add root src directory to path to import shared config\n",
    "root_dir = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(root_dir / \"src\"))\n",
    "\n",
    "# Import centralized configuration\n",
    "from config import LLAMA_STACK_URL, MODEL, CONFIG\n",
    "\n",
    "# For communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "# Configuration values (automatically detected based on environment)\n",
    "llamastack_url = LLAMA_STACK_URL\n",
    "model = MODEL\n",
    "\n",
    "if not llamastack_url:\n",
    "    raise ValueError(\n",
    "        \"LLAMA_STACK_URL is not configured!\\n\"\n",
    "        \"Please run: ./scripts/setup-env.sh\\n\"\n",
    "        \"Or set LLAMA_STACK_URL environment variable:\\n\"\n",
    "        \"  export LLAMA_STACK_URL='https://llamastack-route-my-first-model.apps.ocp.example.com'\"\n",
    "    )\n",
    "\n",
    "print(f\"ðŸ“¡ LlamaStack URL: {llamastack_url}\")\n",
    "print(f\"ðŸ¤– Model: {model}\")\n",
    "print(f\"ðŸ“ Environment: {'Inside OpenShift cluster' if CONFIG['inside_cluster'] else 'Outside OpenShift cluster'}\")\n",
    "print(f\"ðŸ“¦ Namespace: {CONFIG['namespace']}\")\n",
    "\n",
    "# Tavily search API key is optional (only needed for some demos)\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "client = LlamaStackClient(\n",
    "    base_url=llamastack_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    model_count = len(models.data) if hasattr(models, 'data') else len(models)\n",
    "    print(f\"\\nâœ… Connected to LlamaStack\")\n",
    "    print(f\"   Available models: {model_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Cannot connect to LlamaStack: {e}\")\n",
    "    print(\"\\nðŸ’¡ Troubleshooting:\")\n",
    "    print(\"   1. Check if route exists: oc get route llamastack-route -n my-first-model\")\n",
    "    print(\"   2. Run setup script: ./scripts/setup-env.sh\")\n",
    "    print(\"   3. Or set LLAMA_STACK_URL manually in .env file\")\n",
    "    raise\n",
    "\n",
    "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
    "model_id = \"ollama/llama3.2:3b\"\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Finally, we complete the setup by initializing the document collection we will use for RAG ingestion and retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2. Indexing the Documents\n",
    "- Initialize a new document collection in our vector database. All parameters related to the vector database, such as the embedding model and dimension, must be specified here.\n",
    "- Create RAG documents by combining multiple fields from the dataset: `short_description`, `content`, and `close_notes`. This provides richer context for better retrieval and understanding of the tickets.\n",
    "- Provide the list of documents to the RAG tool. Llama Stack will handle chunking and indexing the content into the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.providers.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Explicit - specify embedding model and/or provider when you need specific ones\n",
    "vs_chroma = client.vector_stores.create(\n",
    "    extra_body={\n",
    "        \"provider_id\": \"chromadb\",  # Optional: specify vector store provider\n",
    "        \"embedding_model\": \"sentence-transformers/nomic-ai/nomic-embed-text-v1.5\",\n",
    "        \"embedding_dimension\": 768  # Optional: will be auto-detected if not provided\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limita o dataset para os primeiros 1000 registros para processamento mais rÃ¡pido\n",
    "df_1000 = df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria documentos RAG combinando mÃºltiplos campos para enriquecer o contexto\n",
    "# Os campos 'short_description', 'content' e 'close_notes' sÃ£o combinados no conteÃºdo principal\n",
    "# Os demais campos sÃ£o armazenados como metadados para filtragem e referÃªncia\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"{df_1000.iloc[i]['short_description']}\\n\\n{df_1000.iloc[i]['content']}\\n\\n{df_1000.iloc[i]['close_notes']}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata=df_1000.iloc[i].drop([\"short_description\", \"content\", \"close_notes\"]).to_dict(),\n",
    "    )\n",
    "    for i in range(len(df_1000))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.tool_runtime.rag_tool.insert( \n",
    "    chunk_size_in_tokens=1024,\n",
    "    documents=documents,\n",
    "    vector_db_id=str(vs_chroma.id),\n",
    "    extra_body={\"vector_store_id\": str(vs_chroma.id)},\n",
    "    extra_headers=None,\n",
    "    extra_query=None,\n",
    "    timeout=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.vector_io.query(vector_db_id=vs_chroma.id,query=\"ZTrend crashes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 3. Executing Queries via the Built-in RAG Tool\n",
    "- Directly invoke the RAG tool to query the vector database we ingested into at the previous stage.\n",
    "- Construct an extended prompt using the retrieved chunks.\n",
    "- Query the model with the extended prompt.\n",
    "- Output the reply received from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What was the root cause and resolution for application crashes related to memory issues?\",\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # RAG retrieval call\n",
    "    rag_response = client.tool_runtime.rag_tool.query(\n",
    "        content=prompt,\n",
    "        vector_db_ids=[str(vs_chroma.id)],   # o SDK exige isso\n",
    "        extra_body={\"vector_store_ids\": [str(vs_chroma.id)]},  # o backend exige isso\n",
    "    )\n",
    "\n",
    "    print(rag_response.content)\n",
    "    # the list of messages to be sent to the model must start with the system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "    # construct the actual prompt to be executed, incorporating the original query and the retrieved content\n",
    "    prompt_context = rag_response.content\n",
    "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{prompt_context}\\n\\nQUERY:\\n{prompt}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "    # use Llama Stack inference API to directly communicate with the desired model\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        stream=stream,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "if stream:\n",
    "    for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "    print()  # nova linha apÃ³s streaming\n",
    "else:\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 4. Why Multi-Field RAG is Better: Example Queries\n",
    "\n",
    "Using multiple fields (`short_description`, `content`, and `close_notes`) instead of just `short_description` significantly improves retrieval quality for certain types of queries. Here are examples where multi-field RAG outperforms single-field RAG:\n",
    "\n",
    "### Example 1: Troubleshooting Steps and Solutions\n",
    "**Query**: \"How do I fix ZTrend crashes when saving files?\"\n",
    "\n",
    "- **Single-field (short_description only)**: May retrieve tickets about crashes, but won't have the solution steps\n",
    "- **Multi-field**: Retrieves tickets with both the problem description AND the detailed troubleshooting steps from `close_notes`, providing complete answers\n",
    "\n",
    "### Example 2: Historical Context and Resolution\n",
    "**Query**: \"What was the root cause and resolution for application crashes related to memory issues?\"\n",
    "\n",
    "- **Single-field**: Only finds tickets mentioning \"crashes\" but misses the diagnostic details and resolution steps\n",
    "- **Multi-field**: Retrieves tickets with full context from `content` (initial problem description) and `close_notes` (diagnostic findings and resolution), enabling comprehensive answers\n",
    "\n",
    "### Example 3: Pattern Recognition Across Problem-Solution Pairs\n",
    "**Query**: \"What are common solutions for software crashes that involve configuration files?\"\n",
    "\n",
    "- **Single-field**: Can identify crash-related tickets but can't see the solutions\n",
    "- **Multi-field**: Can match both problem patterns (from `short_description`/`content`) and solution patterns (from `close_notes`), enabling identification of recurring problem-solution patterns\n",
    "\n",
    "### Example 4: Detailed Technical Information\n",
    "**Query**: \"Show me tickets where log file analysis revealed the issue\"\n",
    "\n",
    "- **Single-field**: May miss tickets where log analysis is only mentioned in `content` or `close_notes`\n",
    "- **Multi-field**: Captures technical details from all fields, ensuring comprehensive retrieval of relevant tickets\n",
    "\n",
    "### Example 5: End-to-End Ticket Understanding\n",
    "**Query**: \"Find tickets where the customer reported a problem, diagnostics were performed, and the issue was resolved by reinstalling software\"\n",
    "\n",
    "- **Single-field**: Can't capture the full narrative flow from problem â†’ diagnosis â†’ solution\n",
    "- **Multi-field**: Preserves the complete ticket lifecycle, enabling retrieval based on complex multi-stage scenarios\n",
    "\n",
    "**Key Insight**: Multi-field RAG is especially powerful for queries that require understanding both the problem AND the solution, or queries that need to match patterns across different stages of the ticket lifecycle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "This notebook demonstrated how to set up and use the built-in RAG tool for ingesting user-provided documents in a vector database and utilizing them during inference via direct retrieval. \n",
    "\n",
    "Key points:\n",
    "- **Multi-field content**: We combined `short_description`, `content`, and `close_notes` fields to create richer document representations, improving the quality of retrieval and context understanding.\n",
    "- **Metadata preservation**: Other fields from the dataset are stored as metadata, allowing for filtering and additional context during retrieval.\n",
    "- **Vector database integration**: The documents are chunked and indexed into ChromaDB using Llama Stack's RAG tool, enabling semantic search over the ticket data.\n",
    "- **Query advantages**: As shown in Section 4, multi-field RAG excels at queries requiring both problem and solution context, pattern recognition across ticket lifecycle stages, and comprehensive technical information retrieval.\n",
    "\n",
    "Now that we've seen how easy it is to implement RAG with Llama Stack, We'll move on to building a simple agent with Llama Stack next in our [Simple Agents](./Level2_simple_agent_with_websearch.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
