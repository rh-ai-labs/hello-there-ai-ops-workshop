# üìä Project Status Document

**Project:** AI Test Drive ‚Äì Cen√°rio 2: Enriquecendo Incidentes com IA  
**Last Updated:** November 2025  
**Status:** üü° In Progress

---

## ‚úÖ What Has Been Completed

### 1. Project Foundation
- ‚úÖ **Project structure** - Created basic directory structure (`src/`, `notebooks/`, `data/`)
- ‚úÖ **requirements.txt** - Defined all dependencies including:
  - Core data science libraries (pandas, numpy, matplotlib, seaborn)
  - Hugging Face datasets and transformers
  - LangChain for LLM integration
  - MLflow for experiment tracking
  - Evaluation libraries (rouge-score, sentence-transformers)
- ‚úÖ **.gitignore** - Python/Jupyter/ML project specific ignore patterns

### 2. Source Code Modules (`src/`)

#### ‚úÖ `utils.py` - Utility Functions
- `load_incident_dataset()` - Loads dataset from Hugging Face with sampling support
- `prepare_incident_for_enrichment()` - Prepares incident data for LLM prompts
- `calculate_basic_stats()` - Computes dataset statistics
- `save_enriched_results()` - Saves results to CSV
- `load_ground_truth_embeddings()` - Loads pre-computed embeddings for ground truth
- `compute_semantic_similarity()` - Computes semantic similarity between two texts
- `find_most_similar_close_note()` - Finds most semantically similar close notes

#### ‚úÖ `prompts.py` - Prompt Templates
- `get_base_enrichment_prompt()` - Basic enrichment prompt
- `get_structured_enrichment_prompt()` - JSON-structured output prompt
- `get_minimal_enrichment_prompt()` - Minimal prompt (Scenario A - general LLM)
- `get_detailed_enrichment_prompt()` - Detailed prompt (Scenario B - tuned LLM)
- `get_prompt_variants()` - Returns dictionary of all prompt variants

#### ‚úÖ `evaluator.py` - Evaluation Framework
- `IncidentEnrichmentEvaluator` class with:
  - `evaluate_with_ground_truth()` - ROUGE scores and semantic similarity
  - `evaluate_json_structure()` - Validates JSON structure and required fields
  - `evaluate_specificity()` - Measures specificity (avoids vague statements)
  - `evaluate_actionability()` - Assesses actionability of resolution steps
  - `comprehensive_evaluation()` - Combines all metrics into overall quality score
  - **Embedding Model:** Uses `BAAI/bge-m3` with FlagEmbedding fallback support
  - Supports multilingual, multi-granularity embeddings (100+ languages, up to 8,192 tokens)

#### ‚úÖ `mlflow_tracking.py` - Experiment Tracking
- `setup_mlflow()` - Configures MLflow experiment
- `log_incident_enrichment_run()` - Logs individual runs with metrics
- `log_comparison_run()` - Logs comparison between Scenario A and B
- `flatten_dict()` - Helper for nested dictionary logging
- `get_best_run()` - Retrieves best run based on metrics

### 3. Notebooks

#### ‚úÖ `01_load_and_explore_dataset.ipynb` - **COMPLETE**
Comprehensive data exploration notebook with:
- Dataset loading from Hugging Face
- Basic dataset overview (structure, types, missing values)
- Dataset statistics calculation
- 9-panel visualization dashboard
- 4-panel content quality analysis
- Sample incident examination
- Data preparation for experiments
- Saves prepared datasets (`incidents_prepared.csv`, `incidents_sample.csv`)

#### ‚úÖ `02_create_ground_truth.ipynb` - **COMPLETE**
Ground truth dataset creation notebook with:
- High-quality close notes filtering (info_score ‚â• 0.8, poor_score ‚â§ 0.1)
- Generic phrase exclusion
- Balanced sampling across categories
- Ground truth dataset creation (`gt_close_notes.csv`)
- **Semantic Embeddings Analysis:**
  - Embedding generation using BGE-M3 model
  - Semantic similarity analysis within and between categories
  - t-SNE visualization for cluster analysis
  - Embeddings saved for future evaluation (`gt_close_notes_embeddings.npy`)
  - Metadata tracking (`gt_close_notes_embeddings_metadata.pkl`)

### 4. Dataset Understanding
- ‚úÖ Dataset loaded and explored
- ‚úÖ Understanding of actual columns:
  - Primary columns: `number`, `type`, `date`, `contact_type`, `short_description`, `content`, `category`, `subcategory`, `customer`, `resolved_at`, `close_notes`, `agent`, `reassigned_count`, `resolution_time`, `issue/request`, `software/system`, `assignment_group`, `info_score_close_notes`
  - Categories: SOFTWARE (dominant), ACCOUNT, PIV CARD, EMAIL, PRINTER, NETWORK
  - Subcategories: ERROR, MALFUNCTION, CONFIGURATION, INSTALLATION, ACCESS, etc.
  - Contact types: Email, Chat, Phone, Self-service
  - Types: Incident, Request

### 5. Ground Truth Dataset ‚úÖ **COMPLETE**
- ‚úÖ **Ground truth dataset created** (`data/gt_close_notes.csv`)
  - High-quality close notes filtered and validated
  - Balanced sampling across categories
  - 26 high-quality reference examples with metadata
- ‚úÖ **Semantic embeddings generated**
  - Model: `BAAI/bge-m3` (multilingual, multi-granularity)
  - Semantic similarity analysis implemented
  - t-SNE visualization for cluster exploration
  - Embeddings saved for evaluation pipeline
- ‚úÖ **Embedding utilities**
  - Support for FlagEmbedding fallback
  - Functions for loading and using pre-computed embeddings
  - Semantic similarity computation functions

---

## üöß Next Steps - Implementation Roadmap

### üéØ Phase 1: Ground Truth Creation ‚úÖ **COMPLETE**

#### ‚úÖ Step 1: Criar o Ground Truth de `close_notes` - **COMPLETED**

**Status:** ‚úÖ Implementado e conclu√≠do

**Entregas:**
- ‚úÖ Notebook `02_create_ground_truth.ipynb` criado e funcional
- ‚úÖ Ground truth dataset `data/gt_close_notes.csv` criado (26 exemplos de alta qualidade)
- ‚úÖ Embeddings sem√¢nticos gerados usando BGE-M3
- ‚úÖ An√°lise de similaridade sem√¢ntica implementada
- ‚úÖ Visualiza√ß√£o t-SNE para explora√ß√£o de clusters
- ‚úÖ Arquivos de embeddings salvos para uso futuro:
  - `data/gt_close_notes_embeddings.npy` (embeddings array)
  - `data/gt_close_notes_embeddings_metadata.pkl` (metadados)

**Caracter√≠sticas do Dataset:**
- Filtros aplicados: `info_score_close_notes` ‚â• 0.8, `info_score_poor_close_notes` ‚â§ 0.1
- Exclus√£o de frases gen√©ricas
- Amostragem balanceada por categoria
- Metadados inclu√≠dos: category, subcategory, contact_type, info_score

**Modelo de Embedding:**
- Modelo: `BAAI/bge-m3`
- Suporte multil√≠ngue (100+ idiomas)
- Multi-granularidade (frases a documentos longos)
- Fallback para FlagEmbedding se necess√°rio

---

### üéØ Phase 2: N-gram Baseline Analysis

#### üìã Step 2: Baseline N-gram Comparison (gt_close_notes √ó incident_descriptions)

**Objetivo:** Realizar uma an√°lise explorat√≥ria para testar se m√©tricas n-gram s√£o √∫teis para avaliar qualidade de close notes.

**Contexto:**
1. Temos um dataset de incidentes com descri√ß√µes originais (`content`)
2. Extra√≠mos close notes de alta qualidade para servir como refer√™ncia (`close_notes_ref`)
3. Objetivo final: Avaliar close notes (existentes ou geradas por LLM) contra essas refer√™ncias

**Hip√≥tese:** Descri√ß√µes de incidentes e close notes usam linguagem muito diferente, tornando m√©tricas n-gram menos √∫teis para avalia√ß√£o.

**Teste:** Comparar close notes de ground truth vs descri√ß√µes de incidentes usando m√©tricas n-gram.

**Resultado Esperado:** Se os scores n-gram forem muito baixos (0.1-0.3), confirma que descri√ß√µes de incidentes e close notes usam vocabul√°rio diferente, validando que devemos usar **LLM-as-a-Judge** (avalia√ß√£o sem√¢ntica) em vez de n-grams para a avalia√ß√£o principal.

**Por que usar Unitxt?**

‚úÖ **Padroniza√ß√£o**: Framework padronizado para avalia√ß√£o de modelos  
‚úÖ **Efici√™ncia**: M√©tricas pr√©-implementadas e otimizadas  
‚úÖ **Escalabilidade**: Processamento eficiente de grandes volumes  
‚úÖ **Manutenibilidade**: Menos c√≥digo customizado para manter  
‚úÖ **Reprodutibilidade**: Resultados consistentes e compar√°veis

**Abordagem:** Usar Unitxt para realizar compara√ß√µes n-gram entre:
- **Ground Truth Dataset**: `data/gt_close_notes.csv` (cont√©m `close_notes_ref`)
- **Incidents Dataset**: `data/incidents_prepared.csv` (cont√©m `content` - descri√ß√µes de problemas)

**Nota:** Esta compara√ß√£o √© um **teste de baseline**. A avalia√ß√£o real ser√° feita na Phase 4 usando LLM-as-a-Judge, que compara close notes contra close notes usando crit√©rios sem√¢nticos.

**Etapas:**

1. **Preparar datasets para Unitxt:**
   - Carregar `gt_close_notes.csv` com campo `close_notes_ref`
   - Carregar `incidents_prepared.csv` com campo `content`
   - Estruturar dados no formato esperado pelo Unitxt

2. **Configurar m√©tricas n-gram no Unitxt:**
   - ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-Lsum (n-gram overlap)

3. **Executar compara√ß√µes:**
   - Para cada par (ground truth close note, incident description)
   - Calcular m√©tricas n-gram usando Unitxt
   - Agregar resultados por categoria/subcategoria

4. **Analisar resultados e concluir:**
   - Gerar visualiza√ß√µes de distribui√ß√£o de scores
   - Identificar padr√µes por categoria
   - **Conclus√£o:** Se scores s√£o baixos, confirma hip√≥tese e valida uso de LLM-as-a-Judge

**Deliverable:** 
- Notebook `notebooks/03_ngram_comparisons.ipynb` usando Unitxt
- M√©tricas n-gram comparando `close_notes_ref` vs `content`
- Visualiza√ß√µes e an√°lise de resultados
- Conclus√£o sobre relev√¢ncia de n-grams para avalia√ß√£o

**Notebook:** ‚úÖ Criado `notebooks/03_ngram_comparisons.ipynb` usando Unitxt

**Dependencies:** 
- ‚úÖ `data/gt_close_notes.csv` (do Step 1) - **COMPLETE**
- ‚úÖ `data/incidents_prepared.csv` - **COMPLETE**
- ‚úÖ Unitxt instalado (`unitxt>=1.0.0`) - **COMPLETE**
- ‚úÖ Configura√ß√£o Unitxt para n-gram metrics - **COMPLETE**

**Status:** ‚úÖ **COMPLETE** - Notebook criado e funcional

---

### üéØ Phase 3: Semantic Baseline Analysis (Optional)

#### üìã Step 3: Baseline Semantic Comparison (gt_close_notes √ó incident_descriptions)

**Status:** üü° **OPTIONAL** - Pode ser pulado se Phase 2 j√° confirmar que m√©tricas tradicionais n√£o s√£o adequadas

**Objetivo:** Comparar o texto de refer√™ncia (`close_notes_ref` do ground truth dataset) com os textos de incidentes (`content` do incidents dataset) usando m√©tricas de similaridade sem√¢ntica como an√°lise complementar.

**Princ√≠pio:** Usar embeddings sem√¢nticos para medir similaridade de significado entre textos, capturando rela√ß√µes que m√©tricas n-gram n√£o conseguem capturar.

**Nota:** Similar √† Phase 2, esta √© uma an√°lise de baseline. Se Phase 2 j√° confirmar que m√©tricas tradicionais n√£o s√£o adequadas, esta fase pode ser opcional. A avalia√ß√£o principal ser√° feita na Phase 4 usando LLM-as-a-Judge.

**Por que usar Unitxt?**

‚úÖ **Padroniza√ß√£o**: Framework padronizado para avalia√ß√£o de modelos  
‚úÖ **Efici√™ncia**: M√©tricas de similaridade sem√¢ntica pr√©-implementadas  
‚úÖ **Escalabilidade**: Processamento eficiente de grandes volumes  
‚úÖ **Manutenibilidade**: Menos c√≥digo customizado para manter  
‚úÖ **Reprodutibilidade**: Resultados consistentes e compar√°veis

**Abordagem:** Usar Unitxt para realizar compara√ß√µes sem√¢nticas entre:
- **Ground Truth Dataset**: `data/gt_close_notes.csv` (cont√©m `close_notes_ref`)
- **Incidents Dataset**: `data/incidents_prepared.csv` (cont√©m `content`)

**Etapas:**

1. **Preparar datasets para Unitxt:**
   - Carregar `gt_close_notes.csv` com campo `close_notes_ref`
   - Carregar `incidents_prepared.csv` com campo `content`
   - Estruturar dados no formato esperado pelo Unitxt

2. **Configurar m√©tricas sem√¢nticas no Unitxt:**
   - Cosine similarity usando embeddings (sentence-transformers)
   - Semantic similarity scores
   - Opcionalmente usar embeddings pr√©-computados (`gt_close_notes_embeddings.npy`)

3. **Executar compara√ß√µes sem√¢nticas:**
   - Para cada par (ground truth close note, incident description)
   - Calcular similaridade sem√¢ntica usando Unitxt
   - Comparar com m√©tricas n-gram da Phase 2
   - Agregar resultados por categoria/subcategoria

4. **Analisar resultados:**
   - Gerar visualiza√ß√µes de distribui√ß√£o de scores sem√¢nticos
   - Comparar com scores n-gram (Phase 2)
   - Identificar padr√µes por categoria
   - Conclus√£o sobre relev√¢ncia de m√©tricas sem√¢nticas

**Deliverable:** 
- Notebook `notebooks/04_semantic_comparisons.ipynb` usando Unitxt (opcional)
- M√©tricas de similaridade sem√¢ntica comparando `close_notes_ref` vs `content`
- Visualiza√ß√µes e an√°lise comparativa com Phase 2

**Notebook:** üü° Criar `notebooks/04_semantic_comparisons.ipynb` usando Unitxt (opcional)

**Dependencies:** 
- ‚úÖ `data/gt_close_notes.csv` (do Step 1) - **COMPLETE**
- ‚úÖ `data/gt_close_notes_embeddings.npy` (opcional) - **COMPLETE**
- ‚úÖ `data/incidents_prepared.csv` - **COMPLETE**
- ‚úÖ Unitxt instalado (`unitxt>=1.0.0`) - **COMPLETE**
- ‚úÖ Sentence-transformers/BGE-M3 para embeddings - **COMPLETE**
- üî¥ Configura√ß√£o Unitxt para semantic similarity metrics - **OPTIONAL**

**Refer√™ncia:** Unitxt documentation for semantic similarity metrics

---                                | Description                                                                            | Example of ‚ÄúGood‚Äù (Score 5)                                                                              |
| ---------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- |
| **Incident Coverage**                    | Does the generated note cover the same core problem and context as the reference note? | Mentions the same issue (e.g., ‚ÄúGoogle Workspace crashing when saving files‚Äù) and troubleshooting scope. |
| **Technical Steps & Resolution Actions** | Are the main diagnostic or corrective steps included and technically sound?            | Lists actions such as ‚Äúcleared cache,‚Äù ‚Äúreinstalled software,‚Äù or ‚Äúupdated drivers.‚Äù                     |
| **Accuracy of Facts**                    | Does it avoid adding or changing facts not present in the reference note?              | No new systems, error codes, or users invented.                                                          |
| **Customer/System Context**              | Does it correctly reference the affected system, user, or service?                     | Correctly identifies the impacted system (e.g., ‚ÄúEpson ET-2760 printer‚Äù) and user role.                  |
| **Clarity & Structure**                  | Is the note logically structured (problem ‚Üí action ‚Üí result)?                          | Uses concise sentences, chronological order, and readable formatting.                                    |
| **Resolution Summary / Conclusion**      | Does it clearly describe the outcome and confirm resolution or escalation?             | Ends with ‚ÄúIssue resolved and verified with user‚Äù or equivalent closure statement.                       |

Each dimension is rated from **0 to 5**, where 5 = excellent alignment, 0 = completely incorrect.

---

## üß© Step 2 ‚Äî Judge Prompt Template

The evaluation LLM (judge) must follow a **structured JSON output**, ensuring consistency and automation.
This is the reusable prompt you‚Äôll give to the evaluator model:

---

> **System Prompt:**
> You are an expert in IT Service Management and incident documentation.
> Your task is to evaluate how accurately and completely a *generated close note* describes the resolution of an incident, compared to a *reference note*.
>
> Compare the following texts:
>
> * **Reference (ground truth) close note:**
>   {close_notes_ref}
>
> * **Generated close note:**
>   {close_notes_pred}
>
> Evaluate the generated note according to the following criteria.
> For each, assign a **score from 0 to 5** and include a one-sentence explanation.
>
> 1. **Incident coverage (0‚Äì5)** ‚Äî Does it address the same issue and context?
> 2. **Technical steps & resolution actions (0‚Äì5)** ‚Äî Are the main diagnostic and corrective actions consistent and complete?
> 3. **Accuracy of facts (0‚Äì5)** ‚Äî Does it avoid inventing systems, errors, or results?
> 4. **Customer/system context (0‚Äì5)** ‚Äî Does it correctly reference the affected service, device, or user?
> 5. **Clarity & structure (0‚Äì5)** ‚Äî Is it readable, logically ordered, and professionally written?
> 6. **Resolution summary (0‚Äì5)** ‚Äî Does it clearly describe the outcome or confirmation of resolution?
>
> Then compute:
>
> * `"general_score"` ‚Äî the average of the six scores
> * `"general_score_explanation"` ‚Äî a brief summary of your overall judgment
>
> Return the evaluation as valid JSON only:
>
> ```json
> {
>   "check_incident_coverage": 5,
>   "check_incident_coverage_explanation": "...",
>   "check_technical_steps": 5,
>   "check_technical_steps_explanation": "...",
>   "check_accuracy_of_facts": 5,
>   "check_accuracy_of_facts_explanation": "...",
>   "check_customer_context": 5,
>   "check_customer_context_explanation": "...",
>   "check_clarity_structure": 4,
>   "check_clarity_structure_explanation": "...",
>   "check_resolution_summary": 5,
>   "check_resolution_summary_explanation": "...",
>   "general_score": 4.83,
>   "general_score_explanation": "The generated close note accurately covers the same incident, includes consistent troubleshooting steps, and provides a clear resolution summary with no invented facts."
> }
> ```

---

## üßÆ Step 3 ‚Äî Scoring Standards

| Score             | Interpretation                                                    | Example                                                             |
| ----------------- | ----------------------------------------------------------------- | ------------------------------------------------------------------- |
| **5 (Excellent)** | Fully accurate and complete; aligns perfectly with the reference. | Mentions identical issue, actions, and outcome in a structured way. |
| **4 (Good)**      | Mostly accurate with minor omissions or paraphrasing.             | Slightly simplified version but conveys same meaning.               |
| **3 (Adequate)**  | Covers the main idea but misses important details.                | Omits one or two troubleshooting steps.                             |
| **2 (Weak)**      | Only partially correct; vague or incomplete.                      | Describes the issue but not the fix.                                |
| **1 (Poor)**      | Misleading or incorrect content.                                  | Introduces wrong system or incorrect result.                        |
| **0 (Invalid)**   | Completely unrelated or hallucinated.                             | Talks about something entirely different.                           |

---

## üîç Step 4 ‚Äî How to Run the Evaluation

1. **Select dataset:** use incident records for which we have:
   - *Reference* close note (`close_notes_ref` from ground truth)
   - *Close note to evaluate* (`close_notes` from dataset, or LLM-generated)
   
   **Note:** Each incident has different context, so we match by category/similarity or use same incident pairs where available.

2. **Send each pair** (`close_notes_ref`, `close_notes_to_evaluate`) through the judge prompt.
3. **Collect JSON outputs** for all samples.
4. **Aggregate scores** across all dimensions and samples:

   * Mean score per dimension;
   * Mean `general_score`;
   * Distribution of low scores (to detect weak generations).

This can be done as a batch notebook or automated evaluation job.

**Challenges & Solutions:**
- **Challenge:** Each incident has different context ‚Üí different close notes expected
- **Solution:** Match by category/subcategory, or use semantic similarity to find similar incidents
- **Solution:** Provide context (incident description) to judge for better evaluation

---

## üìä Step 5 ‚Äî Interpreting Results

* **High-performing models** should achieve:

  * `general_score ‚â• 4.0`
  * No dimension consistently below 3.5.

* **Low-performing prompts/models** typically show:

  * Low ‚Äútechnical steps‚Äù or ‚Äúaccuracy of facts‚Äù scores;
  * Inconsistent coverage or missing conclusions.

Use these scores to rank:

* Prompt templates;
* Model versions (generic vs. fine-tuned);
* Post-processing strategies (structured outputs vs. free text).

---

## üîó Step 6 ‚Äî Integrating with Langfuse & MLflow

### **Langfuse**

* Log every evaluation as a trace:

  * `input` = reference note + generated note,
  * `output` = JSON with judge scores,
  * Tags for each dimension and score (e.g., `check_technical_steps=4`).
* Build dashboards to track score evolution over prompt versions.

### **MLflow**

* Log:

  * `general_score_mean` per experiment,
  * All six sub-scores as metrics,
  * Associated prompt/model identifiers.
* Use these logs to correlate **model quality vs. cost vs. runtime**.

---

## ‚úÖ Deliverables Checklist

By the end of the LLM-as-a-Judge setup, you should have:

* [ ] **Evaluation schema** (the six ITSM-specific dimensions)
* [ ] **Reference dataset (`gt_close_notes.csv`)**
* [ ] **Judge prompt template (structured JSON)**
* [ ] **Scoring scale (0‚Äì5)** documented

**Usando Llama Stack `/scoring` API:**

- Criar scoring function customizada para LLM-as-a-Judge
- Configurar prompt de avalia√ß√£o com crit√©rios espec√≠ficos
- Usar `/scoring` API para executar avalia√ß√µes em batch
- Obter scores estruturados e explica√ß√µes qualitativas

**Crit√©rios de avalia√ß√£o recomendados:**

1. **Cobertura de t√≥pico** ‚Äì O texto cobre os mesmos aspectos do problema?
2. **Uso de dados do cliente** ‚Äì Cont√©m as informa√ß√µes corretas sobre o contexto?
3. **Fatos de suporte** ‚Äì Inclui os mesmos fatos ou passos de resolu√ß√£o?
4. **Aus√™ncia de inven√ß√µes** ‚Äì Evita criar informa√ß√µes inexistentes?
5. **Estrutura e clareza** ‚Äì Est√° bem organizado e compreens√≠vel?
6. **Conclus√£o** ‚Äì Apresenta fechamento adequado e explicativo?

**Sistema de pontua√ß√£o:**
- Cada crit√©rio pontuado de 0 a 5
- Score geral calculado com base na m√©dia ponderada
- Incluir explica√ß√µes qualitativas para cada crit√©rio
- Configurar via scoring function no Llama Stack

**Resultados esperados:**
- **Respostas boas** ‚Üí score m√©dio entre 4 e 5
- **Respostas medianas ou vagas** ‚Üí score entre 2,5 e 3,5
- **Respostas ruins ou inventadas** ‚Üí score < 2,5

**Benef√≠cios:**
- An√°lise reprodut√≠vel, r√°pida e escal√°vel
- Feedback instant√¢neo durante a itera√ß√£o de prompts
- Integra√ß√£o nativa com Llama Stack e outros componentes
- Pode ser integrada com Langfuse e MLflow para rastrear resultados

**Mitiga√ß√£o de vieses:**
- **Position swapping**: Trocar posi√ß√µes de refer√™ncia e resultado para contrarrestar vi√©s de posi√ß√£o
- **Few-shot prompting**: Adicionar exemplos ao scoring function para calibrar avaliador
- **Awareness**: Estar ciente de que LLMs podem preferir texto gerado por LLM sobre texto humano

**Deliverable:** 
- Scoring function para LLM-as-a-Judge no Llama Stack
- Notebook demonstrando uso do `/scoring` API
- Integra√ß√£o com pipeline de avalia√ß√£o existente

**Notebook:** Criar `notebooks/04_llm_as_judge_evaluation.ipynb` usando Llama Stack `/scoring` API

**Dependencies:**
- Llama Stack configurado (Phase 2)
- LLM integration (ver Step 5)
- `data/gt_close_notes.csv`

---

### üéØ Phase 4: LLM-as-a-Judge Evaluation ‚≠ê **MAIN EVALUATION METHOD**

#### üìã Step 4: LLM-as-a-Judge Evaluation

**Objetivo:** Usar um modelo de linguagem como avaliador autom√°tico para comparar **close notes** (existentes ou geradas por LLM) com **close notes de refer√™ncia** (ground truth), substituindo (ou complementando) revis√µes humanas.

**Este √© o m√©todo de avalia√ß√£o principal** que ser√° usado para avaliar qualidade de close notes, superando as limita√ß√µes das m√©tricas n-gram e sem√¢nticas (Phase 2 e 3).

**Princ√≠pio:** O LLM √© instru√≠do a comparar dois textos de close notes: o gerado/avaliado e o de refer√™ncia (ground truth). Ele analisa o quanto o texto avaliado cobre os mesmos pontos, √© claro, completo e n√£o inventa informa√ß√µes.

**Compara√ß√£o:**
- **Refer√™ncia**: Close notes de ground truth (`close_notes_ref`)
- **Avaliado**: Close notes existentes (`close_notes` do dataset) ou geradas por LLM

**Nota:** Diferente das Phases 2 e 3, aqui comparamos **close notes vs close notes**, n√£o close notes vs descri√ß√µes de incidentes.

## üéØ Objective

Evaluate how well a model-generated *close_note* summarizes and documents the resolution of an IT incident, compared to a *reference (ground truth)* close note.

The goal is to measure:

* **Accuracy** ‚Äî Are the steps and facts consistent with the reference?
* **Completeness** ‚Äî Does the note include all essential resolution details?
* **Clarity** ‚Äî Is the note written in a clear, professional IT support style?

This approach uses an **LLM as a structured evaluator ("judge")** to produce **quantitative (scores)** and **qualitative (explanations)** feedback ‚Äî replicating the *SumUp* benchmark method, but focused on ITSM workflows.

---

## ‚öôÔ∏è Step 1 ‚Äî Define Evaluation Dimensions for ITSM Context

Each generated *close_note* is assessed along **six concrete quality dimensions** relevant to incident and service management documentation:

| Dimension                                | Description                                                                            | Example of "Good" (Score 5)                                                                              |
| ---------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- |
| **Incident Coverage**                    | Does the generated note cover the same core problem and context as the reference note? | Mentions the same issue (e.g., "Google Workspace crashing when saving files") and troubleshooting scope. |
| **Technical Steps & Resolution Actions** | Are the main diagnostic or corrective steps included and technically sound?            | Lists actions such as "cleared cache," "reinstalled software," or "updated drivers."                     |
| **Accuracy of Facts**                    | Does it avoid adding or changing facts not present in the reference note?              | No new systems, error codes, or users invented.                                                          |
| **Customer/System Context**              | Does it correctly reference the affected system, user, or service?                     | Correctly identifies the impacted system (e.g., "Epson ET-2760 printer") and user role.                  |
| **Clarity & Structure**                  | Is the note logically structured (problem ‚Üí action ‚Üí result)?                          | Uses concise sentences, chronological order, and readable formatting.                                    |
| **Resolution Summary / Conclusion**      | Does it clearly describe the outcome and confirm resolution or escalation?             | Ends with "Issue resolved and verified with user" or equivalent closure statement.                       |

Each dimension is rated from **0 to 5**, where 5 = excellent alignment, 0 = completely incorrect.

---

## üß© Step 2 ‚Äî Judge Prompt Template

The evaluation LLM (judge) must follow a **structured JSON output**, ensuring consistency and automation.
This is the reusable prompt you'll give to the evaluator model:

---

> **System Prompt:**
> You are an expert in IT Service Management and incident documentation.
> Your task is to evaluate how accurately and completely a *generated close note* describes the resolution of an incident, compared to a *reference note*.
>
> Compare the following texts:
>
> * **Reference (ground truth) close note:**
>   {close_notes_ref}
>
> * **Generated close note:**
>   {close_notes_pred}
>
> Evaluate the generated note according to the following criteria.
> For each, assign a **score from 0 to 5** and include a one-sentence explanation.
>
> 1. **Incident coverage (0‚Äì5)** ‚Äî Does it address the same issue and context?
> 2. **Technical steps & resolution actions (0‚Äì5)** ‚Äî Are the main diagnostic and corrective actions consistent and complete?
> 3. **Accuracy of facts (0‚Äì5)** ‚Äî Does it avoid inventing systems, errors, or results?
> 4. **Customer/system context (0‚Äì5)** ‚Äî Does it correctly reference the affected service, device, or user?
> 5. **Clarity & structure (0‚Äì5)** ‚Äî Is it readable, logically ordered, and professionally written?
> 6. **Resolution summary (0‚Äì5)** ‚Äî Does it clearly describe the outcome or confirmation of resolution?
>
> Then compute:
>
> * `"general_score"` ‚Äî the average of the six scores
> * `"general_score_explanation"` ‚Äî a brief summary of your overall judgment
>
> Return the evaluation as valid JSON only:
>
> ```json
> {
>   "check_incident_coverage": 5,
>   "check_incident_coverage_explanation": "...",
>   "check_technical_steps": 5,
>   "check_technical_steps_explanation": "...",
>   "check_accuracy_of_facts": 5,
>   "check_accuracy_of_facts_explanation": "...",
>   "check_customer_context": 5,
>   "check_customer_context_explanation": "...",
>   "check_clarity_structure": 4,
>   "check_clarity_structure_explanation": "...",
>   "check_resolution_summary": 5,
>   "check_resolution_summary_explanation": "...",
>   "general_score": 4.83,
>   "general_score_explanation": "The generated close note accurately covers the same incident, includes consistent troubleshooting steps, and provides a clear resolution summary with no invented facts."
> }
> ```
>
> ---
>
> ## üßÆ Step 3 ‚Äî Scoring Standards
>
> | Score             | Interpretation                                                    | Example                                                             |
> | ----------------- | ----------------------------------------------------------------- | ------------------------------------------------------------------- |
> | **5 (Excellent)** | Fully accurate and complete; aligns perfectly with the reference. | Mentions identical issue, actions, and outcome in a structured way. |
> | **4 (Good)**      | Mostly accurate with minor omissions or paraphrasing.             | Slightly simplified version but conveys same meaning.               |
> | **3 (Adequate)**  | Covers the main idea but misses important details.                | Omits one or two troubleshooting steps.                             |
> | **2 (Weak)**      | Only partially correct; vague or incomplete.                      | Describes the issue but not the fix.                                |
> | **1 (Poor)**      | Misleading or incorrect content.                                  | Introduces wrong system or incorrect result.                        |
> | **0 (Invalid)**   | Completely unrelated or hallucinated.                             | Talks about something entirely different.                           |
>
> ---
>
> ## üîç Step 4 ‚Äî How to Run the Evaluation
>
> 1. **Select dataset:** use the same incident records for which both a *reference* and *generated* close note exist.
> 2. **Send each pair** (`close_notes_ref`, `close_notes_pred`) through the judge prompt.
> 3. **Collect JSON outputs** for all samples.
> 4. **Aggregate scores** across all dimensions and samples:
>
>    * Mean score per dimension;
>    * Mean `general_score`;
>    * Distribution of low scores (to detect weak generations).
>
> This can be done as a batch notebook or automated evaluation job.

**Implementa√ß√£o:**

- Usar LangChain para orquestra√ß√£o de chamadas LLM
- Integrar com Ollama ou outros providers LLM
- Processar avalia√ß√µes em batch
- Armazenar resultados para an√°lise
- **Estrutura de avalia√ß√£o:**
  - Para cada close note a avaliar:
    1. Encontrar close note de refer√™ncia similar (por categoria ou similaridade sem√¢ntica)
    2. Opcionalmente incluir contexto do incident (`content`) para melhor avalia√ß√£o
    3. Enviar par (refer√™ncia, avaliado) para LLM judge
    4. Obter scores estruturados (JSON) com explica√ß√µes

**Mitiga√ß√£o de Vieses:**
- **Position swapping**: Trocar posi√ß√µes de refer√™ncia e resultado para contrarrestar vi√©s de posi√ß√£o
- **Few-shot prompting**: Adicionar exemplos ao prompt para calibrar avaliador
- **Context awareness**: Incluir descri√ß√£o do incident para melhor contexto

**Deliverable:** 
- Notebook `notebooks/05_llm_as_judge_evaluation.ipynb`
- M√≥dulo `src/llm_judge.py` com implementa√ß√£o do judge
- Integra√ß√£o com pipeline de avalia√ß√£o existente
- M√©tricas agregadas e visualiza√ß√µes
- Exemplos de boas e ruins gera√ß√µes com scores correspondentes

**Notebook:** üî¥ Criar `notebooks/05_llm_as_judge_evaluation.ipynb`

**Dependencies:**
- ‚úÖ `data/gt_close_notes.csv` - **COMPLETE**
- ‚úÖ LangChain instalado - **COMPLETE**
- üî¥ LLM integration (Ollama ou outro provider) - **NEEDED**
- üî¥ LLM Client implementation (Phase 6) - **NEEDED**

---

### üéØ Phase 5: Observability Integration

#### üìã Step 5: Integra√ß√£o com Langfuse

**Objetivo:** Centralizar os logs de prompts, respostas, m√©tricas e julgamentos dos modelos.

**O que monitorar:**

- Cada chamada de modelo (prompt ‚Üí resposta)
- Tempo de execu√ß√£o e custo
- Score de avalia√ß√£o (sem√¢ntica ou via LLM-as-a-judge)
- Hist√≥rico de vers√µes de prompts e modelos
- Compara√ß√£o entre diferentes configura√ß√µes

**Como usar na pr√°tica:**

1. **Registrar automaticamente cada itera√ß√£o:**
   - Cada chamada de prompt/modelo
   - Cada resultado de avalia√ß√£o
   - M√©tricas de desempenho

2. **Visualizar e comparar outputs:**
   - Painel Langfuse para an√°lise visual
   - Compara√ß√£o lado a lado de diferentes prompts/modelos
   - An√°lise de tend√™ncias ao longo do tempo

3. **Integrar com MLflow:**
   - Manter coer√™ncia entre rastreamento t√©cnico (ML) e observabilidade sem√¢ntica (LLM)
   - MLflow para m√©tricas t√©cnicas
   - Langfuse para qualidade textual e sem√¢ntica

**Resultado:** Observabilidade completa do pipeline de IA: t√©cnica (m√©tricas) + sem√¢ntica (qualidade textual)

**Deliverable:** 
- M√≥dulo `src/langfuse_tracking.py` (similar ao `mlflow_tracking.py`)
- Integra√ß√£o nos notebooks de avalia√ß√£o
- Instru√ß√µes de configura√ß√£o

**Dependencies:**
- Langfuse instalado (comentado em `requirements.txt`)
- LLM integration funcionando

---

### üéØ Phase 6: LLM Integration

#### üìã Step 6: Implementar LLM Client

**Objetivo:** Criar m√≥dulo para integra√ß√£o com modelos LLM (OpenShift AI, vLLM, ou outros).

**Requisitos:**

- Conectar a endpoints LLM (OpenShift AI, vLLM, ou outros)
- Suportar m√∫ltiplos tipos de modelo:
  - Scenario A: Modelo gen√©rico grande (40B+)
  - Scenario B: Modelo menor ajustado (3B-7B)
- Usar LangChain para orquestra√ß√£o
- Tratar chamadas de API, error handling, retries
- Suportar sa√≠das estruturadas (JSON) e n√£o estruturadas
- Configur√°vel via vari√°veis de ambiente

**Estrutura sugerida:**

```python
class LLMClient:
    def __init__(self, model_name, endpoint_url, api_key=None)
    def enrich_incident(self, prompt: str, temperature: float = 0.7) -> str
    def enrich_incident_structured(self, prompt: str) -> Dict
    def batch_enrich(self, prompts: List[str]) -> List[str]
```

**Deliverable:** `src/llm_client.py` com integra√ß√£o completa

**Dependencies:**
- LangChain (j√° em `requirements.txt`)
- Configura√ß√£o de endpoints (ver Step 6)

---

### üéØ Phase 7: Configuration & Environment

#### üìã Step 7: Configura√ß√£o de Ambiente

**Objetivo:** Facilitar configura√ß√£o e deployment do projeto.

**Arquivos a criar:**

1. **`.env.example`** - Template para vari√°veis de ambiente:
   ```env
   # LLM Configuration
   LLM_ENDPOINT_URL_SCENARIO_A=https://...
   LLM_ENDPOINT_URL_SCENARIO_B=https://...
   LLM_API_KEY=...
   MODEL_NAME_SCENARIO_A=llama-2-40b
   MODEL_NAME_SCENARIO_B=llama-2-7b-tuned
   
   # MLflow Configuration
   MLFLOW_TRACKING_URI=http://localhost:5000
   
   # Langfuse Configuration
   LANGFUSE_SECRET_KEY=...
   LANGFUSE_PUBLIC_KEY=...
   LANGFUSE_HOST=https://cloud.langfuse.com
   
   # TrustyAI Configuration
   TRUSTYAI_ENDPOINT=...
   ```

2. **`config.py`** - M√≥dulo de configura√ß√£o centralizado:
   - Carregar vari√°veis de ambiente
   - Valida√ß√£o de configura√ß√£o
   - Valores padr√£o

**Deliverable:** `.env.example` e `src/config.py`

---

### üéØ Phase 8: TrustyAI Integration

#### üìã Step 8: Integra√ß√£o com TrustyAI

**Objetivo:** Adicionar an√°lises de fairness, explainability e bias detection usando TrustyAI.

**Funcionalidades:**

- **Fairness metrics** - Analisar diferen√ßas de desempenho entre categorias
- **Explainability** - Explicar por que certos enriquecimentos t√™m scores mais altos
- **Bias detection** - Verificar diferen√ßas sistem√°ticas entre grupos
- **Confidence scores** - Medir confian√ßa nas avalia√ß√µes

**Integra√ß√£o:**
- TrustyAI pode ser usado como parte do pipeline de avalia√ß√£o
- Integrar an√°lises de fairness e explainability nos processos de avalia√ß√£o
- Usar resultados do TrustyAI como m√©tricas adicionais

**Deliverable:** 
- Integra√ß√£o TrustyAI com pipeline de avalia√ß√£o
- Notebook demonstrando uso combinado
- Atualizar `requirements.txt` (descomentar TrustyAI)
- M√≥dulo `src/trustyai_integration.py` (se necess√°rio)

**Dependencies:**
- TrustyAI dispon√≠vel no ambiente
- Phases 2, 3, e 4 completas
- Configura√ß√£o de endpoints

---

## üìã Expected Deliverables (Final)

Ao final da implementa√ß√£o, os participantes ter√£o:

1. ‚úÖ **Dataset com ground truth de close_notes** (`data/gt_close_notes.csv`)
2. ‚úÖ **An√°lise baseline n-gram** (Phase 2 usando Unitxt) - Valida√ß√£o de hip√≥tese
3. üü° **An√°lise baseline sem√¢ntica** (Phase 3 usando Unitxt) - Opcional
4. üî¥ **Camada de avalia√ß√£o automatizada via LLM-as-a-Judge** (Phase 4) ‚≠ê **MAIN METHOD**
   - Avalia√ß√£o estruturada com 6 crit√©rios (0-5)
   - Compara√ß√£o de close notes vs ground truth
   - Scores explic√°veis com reasoning
5. üî¥ **Observabilidade e rastreabilidade** com Langfuse e MLflow
6. üî¥ **Capacidade de demonstrar** que um modelo menor e governado (ajustado e avaliado) produz resultados mais confi√°veis, explic√°veis e consistentes que um LLM gen√©rico

---

## üìä Implementation Priority

### üî¥ Critical Path (Must Have)
1. ‚úÖ **Complete** - Notebook 01: Data exploration
2. ‚úÖ **Complete** - Step 1: Create Ground Truth dataset
3. ‚úÖ **Complete** - Phase 2: N-gram Baseline Analysis using Unitxt
   - ‚úÖ Created `notebooks/03_ngram_comparisons.ipynb`
   - ‚úÖ Implemented n-gram metrics (ROUGE) using Unitxt
   - ‚úÖ Compare gt_close_notes √ó incident_descriptions (baseline test)
4. üü° **Optional** - Phase 3: Semantic Baseline Analysis using Unitxt
   - üü° Create `notebooks/04_semantic_comparisons.ipynb` (optional)
   - üü° Implement semantic similarity metrics using Unitxt
   - üü° Compare gt_close_notes √ó incident_descriptions (optional baseline)
5. üî¥ **Next** - Phase 4: LLM-as-a-Judge Evaluation ‚≠ê **MAIN EVALUATION**
   - üî¥ Create `notebooks/05_llm_as_judge_evaluation.ipynb`
   - üî¥ Implement LLM judge for structured evaluation
   - üî¥ Compare close_notes (existing/LLM-generated) √ó gt_close_notes
   - üî¥ Implement 6 evaluation criteria with 0-5 scoring
6. üî¥ **Next** - Phase 6: Implement LLM Client (for generating close notes from incidents)

### üü° Important (Should Have)
7. üü° - Phase 5: Langfuse Integration
8. üü° - Phase 8: TrustyAI Integration
9. üü° - Phase 7: Environment Configuration

### üü¢ Nice to Have (Optional)
9. üü¢ - Unit tests
10. üü¢ - Integration tests
11. üü¢ - Comprehensive documentation

---

## üéØ Key Decisions Needed

1. **Evaluation Framework**: ‚úÖ **RESOLVED**
   - ‚úÖ Unitxt selected as evaluation framework
   - ‚úÖ Unitxt installed (`unitxt>=1.0.0`)
   - üî¥ **Next:** Configure Unitxt for n-gram and semantic metrics

2. **LLM Endpoints**: üî¥ **IN PROGRESS**
   - ‚úÖ Ollama configured: `http://localhost:11434` (using llama3.2:3b model)
   - üî¥ **Next:** Determine if we need additional endpoints or can use Ollama for LLM-as-a-Judge
   - Note: Can use different Ollama models for different scenarios

3. **Model Selection**: 
   - For LLM-as-a-Judge: Which model to use as judge?
   - For generating close notes: Which models for Scenario A vs Scenario B?

4. **TrustyAI**: 
   - Is TrustyAI available in the environment?
   - Integration approach with evaluation pipeline?
   - What version and API should be used?

5. **Langfuse**: 
   - Will use cloud version or self-hosted?
   - API keys and configuration?
   - Integration with evaluation results?

---

## üìù Notes

### Current Approach
- **`content`** = Original incident description (input to LLM)
- **`close_notes`** (in dataset) = Example close notes (can be used as reference)
- **LLM Output** = `close_notes` generated by the model from `content`

### Evaluation Strategy

**Fase de Baseline (Phase 2-3):**
1. **Phase 2: N-gram Comparisons** (ROUGE, BLEU) using Unitxt
   - **Objetivo:** Testar hip√≥tese de que descri√ß√µes de incidentes e close notes usam linguagem diferente
   - **Compara√ß√£o:** Ground truth close notes vs incident descriptions
   - **Resultado esperado:** Scores baixos confirmam que n-grams n√£o s√£o adequados

2. **Phase 3: Semantic Comparisons** (embedding similarity) using Unitxt - **OPCIONAL**
   - **Objetivo:** An√°lise complementar de similaridade sem√¢ntica
   - **Compara√ß√£o:** Ground truth close notes vs incident descriptions
   - **Status:** Opcional se Phase 2 j√° confirmar que m√©tricas tradicionais n√£o s√£o adequadas

**Fase de Avalia√ß√£o Principal (Phase 4):**
3. **Phase 4: LLM-as-a-Judge** ‚≠ê **MAIN EVALUATION METHOD**
   - **Objetivo:** Avaliar qualidade de close notes usando crit√©rios sem√¢nticos estruturados
   - **Compara√ß√£o:** Close notes (existentes ou LLM-geradas) vs ground truth close notes
   - **Crit√©rios:** Topic coverage, accuracy, facts, structure, conclusion
   - **Vantagem:** Avalia significado e qualidade, n√£o apenas overlap de palavras
   - **Escal√°vel:** N√£o requer labeling humano

**Fase de Integra√ß√£o:**
4. **TrustyAI Integration** - Fairness, explainability, bias detection

### Code Quality
- ‚úÖ Good separation of concerns in `src/` modules
- ‚úÖ Comprehensive evaluation framework (`evaluator.py` - ser√° integrado com Unitxt)
- ‚úÖ Well-structured prompt templates
- ‚ö†Ô∏è Missing error handling in some utility functions
- ‚ö†Ô∏è No logging framework (could use Python logging)

### **IMPORTANTE: Mudan√ßa de Abordagem**

**Por que usar Unitxt em vez de c√≥digo customizado:**

1. **Framework Padronizado**: Unitxt fornece framework padronizado para avalia√ß√£o
2. **Menos C√≥digo**: N√£o precisamos reimplementar funcionalidades j√° dispon√≠veis
3. **Melhor Integra√ß√£o**: Facilita integra√ß√£o com outros componentes
4. **Manutenibilidade**: Menos c√≥digo customizado = menos manuten√ß√£o
5. **Escalabilidade**: Processamento otimizado para grandes volumes
6. **Reprodutibilidade**: Resultados consistentes e compar√°veis

**O que manter do c√≥digo atual:**
- `src/prompts.py` - Templates de prompts ainda s√£o √∫teis
- `src/utils.py` - Fun√ß√µes utilit√°rias para prepara√ß√£o de dados
- `src/mlflow_tracking.py` - Tracking continua √∫til
- `src/evaluator.py` - Pode ser adaptado para usar Unitxt como backend

**O que melhorar/integrar:**
- `src/evaluator.py` - Integrar com Unitxt para m√©tricas padronizadas
- M√©tricas customizadas - Usar Unitxt com m√©tricas apropriadas
- Pipeline de avalia√ß√£o - Usar Unitxt para execu√ß√£o padronizada

---

**Document Status:** ‚úÖ Complete  
**Last Review:** December 2024  
**Next Review:** After Phase 1 completion
