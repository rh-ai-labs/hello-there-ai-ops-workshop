{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: N-gram Baseline Analysis\n",
    "\n",
    "## üéØ What is This Notebook About?\n",
    "\n",
    "This notebook performs a **baseline exploration** to test whether n-gram metrics (word/phrase overlap) are useful for evaluating close notes quality.\n",
    "\n",
    "**Context:**\n",
    "1. We have an **incident dataset** with original problem descriptions\n",
    "2. We extracted some **high-quality close notes** from that dataset to serve as **ground truth references**\n",
    "3. Our goal is to evaluate close notes (existing ones or LLM-generated ones) against these ground truth references\n",
    "\n",
    "**This notebook's purpose:**\n",
    "- **Hypothesis:** Incident descriptions and close notes might use very different language, making n-gram metrics less useful\n",
    "- **Test:** Compare ground-truth close notes vs incident descriptions using n-gram metrics\n",
    "- **Goal:** Determine if n-grams are relevant, or if we should focus on LLM-as-a-Judge evaluation instead\n",
    "\n",
    "**What we're comparing:**\n",
    "- **From Ground Truth Dataset:** Pairs of (`close_notes_ref`, `content`) from the **same incident**\n",
    "- **From Incidents Dataset:** Pairs of (`close_notes`, `content`) from the **same incident**\n",
    "- **Then compare:** N-gram scores between ground truth pairs vs incidents pairs\n",
    "\n",
    "**Expected outcome:** \n",
    "- If n-gram scores are very low for both datasets, it confirms that incident descriptions and close notes use different language\n",
    "- Comparing scores between datasets helps us understand if ground truth close notes differ more/less from descriptions than regular close notes\n",
    "- This validates that we should use **LLM-as-a-Judge** (semantic evaluation) rather than n-grams for the main evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Key Concepts Explained\n",
    "\n",
    "### What are N-grams?\n",
    "\n",
    "**N-grams** are sequences of N words. For example:\n",
    "- **1-gram (unigram)**: Single words ‚Üí \"the\", \"user\", \"reported\"\n",
    "- **2-gram (bigram)**: Pairs of words ‚Üí \"the user\", \"user reported\", \"reported error\"\n",
    "- **3-gram (trigram)**: Three words ‚Üí \"the user reported\", \"user reported error\"\n",
    "\n",
    "**Why we're testing this:** N-grams measure **lexical overlap** (shared words/phrases). If incident descriptions and close notes use completely different vocabulary, n-grams won't be useful for evaluation.\n",
    "\n",
    "### What are ROUGE Metrics?\n",
    "\n",
    "**ROUGE** (Recall-Oriented Understudy for Gisting Evaluation) measures how well a text matches a reference by counting overlapping n-grams.\n",
    "\n",
    "**The metrics we'll use:**\n",
    "\n",
    "1. **ROUGE-1**: Measures word overlap (unigrams)\n",
    "   - *Example:* \"User reported error\" vs \"User saw error\" ‚Üí Shares 2 words: \"User\" and \"error\"\n",
    "   - *What it tells us:* Do the texts use similar vocabulary?\n",
    "\n",
    "2. **ROUGE-2**: Measures two-word phrase overlap (bigrams)\n",
    "   - *Example:* \"User reported error\" vs \"User reported issue\" ‚Üí Shares 1 phrase: \"User reported\"\n",
    "   - *What it tells us:* Do the texts use similar word combinations?\n",
    "\n",
    "3. **ROUGE-L**: Measures longest common subsequence\n",
    "   - *Example:* Finds the longest sequence of words that appear in the same order in both texts\n",
    "   - *What it tells us:* How well do the texts follow similar sentence structure?\n",
    "\n",
    "4. **ROUGE-Lsum**: Similar to ROUGE-L but optimized for summaries\n",
    "   - *What it tells us:* How well does the text capture the main points?\n",
    "\n",
    "**Score interpretation:**\n",
    "- **0.0** = No overlap (completely different texts)\n",
    "- **1.0** = Perfect match (identical texts)\n",
    "- **0.5** = Moderate similarity (half the words/phrases match)\n",
    "\n",
    "**Our hypothesis:** Scores will be **low (0.1-0.3)** because:\n",
    "- Incident descriptions describe **problems** (\"User cannot login\")\n",
    "- Close notes describe **solutions** (\"Reset password and verified access\")\n",
    "- They use different vocabulary and structure\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "This notebook will:\n",
    "1. **Load** ground truth dataset and incidents dataset\n",
    "2. **Compare** ground-truth close notes vs incident descriptions using n-gram metrics\n",
    "3. **Calculate** ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-Lsum scores using Unitxt\n",
    "4. **Analyze** results to test our hypothesis\n",
    "5. **Conclude** whether n-grams are useful or if we should focus on LLM-as-a-Judge\n",
    "\n",
    "---\n",
    "\n",
    "## üìã What We're Comparing\n",
    "\n",
    "**Dataset Comparison:**\n",
    "- **Ground Truth** (`gt_close_notes.csv`): High-quality close notes extracted from incidents\n",
    "  - Contains: `close_notes_ref` - well-written resolution notes\n",
    "  \n",
    "- **Incidents** (`incidents_prepared.csv`): Original incident dataset\n",
    "  - Contains: `content` - the original problem description\n",
    "  - Also contains: `close_notes` - existing close notes (not used in this comparison)\n",
    "\n",
    "**Why compare ground-truth close notes vs incident descriptions?**\n",
    "- **To test if n-grams are relevant:** If scores are very low, it confirms that incident descriptions and close notes use different language\n",
    "- **Baseline for comparison:** Once we have LLM-generated close notes, we can compare them against ground truth using more sophisticated methods (LLM-as-a-Judge)\n",
    "\n",
    "**Note:** The **real evaluation** will happen in the next notebook using **LLM-as-a-Judge**, which evaluates:\n",
    "- Topic coverage\n",
    "- Accuracy of facts\n",
    "- Text structure\n",
    "- Completeness\n",
    "- And other semantic criteria\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Using Unitxt\n",
    "\n",
    "**Unitxt** is a standardized framework for evaluating text quality. It provides pre-built metrics that ensure consistent and comparable results across different evaluations.\n",
    "\n",
    "**Why Unitxt?** It standardizes how we compute metrics, making results reproducible and comparable across different evaluation phases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# These are the tools we need to work with data and create visualizations\n",
    "\n",
    "import pandas as pd  # For working with tables (like Excel spreadsheets)\n",
    "import numpy as np   # For mathematical operations\n",
    "import matplotlib.pyplot as plt  # For creating charts and graphs\n",
    "import seaborn as sns  # For prettier charts\n",
    "from pathlib import Path  # For handling file paths\n",
    "import sys\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Hide warning messages to keep output clean\n",
    "\n",
    "# Add src directory to path so we can use utility functions\n",
    "sys.path.append(str(Path(\"../src\").resolve()))\n",
    "\n",
    "# Unitxt imports - REQUIRED\n",
    "# Unitxt provides the ROUGE metrics we'll use to compare texts\n",
    "try:\n",
    "    from unitxt.metrics import Rouge\n",
    "    print(\"‚úÖ Unitxt imported successfully\")\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"Unitxt is required but not available: {e}\\n\"\n",
    "        \"Please install Unitxt: pip install unitxt or uv add unitxt\"\n",
    "    )\n",
    "\n",
    "# Set up plotting style (makes charts look nicer)\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn')\n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")  # Use a nice color palette\n",
    "# Display charts in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"‚úÖ Using Unitxt for n-gram metrics evaluation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets\n",
    "\n",
    "**What we're doing:** Loading the two datasets we want to compare.\n",
    "\n",
    "**Why:** We need both datasets in memory before we can compare them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "# We'll load both datasets so we can compare them\n",
    "\n",
    "data_dir = Path(\"../data\")  # Where our data files are stored\n",
    "\n",
    "# Load ground truth dataset\n",
    "# This contains high-quality close notes (our reference texts)\n",
    "gt_path = data_dir / \"gt_close_notes.csv\"\n",
    "gt_df = pd.read_csv(gt_path)  # Read CSV file into a table (DataFrame)\n",
    "print(f\"‚úÖ Loaded ground truth dataset: {len(gt_df)} records\")\n",
    "print(f\"   Columns: {list(gt_df.columns)}\")  # Show what information is in each row\n",
    "\n",
    "# Load incidents dataset\n",
    "# This contains the original incident descriptions\n",
    "incidents_path = data_dir / \"incidents_prepared.csv\"\n",
    "if incidents_path.exists():\n",
    "    incidents_df = pd.read_csv(incidents_path)\n",
    "    print(f\"‚úÖ Loaded incidents dataset: {len(incidents_df)} records\")\n",
    "else:\n",
    "    # Fallback to sample dataset if the prepared one doesn't exist\n",
    "    incidents_path = data_dir / \"incidents_sample.csv\"\n",
    "    incidents_df = pd.read_csv(incidents_path)\n",
    "    print(f\"‚úÖ Loaded incidents sample dataset: {len(incidents_df)} records\")\n",
    "print(f\"   Columns: {list(incidents_df.columns)}\")\n",
    "\n",
    "# Display basic info about both datasets\n",
    "# This helps us understand the data structure and verify everything loaded correctly\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ground Truth Dataset Info:\")\n",
    "print(\"=\"*60)\n",
    "print(gt_df.info())  # Shows number of rows, columns, data types, and missing values\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Incidents Dataset Info:\")\n",
    "print(\"=\"*60)\n",
    "print(incidents_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Comparison\n",
    "\n",
    "**What we're doing:** Checking that our datasets have the right columns to create pairs from the same incident.\n",
    "\n",
    "**Why:** Before comparing, we need to verify:\n",
    "- The ground truth dataset has both `close_notes_ref` AND `content` columns (same incident)\n",
    "- The incidents dataset has both `close_notes` AND `content` columns (same incident)\n",
    "- The data looks correct\n",
    "\n",
    "**What to look for:** Make sure the sample data shows actual text content, not empty values. Each row should have both close notes and content from the same incident.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check required columns\n",
    "# We need BOTH close notes AND content from the same incident in each dataset\n",
    "\n",
    "required_gt_cols = ['close_notes_ref', 'content']  # Ground truth must have both close notes and content\n",
    "required_incident_cols = ['close_notes', 'content']  # Incidents must have both close notes and content\n",
    "\n",
    "# Verify columns exist in our datasets\n",
    "# This checks if the columns we need are actually present\n",
    "missing_gt = [col for col in required_gt_cols if col not in gt_df.columns]\n",
    "missing_incident = [col for col in required_incident_cols if col not in incidents_df.columns]\n",
    "\n",
    "# Report any missing columns\n",
    "if missing_gt:\n",
    "    print(f\"‚ùå Missing columns in ground truth: {missing_gt}\")\n",
    "    print(f\"   Available columns: {list(gt_df.columns)}\")\n",
    "if missing_incident:\n",
    "    print(f\"‚ùå Missing columns in incidents: {missing_incident}\")\n",
    "    print(f\"   Available columns: {list(incidents_df.columns)}\")\n",
    "\n",
    "# If all columns are present, show sample data\n",
    "if not missing_gt and not missing_incident:\n",
    "    print(\"‚úÖ All required columns found!\")\n",
    "    \n",
    "    # Show sample data so you can see what we're working with\n",
    "    # This helps verify the data looks correct before we start comparing\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Sample Ground Truth (same incident):\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Each row has both close_notes_ref and content from the same incident:\")\n",
    "    print(gt_df[['number', 'category', 'content', 'close_notes_ref']].head(2))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Sample Incidents (same incident):\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Each row has both close_notes and content from the same incident:\")\n",
    "    print(incidents_df[['number', 'category', 'content', 'close_notes']].head(2))\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Cannot proceed: Missing required columns\")\n",
    "    print(\"   Please ensure both datasets have the required columns to create pairs from the same incident\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Text Pairs from Same Incident\n",
    "\n",
    "**What we're doing:** Creating pairs of texts from the **same incident** in each dataset.\n",
    "\n",
    "**Why:** We want to compare how similar close notes are to their incident descriptions, and then compare this similarity between ground truth vs regular incidents.\n",
    "\n",
    "**Pair Creation Strategy:**\n",
    "- **Ground Truth Dataset:** For each row, create pair (`close_notes_ref`, `content`) - both from the same incident\n",
    "- **Incidents Dataset:** For each row, create pair (`close_notes`, `content`) - both from the same incident\n",
    "\n",
    "**What happens:**\n",
    "1. From ground truth dataset: Extract pairs where both fields come from the same row (same incident)\n",
    "2. From incidents dataset: Extract pairs where both fields come from the same row (same incident)\n",
    "3. Filter out rows where either field is missing or empty\n",
    "4. We'll compute n-gram metrics for each pair\n",
    "5. Then compare the distribution of scores between ground truth pairs vs incidents pairs\n",
    "\n",
    "**Example from Ground Truth:**\n",
    "- Same incident (e.g., INC009427):\n",
    "  - Content: \"Customer has an issue with Palo Alto Prisma Cloud...\"\n",
    "  - Close Notes Ref: \"The customer reported a SocketException: Connection...\"\n",
    "  - Pair: (content, close_notes_ref) from same incident\n",
    "\n",
    "**Example from Incidents:**\n",
    "- Same incident (e.g., INC0047192):\n",
    "  - Content: \"The customer reports that Google Workspace crashes...\"\n",
    "  - Close Notes: \"Resolved issue with Google Workspace by clearing cache...\"\n",
    "  - Pair: (content, close_notes) from same incident\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_pairs_from_same_incident(\n",
    "    df: pd.DataFrame,\n",
    "    close_notes_col: str,\n",
    "    content_col: str,\n",
    "    dataset_name: str = \"dataset\"\n",
    ") -> Tuple[List[str], List[str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Prepare text pairs from the same incident.\n",
    "    \n",
    "    For each row in the dataframe, creates a pair of (content, close_notes) \n",
    "    where both come from the same incident.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe with both close_notes and content columns\n",
    "        close_notes_col: Name of the close notes column\n",
    "        content_col: Name of the content/description column\n",
    "        dataset_name: Name of dataset (for logging)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (references, predictions, metadata_df)\n",
    "        - references: List of content texts (incident descriptions)\n",
    "        - predictions: List of close notes texts (paired with content from same row)\n",
    "        - metadata_df: DataFrame with incident numbers, category, etc.\n",
    "    \"\"\"\n",
    "    references = []  # Will store content (incident descriptions)\n",
    "    predictions = []  # Will store close notes (from same incident)\n",
    "    metadata = []  # Will store information about each pair\n",
    "    \n",
    "    # Clean text data - make copy so we don't modify original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remove rows where either field is missing\n",
    "    df_clean = df_clean.dropna(subset=[close_notes_col, content_col])\n",
    "    \n",
    "    # Filter out empty strings\n",
    "    df_clean = df_clean[\n",
    "        (df_clean[close_notes_col].str.strip() != '') & \n",
    "        (df_clean[content_col].str.strip() != '')\n",
    "    ]\n",
    "    \n",
    "    print(f\"üìä {dataset_name} records: {len(df)}\")\n",
    "    print(f\"üìä Records with both fields: {len(df_clean)}\")\n",
    "    \n",
    "    # Create pairs from same incident (same row)\n",
    "    for _, row in df_clean.iterrows():\n",
    "        content_text = str(row[content_col]).strip()\n",
    "        close_notes_text = str(row[close_notes_col]).strip()\n",
    "        \n",
    "        # Store the pair (both from same incident)\n",
    "        references.append(content_text)\n",
    "        predictions.append(close_notes_text)\n",
    "        metadata.append({\n",
    "            'number': row.get('number', ''),\n",
    "            'category': row.get('category', 'UNKNOWN'),\n",
    "            'subcategory': row.get('subcategory', ''),\n",
    "        })\n",
    "    \n",
    "    # Convert metadata list to DataFrame for easier analysis\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    print(f\"‚úÖ Created {len(references)} pairs from same incidents\")\n",
    "    \n",
    "    return references, predictions, metadata_df\n",
    "\n",
    "# Prepare pairs from ground truth dataset\n",
    "# Each pair: (content, close_notes_ref) from the same incident\n",
    "print(\"=\"*60)\n",
    "print(\"GROUND TRUTH DATASET PAIRS\")\n",
    "print(\"=\"*60)\n",
    "gt_references, gt_predictions, gt_metadata = prepare_text_pairs_from_same_incident(\n",
    "    gt_df,\n",
    "    close_notes_col='close_notes_ref',\n",
    "    content_col='content',\n",
    "    dataset_name=\"Ground truth\"\n",
    ")\n",
    "\n",
    "# Prepare pairs from incidents dataset\n",
    "# Each pair: (content, close_notes) from the same incident\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INCIDENTS DATASET PAIRS\")\n",
    "print(\"=\"*60)\n",
    "inc_references, inc_predictions, inc_metadata = prepare_text_pairs_from_same_incident(\n",
    "    incidents_df,\n",
    "    close_notes_col='close_notes',\n",
    "    content_col='content',\n",
    "    dataset_name=\"Incidents\"\n",
    ")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE PAIRS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìù Ground Truth Example (same incident):\")\n",
    "print(f\"  Content: {gt_references[0][:150]}...\")\n",
    "print(f\"  Close Notes: {gt_predictions[0][:150]}...\")\n",
    "\n",
    "print(\"\\nüìù Incidents Example (same incident):\")\n",
    "print(f\"  Content: {inc_references[0][:150]}...\")\n",
    "print(f\"  Close Notes: {inc_predictions[0][:150]}...\")\n",
    "print(\"\\nWe'll compare these pairs to see how similar close notes are to their incident descriptions!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute N-gram Metrics Using Unitxt\n",
    "\n",
    "**What we're doing:** Calculating ROUGE scores for each text pair in both datasets separately, then comparing them.\n",
    "\n",
    "**How it works:**\n",
    "1. For each pair (content, close_notes) from the same incident:\n",
    "   - **ROUGE-1**: Counts how many individual words appear in both texts\n",
    "   - **ROUGE-2**: Counts how many two-word phrases appear in both texts\n",
    "   - **ROUGE-L**: Finds the longest sequence of words that appear in order in both texts\n",
    "   - **ROUGE-Lsum**: Similar to ROUGE-L but optimized for longer texts\n",
    "\n",
    "2. Each metric returns a score between 0.0 and 1.0:\n",
    "   - **0.0** = No words/phrases in common\n",
    "   - **0.5** = Half the words/phrases match\n",
    "   - **1.0** = Perfect match (all words/phrases match)\n",
    "\n",
    "3. We compute metrics for:\n",
    "   - **Ground Truth pairs:** (content, close_notes_ref) from ground truth dataset\n",
    "   - **Incidents pairs:** (content, close_notes) from incidents dataset\n",
    "\n",
    "4. Then we compare the distributions to see if ground truth pairs show different patterns\n",
    "\n",
    "**What to expect:**\n",
    "- Scores are typically low (0.1-0.3) because incident descriptions and close notes use different language\n",
    "- Comparing ground truth vs incidents helps us understand if high-quality close notes differ more/less from descriptions\n",
    "- This validates that n-grams aren't suitable for evaluating close notes quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngram_metrics_unitxt(references: List[str], predictions: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute n-gram metrics using Unitxt.\n",
    "    \n",
    "    This function compares each pair of texts and calculates ROUGE scores.\n",
    "    ROUGE scores measure how many words/phrases the two texts share.\n",
    "    \n",
    "    Args:\n",
    "        references: List of reference texts (ground truth close notes)\n",
    "        predictions: List of prediction texts (incident descriptions)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with ROUGE scores for each pair\n",
    "        - rouge1: Word overlap score (0.0 to 1.0)\n",
    "        - rouge2: Two-word phrase overlap score (0.0 to 1.0)\n",
    "        - rougeL: Longest common subsequence score (0.0 to 1.0)\n",
    "        - rougeLsum: Summary-level LCS score (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    results = []  # Will store scores for each pair\n",
    "    \n",
    "    # Initialize Unitxt ROUGE metric\n",
    "    # This is the tool that calculates the similarity scores\n",
    "    rouge_metric = Rouge()\n",
    "    \n",
    "    print(\"üìä Computing ROUGE metrics using Unitxt...\")\n",
    "    print(\"   This compares each pair and counts shared words/phrases\")\n",
    "    print(\"   Note: Using ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-Lsum metrics\")\n",
    "    \n",
    "    # Process each pair one by one\n",
    "    for i, (ref, pred) in enumerate(zip(references, predictions)):\n",
    "        try:\n",
    "            # Compute ROUGE scores for this pair\n",
    "            # Unitxt expects: references (as a list), prediction (as a string), task_data (empty dict)\n",
    "            rouge_scores = rouge_metric.compute(references=[ref], prediction=pred, task_data={})\n",
    "            \n",
    "            # Extract the scores (they come back as numbers between 0.0 and 1.0)\n",
    "            result = {\n",
    "                'rouge1': rouge_scores.get('rouge1', 0.0),  # Word overlap\n",
    "                'rouge2': rouge_scores.get('rouge2', 0.0),  # Two-word phrase overlap\n",
    "                'rougeL': rouge_scores.get('rougeL', 0.0),  # Longest common subsequence\n",
    "                'rougeLsum': rouge_scores.get('rougeLsum', 0.0),  # Summary-level LCS\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Show progress every 10 pairs\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"   Processed {i + 1}/{len(references)} pairs...\")\n",
    "        except Exception as e:\n",
    "            # If something goes wrong with this pair, record zero scores\n",
    "            print(f\"‚ö†Ô∏è  Error processing pair {i+1}: {e}\")\n",
    "            results.append({\n",
    "                'rouge1': 0.0,\n",
    "                'rouge2': 0.0,\n",
    "                'rougeL': 0.0,\n",
    "                'rougeLsum': 0.0,\n",
    "            })\n",
    "    \n",
    "    # Convert results to a DataFrame (table) for easier analysis\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Compute metrics for Ground Truth dataset\n",
    "print(\"=\"*60)\n",
    "print(\"COMPUTING METRICS FOR GROUND TRUTH DATASET\")\n",
    "print(\"=\"*60)\n",
    "gt_metrics_df = compute_ngram_metrics_unitxt(gt_references, gt_predictions)\n",
    "\n",
    "print(f\"\\n‚úÖ Computed metrics for {len(gt_metrics_df)} ground truth pairs\")\n",
    "print(\"\\nüìä Ground Truth Metrics Summary:\")\n",
    "print(gt_metrics_df.describe())\n",
    "\n",
    "# Compute metrics for Incidents dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPUTING METRICS FOR INCIDENTS DATASET\")\n",
    "print(\"=\"*60)\n",
    "inc_metrics_df = compute_ngram_metrics_unitxt(inc_references, inc_predictions)\n",
    "\n",
    "print(f\"\\n‚úÖ Computed metrics for {len(inc_metrics_df)} incidents pairs\")\n",
    "print(\"\\nüìä Incidents Metrics Summary:\")\n",
    "print(inc_metrics_df.describe())\n",
    "\n",
    "# Compare the two datasets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: GROUND TRUTH vs INCIDENTS\")\n",
    "print(\"=\"*60)\n",
    "metric_cols = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': metric_cols,\n",
    "    'Ground Truth Mean': [gt_metrics_df[col].mean() for col in metric_cols],\n",
    "    'Incidents Mean': [inc_metrics_df[col].mean() for col in metric_cols],\n",
    "    'Difference': [gt_metrics_df[col].mean() - inc_metrics_df[col].mean() for col in metric_cols]\n",
    "})\n",
    "print(\"\\nMean scores comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   - Positive difference = Ground truth pairs have higher similarity\")\n",
    "print(\"   - Negative difference = Incidents pairs have higher similarity\")\n",
    "print(\"   - Close to zero = Similar patterns in both datasets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine Results with Metadata\n",
    "\n",
    "**What we're doing:** Combining the ROUGE scores with metadata for both datasets separately.\n",
    "\n",
    "**Why:** This lets us analyze results by category and compare patterns between ground truth and incidents datasets.\n",
    "\n",
    "**What we're adding:**\n",
    "- Dataset source (ground truth vs incidents)\n",
    "- Category information (SOFTWARE, NETWORK, etc.)\n",
    "- Incident numbers (for tracking)\n",
    "- Text lengths (to see if length affects similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine metrics with metadata for Ground Truth dataset\n",
    "gt_results_df = pd.concat([gt_metadata, gt_metrics_df], axis=1)\n",
    "gt_results_df['dataset'] = 'ground_truth'  # Mark as ground truth\n",
    "gt_results_df['ref_length'] = [len(ref) for ref in gt_references]\n",
    "gt_results_df['pred_length'] = [len(pred) for pred in gt_predictions]\n",
    "gt_results_df['length_diff'] = gt_results_df['pred_length'] - gt_results_df['ref_length']\n",
    "\n",
    "# Combine metrics with metadata for Incidents dataset\n",
    "inc_results_df = pd.concat([inc_metadata, inc_metrics_df], axis=1)\n",
    "inc_results_df['dataset'] = 'incidents'  # Mark as incidents\n",
    "inc_results_df['ref_length'] = [len(ref) for ref in inc_references]\n",
    "inc_results_df['pred_length'] = [len(pred) for pred in inc_predictions]\n",
    "inc_results_df['length_diff'] = inc_results_df['pred_length'] - inc_results_df['ref_length']\n",
    "\n",
    "# Combine both datasets into one dataframe for comparison\n",
    "results_df = pd.concat([gt_results_df, inc_results_df], ignore_index=True)\n",
    "\n",
    "print(\"‚úÖ Combined results with metadata for both datasets\")\n",
    "print(f\"\\nüìä Ground Truth Results: {len(gt_results_df)} pairs\")\n",
    "print(f\"üìä Incidents Results: {len(inc_results_df)} pairs\")\n",
    "print(f\"üìä Total Results DataFrame shape: {results_df.shape}\")\n",
    "print(f\"   (rows = pairs, columns = information about each pair)\")\n",
    "print(f\"\\nüìã Columns: {list(results_df.columns)}\")\n",
    "print(\"\\nüìä First few results from each dataset:\")\n",
    "print(\"\\nGround Truth:\")\n",
    "print(gt_results_df.head(2))\n",
    "print(\"\\nIncidents:\")\n",
    "print(inc_results_df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overall Statistics & Comparison\n",
    "\n",
    "**What we're doing:** Calculating summary statistics for each dataset separately, then comparing them to test our hypothesis.\n",
    "\n",
    "**What the statistics mean:**\n",
    "- **Mean (Average)**: The typical similarity score\n",
    "  - *Example:* Mean ROUGE-1 of 0.25 means on average, 25% of words overlap\n",
    "  \n",
    "- **Median**: The middle value (half scores are above, half below)\n",
    "  - *Why useful?* Less affected by outliers than mean\n",
    "  \n",
    "- **Standard Deviation**: How much scores vary\n",
    "  - *Low std dev:* Scores are consistent\n",
    "  - *High std dev:* Some pairs are very similar, others very different\n",
    "\n",
    "- **Min/Max**: The lowest and highest scores\n",
    "  - Shows the range of similarity\n",
    "\n",
    "**Comparison Focus:**\n",
    "- Are ground truth pairs more/less similar than incidents pairs?\n",
    "- Do both datasets show similar patterns (low scores)?\n",
    "- Does this confirm our hypothesis that n-grams aren't suitable?\n",
    "\n",
    "**How to interpret the results:**\n",
    "\n",
    "**If scores are LOW for BOTH datasets (0.1-0.3):**\n",
    "- ‚úÖ **Confirms our hypothesis:** Incident descriptions and close notes use different vocabulary\n",
    "- ‚úÖ **Conclusion:** N-grams are **not suitable** for evaluating close notes quality\n",
    "- ‚úÖ **Action:** Proceed with **LLM-as-a-Judge** evaluation (semantic understanding)\n",
    "\n",
    "**If scores differ significantly between datasets:**\n",
    "- ‚ö†Ô∏è **Interesting finding:** Ground truth close notes might use different language patterns\n",
    "- ‚ö†Ô∏è **Still conclude:** N-grams are not suitable - the difference itself shows inconsistency\n",
    "\n",
    "**If scores are HIGH (0.5+):**\n",
    "- ‚ö†Ô∏è **Surprising result:** Incident descriptions and close notes share significant vocabulary\n",
    "- ‚ö†Ô∏è **Action:** Investigate further - this might indicate the incident descriptions already contain resolution language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall statistics - compute separately for each dataset\n",
    "metric_cols = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']  # The metrics we calculated\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GROUND TRUTH DATASET STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "gt_stats = gt_results_df[metric_cols].describe()\n",
    "print(gt_stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INCIDENTS DATASET STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "inc_stats = inc_results_df[metric_cols].describe()\n",
    "print(inc_stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: MEAN SCORES\")\n",
    "print(\"=\"*60)\n",
    "comparison_stats = pd.DataFrame({\n",
    "    'Metric': metric_cols,\n",
    "    'Ground Truth Mean': [gt_results_df[col].mean() for col in metric_cols],\n",
    "    'Incidents Mean': [inc_results_df[col].mean() for col in metric_cols],\n",
    "    'Difference': [gt_results_df[col].mean() - inc_results_df[col].mean() for col in metric_cols],\n",
    "    'GT Std Dev': [gt_results_df[col].std() for col in metric_cols],\n",
    "    'Inc Std Dev': [inc_results_df[col].std() for col in metric_cols]\n",
    "})\n",
    "print(comparison_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: MEDIAN SCORES\")\n",
    "print(\"=\"*60)\n",
    "median_comparison = pd.DataFrame({\n",
    "    'Metric': metric_cols,\n",
    "    'Ground Truth Median': [gt_results_df[col].median() for col in metric_cols],\n",
    "    'Incidents Median': [inc_results_df[col].median() for col in metric_cols],\n",
    "    'Difference': [gt_results_df[col].median() - inc_results_df[col].median() for col in metric_cols]\n",
    "})\n",
    "print(median_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   - Low scores (0.1-0.3) for BOTH datasets = Confirms hypothesis\")\n",
    "print(\"   - Similar patterns = Both datasets show same language differences\")\n",
    "print(\"   - Different patterns = Interesting finding about ground truth quality\")\n",
    "print(\"   - This validates that n-grams are NOT suitable for evaluating close notes!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optional: Analysis by Category\n",
    "\n",
    "**What we're doing:** (Optional) Grouping results by category to see if patterns vary by incident type.\n",
    "\n",
    "**Why this is optional:**\n",
    "- The main comparison (Ground Truth vs Incidents) is more important\n",
    "- Category analysis can provide additional insights but isn't critical for hypothesis testing\n",
    "\n",
    "**What to look for (if reviewing):**\n",
    "- **Consistent low scores across categories** = Strong confirmation of hypothesis\n",
    "- **Variable scores** = Some categories might have different patterns (interesting but doesn't change main conclusion)\n",
    "\n",
    "**Main takeaway:** Even if categories vary, the overall low scores confirm that n-grams aren't suitable for evaluating close notes quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Group by category\n",
    "# This is optional - the main comparison is Ground Truth vs Incidents\n",
    "\n",
    "if 'category' in results_df.columns and len(results_df['category'].unique()) > 1:\n",
    "    print(\"=\"*60)\n",
    "    print(\"OPTIONAL: METRICS BY CATEGORY\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"(This analysis is optional - main focus is dataset comparison above)\\n\")\n",
    "    \n",
    "    # Quick summary by category for both datasets\n",
    "    print(\"üìä Quick Summary by Category (across both datasets):\")\n",
    "    category_summary = results_df.groupby(['dataset', 'category'])[metric_cols].mean()\n",
    "    print(category_summary)\n",
    "    \n",
    "    print(\"\\nüí° Note: Main conclusion from dataset comparison is more important than category analysis\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Category column not found - skipping optional category analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations: Comparing Datasets\n",
    "\n",
    "**What we're doing:** Creating charts to compare n-gram scores between Ground Truth and Incidents datasets.\n",
    "\n",
    "**Focus:** We want to see if both datasets show similar patterns (low scores), which would confirm our hypothesis that n-grams aren't suitable for evaluating close notes.\n",
    "\n",
    "**Charts we'll create:**\n",
    "1. **Comparison Bar Chart**: Mean scores for each metric, side-by-side for both datasets\n",
    "2. **Box Plot Comparison**: Distribution comparison showing if patterns are similar\n",
    "3. **Summary Table**: Quick visual comparison of key statistics\n",
    "\n",
    "**What to look for:**\n",
    "- **Similar low scores** in both datasets = Confirms hypothesis\n",
    "- **Different patterns** = Interesting finding about ground truth quality\n",
    "- **Overall conclusion**: Low scores validate that n-grams aren't suitable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "# Focus on comparing Ground Truth vs Incidents datasets\n",
    "\n",
    "metric_cols = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "metric_labels = ['ROUGE-1\\n(Word Overlap)', 'ROUGE-2\\n(Phrase Overlap)', \n",
    "                 'ROUGE-L\\n(Sequence)', 'ROUGE-Lsum\\n(Summary)']\n",
    "\n",
    "# Prepare data for comparison\n",
    "gt_means = [gt_results_df[col].mean() for col in metric_cols]\n",
    "inc_means = [inc_results_df[col].mean() for col in metric_cols]\n",
    "\n",
    "# Chart 1: Comparison Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(metric_labels))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, gt_means, width, label='Ground Truth', alpha=0.8, color='#2ecc71')\n",
    "bars2 = ax.bar(x + width/2, inc_means, width, label='Incidents', alpha=0.8, color='#3498db')\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Mean Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comparison: Mean N-gram Scores\\n(Ground Truth vs Incidents)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metric_labels)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, max(max(gt_means), max(inc_means)) * 1.2])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Reading the bar chart:\")\n",
    "print(\"   - Compare the height of bars for each metric\")\n",
    "print(\"   - Similar heights = Both datasets show similar patterns\")\n",
    "print(\"   - Low scores (0.1-0.3) = Confirms hypothesis that n-grams aren't suitable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 2: Box Plot Comparison - Distribution of scores by dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Distribution Comparison: Ground Truth vs Incidents', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Prepare data for box plots\n",
    "data_to_plot = []\n",
    "for col in metric_cols:\n",
    "    data_to_plot.append([\n",
    "        gt_results_df[col].values,\n",
    "        inc_results_df[col].values\n",
    "    ])\n",
    "\n",
    "positions = [[1, 2], [1, 2], [1, 2], [1, 2]]\n",
    "colors = ['#2ecc71', '#3498db']\n",
    "labels = ['Ground Truth', 'Incidents']\n",
    "\n",
    "for idx, (col, label) in enumerate(zip(metric_cols, metric_labels)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    bp = ax.boxplot(data_to_plot[idx], positions=[1, 2], widths=0.6, \n",
    "                    patch_artist=True, labels=labels)\n",
    "    \n",
    "    # Color the boxes\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_title(f'{label}', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim([0, max(max(gt_results_df[col]), max(inc_results_df[col])) * 1.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Reading the box plots:\")\n",
    "print(\"   - Compare the boxes side-by-side for each metric\")\n",
    "print(\"   - Similar box positions = Both datasets show similar patterns\")\n",
    "print(\"   - Low boxes (scores 0.1-0.3) = Confirms hypothesis\")\n",
    "print(\"   - The box shows middle 50% of scores, line shows median\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 3: Summary Comparison Table Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for col, label in zip(metric_cols, ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE-Lsum']):\n",
    "    comparison_data.append([\n",
    "        label,\n",
    "        f\"{gt_results_df[col].mean():.4f}\",\n",
    "        f\"{inc_results_df[col].mean():.4f}\",\n",
    "        f\"{gt_results_df[col].mean() - inc_results_df[col].mean():+.4f}\",\n",
    "        f\"{gt_results_df[col].median():.4f}\",\n",
    "        f\"{inc_results_df[col].median():.4f}\"\n",
    "    ])\n",
    "\n",
    "table = ax.table(cellText=comparison_data,\n",
    "                colLabels=['Metric', 'GT Mean', 'Inc Mean', 'Difference', 'GT Median', 'Inc Median'],\n",
    "                cellLoc='center',\n",
    "                loc='center',\n",
    "                colWidths=[0.2, 0.15, 0.15, 0.15, 0.15, 0.15])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Style the header\n",
    "for i in range(6):\n",
    "    table[(0, i)].set_facecolor('#34495e')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Color code differences\n",
    "for i in range(1, len(comparison_data) + 1):\n",
    "    diff_val = float(comparison_data[i-1][3])\n",
    "    if abs(diff_val) < 0.01:\n",
    "        table[(i, 3)].set_facecolor('#ecf0f1')  # Very similar (gray)\n",
    "    elif diff_val > 0:\n",
    "        table[(i, 3)].set_facecolor('#d5f4e6')  # GT higher (light green)\n",
    "    else:\n",
    "        table[(i, 3)].set_facecolor('#fadbd8')  # Inc higher (light red)\n",
    "\n",
    "ax.set_title('Summary Comparison: Ground Truth vs Incidents', \n",
    "             fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Summary:\")\n",
    "print(\"   - Low scores (< 0.3) in both datasets = Confirms hypothesis\")\n",
    "print(\"   - Similar patterns = Both show same language differences\")\n",
    "print(\"   - This validates that n-grams are NOT suitable for evaluating close notes quality\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results\n",
    "\n",
    "**What we're doing:** Saving all the computed scores and metadata to a CSV file.\n",
    "\n",
    "**Why save:**\n",
    "- Can analyze results later without re-running the notebook\n",
    "- Can compare these n-gram results with semantic similarity results (from next notebook)\n",
    "- Can share results with team members\n",
    "- Can track improvements over time\n",
    "\n",
    "**What gets saved:**\n",
    "- All ROUGE scores for each pair\n",
    "- Category and metadata information\n",
    "- Text lengths and other analysis columns\n",
    "\n",
    "**File location:** `data/ngram_comparison_results.csv`\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Summary: What Did We Learn?\n",
    "\n",
    "### Key Findings from N-gram Analysis\n",
    "\n",
    "1. **N-gram metrics measure word/phrase overlap** between two texts\n",
    "2. **ROUGE scores tell us** how similar the vocabulary and phrases are\n",
    "3. **Expected result:** Low scores confirm that incident descriptions and close notes use different language\n",
    "\n",
    "### Interpretation of Results\n",
    "\n",
    "**If scores are LOW (0.1-0.3):**\n",
    "- ‚úÖ **Confirms our hypothesis:** Incident descriptions and close notes use different vocabulary\n",
    "- ‚úÖ **Conclusion:** N-grams are **not suitable** for evaluating close notes quality\n",
    "- ‚úÖ **Next step:** Focus on **LLM-as-a-Judge** evaluation (semantic understanding)\n",
    "\n",
    "**If scores are HIGH (0.5+):**\n",
    "- ‚ö†Ô∏è **Surprising result:** Incident descriptions and close notes share significant vocabulary\n",
    "- ‚ö†Ô∏è **Conclusion:** N-grams might be useful, but we still need semantic evaluation\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "This baseline analysis helps us understand:\n",
    "- **What metrics NOT to use:** If n-grams are low, they won't help evaluate close notes\n",
    "- **What to focus on:** LLM-as-a-Judge will evaluate semantic quality, not just word overlap\n",
    "- **Baseline for comparison:** We can compare future LLM-generated close notes against these ground truth references\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Next Steps: LLM-as-a-Judge Evaluation\n",
    "\n",
    "The **real evaluation** will happen in the next phase using **LLM-as-a-Judge**, which will:\n",
    "\n",
    "1. **Compare close notes** (existing or LLM-generated) against ground truth references\n",
    "2. **Evaluate multiple criteria:**\n",
    "   - **Topic coverage:** Does the close note cover the same topics as the reference?\n",
    "   - **Profile data accuracy:** Is client/system information correct?\n",
    "   - **Supporting facts:** Are the facts consistent with the reference?\n",
    "   - **No invented facts:** Does it avoid making up information?\n",
    "   - **Text structure:** Is it well-organized and clear?\n",
    "   - **Conclusion quality:** Does it provide a clear resolution summary?\n",
    "\n",
    "3. **Provide scores (0-5)** for each criterion, similar to the example provided:\n",
    "   ```json\n",
    "   {\n",
    "     \"check_topic_coverage\": 4,\n",
    "     \"check_profile_data\": 5,\n",
    "     \"check_supporting_facts\": 5,\n",
    "     \"check_facts_are_not_invented\": 5,\n",
    "     \"check_text_structure\": 4,\n",
    "     \"check_conclusion\": 5,\n",
    "     \"general_score\": 4.67\n",
    "   }\n",
    "   ```\n",
    "\n",
    "4. **Handle context differences:** Each incident has different context, so evaluation will be relative to similar incidents/categories\n",
    "\n",
    "**This approach is more suitable** because:\n",
    "- It evaluates **meaning and quality**, not just word overlap\n",
    "- It can handle **different contexts** (each incident is unique)\n",
    "- It provides **explainable scores** with reasoning\n",
    "- It's **scalable** and doesn't require human labeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "# This saves all our computed scores and metadata to a CSV file\n",
    "# You can open this file in Excel or any spreadsheet program later\n",
    "\n",
    "output_path = data_dir / \"ngram_comparison_results.csv\"\n",
    "results_df.to_csv(output_path, index=False)  # Save as CSV (comma-separated values)\n",
    "print(f\"‚úÖ Saved results to: {output_path}\")\n",
    "print(f\"   Total pairs: {len(results_df)}\")\n",
    "print(f\"   Columns: {list(results_df.columns)}\")\n",
    "print(\"\\nüí° You can now:\")\n",
    "print(\"   - Open this file in Excel or Google Sheets\")\n",
    "print(\"   - Filter/sort by category or score\")\n",
    "print(\"   - Use this as a baseline for future comparisons\")\n",
    "\n",
    "# Summary statistics\n",
    "# Print a final summary comparing both datasets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Ground Truth pairs evaluated: {len(gt_results_df)}\")\n",
    "print(f\"Incidents pairs evaluated: {len(inc_results_df)}\")\n",
    "print(f\"Total pairs: {len(results_df)}\")\n",
    "\n",
    "print(f\"\\nMean Scores Comparison:\")\n",
    "print(f\"{'Metric':<15} {'Ground Truth':<15} {'Incidents':<15} {'Difference':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for metric in metric_cols:\n",
    "    gt_mean = gt_results_df[metric].mean()\n",
    "    inc_mean = inc_results_df[metric].mean()\n",
    "    diff = gt_mean - inc_mean\n",
    "    print(f\"{metric:<15} {gt_mean:<15.4f} {inc_mean:<15.4f} {diff:+.4f}\")\n",
    "\n",
    "# Conclusion based on comparison of both datasets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCLUSION & RECOMMENDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gt_mean_rouge1 = gt_results_df['rouge1'].mean()\n",
    "inc_mean_rouge1 = inc_results_df['rouge1'].mean()\n",
    "overall_mean = (gt_mean_rouge1 + inc_mean_rouge1) / 2\n",
    "\n",
    "if overall_mean < 0.3:\n",
    "    print(\"‚úÖ Hypothesis CONFIRMED: Low n-gram scores detected in BOTH datasets\")\n",
    "    print(f\"   - Ground Truth mean ROUGE-1: {gt_mean_rouge1:.4f}\")\n",
    "    print(f\"   - Incidents mean ROUGE-1: {inc_mean_rouge1:.4f}\")\n",
    "    print(\"   - Incident descriptions and close notes use different vocabulary\")\n",
    "    print(\"   - N-grams are NOT suitable for evaluating close notes quality\")\n",
    "    print(\"   - Recommendation: Proceed with LLM-as-a-Judge evaluation\")\n",
    "elif overall_mean < 0.5:\n",
    "    print(\"‚ö†Ô∏è  Partial overlap detected in both datasets\")\n",
    "    print(f\"   - Ground Truth mean ROUGE-1: {gt_mean_rouge1:.4f}\")\n",
    "    print(f\"   - Incidents mean ROUGE-1: {inc_mean_rouge1:.4f}\")\n",
    "    print(\"   - Some vocabulary is shared, but significant differences remain\")\n",
    "    print(\"   - Recommendation: Use LLM-as-a-Judge for semantic evaluation\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Higher overlap than expected\")\n",
    "    print(f\"   - Ground Truth mean ROUGE-1: {gt_mean_rouge1:.4f}\")\n",
    "    print(f\"   - Incidents mean ROUGE-1: {inc_mean_rouge1:.4f}\")\n",
    "    print(\"   - This might indicate incident descriptions contain resolution language\")\n",
    "    print(\"   - Recommendation: Investigate further, but still use LLM-as-a-Judge\")\n",
    "\n",
    "# Add comparison insight\n",
    "if abs(gt_mean_rouge1 - inc_mean_rouge1) < 0.05:\n",
    "    print(\"\\nüí° Insight: Both datasets show similar patterns - validates comparison approach\")\n",
    "else:\n",
    "    print(f\"\\nüí° Insight: Datasets differ by {abs(gt_mean_rouge1 - inc_mean_rouge1):.4f} - interesting finding!\")\n",
    "\n",
    "print(f\"\\nResults saved to: {output_path}\")\n",
    "print(\"\\nüéâ Baseline analysis complete!\")\n",
    "print(\"   Next step: Implement LLM-as-a-Judge evaluation (Notebook 05)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
