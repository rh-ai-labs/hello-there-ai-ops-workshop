{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: LLM-as-a-Judge Evaluation\n",
    "\n",
    "## ðŸŽ¯ What is This Notebook About?\n",
    "\n",
    "This notebook evaluates close notes quality using **LLM-as-a-Judge** - an automated evaluation method that uses a Large Language Model (LLM) to assess close notes based on structured criteria.\n",
    "\n",
    "**Context:**\n",
    "1. We have **two datasets:**\n",
    "   - **Reference Dataset** (good close notes) - High-quality examples\n",
    "   - **Other Incidents Dataset** (bad/regular close notes) - Standard examples\n",
    "   \n",
    "2. We want to **evaluate close notes** using multiple quality criteria:\n",
    "   - Does it provide useful information?\n",
    "   - Is it specific and detailed?\n",
    "   - Is it complete?\n",
    "   - Does it avoid generic phrases?\n",
    "   - Is it clear and well-written?\n",
    "\n",
    "**This notebook's purpose:**\n",
    "- **Set up evaluation criteria** - Define what makes a good close note\n",
    "- **Evaluate close notes** - Use LLM to judge quality across multiple dimensions\n",
    "- **Compare results** - See how good vs bad close notes score differently\n",
    "- **Understand scoring** - Learn what the scores mean and how to interpret them\n",
    "\n",
    "**What we'll learn:**\n",
    "- LLM-as-a-Judge provides structured, explainable evaluation\n",
    "- Good close notes score higher across all criteria\n",
    "- Bad close notes score lower, especially on specificity and completeness\n",
    "- This evaluation method can be used to assess AI-generated close notes\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Key Concepts Explained\n",
    "\n",
    "### What is LLM-as-a-Judge?\n",
    "\n",
    "**LLM-as-a-Judge** is a method where we use a Large Language Model (like Llama) to evaluate text quality, similar to how a human judge would evaluate it.\n",
    "\n",
    "**Think of it like this:**\n",
    "- **Human judge:** Reads a close note and gives it a score based on criteria\n",
    "- **LLM judge:** Does the same thing, but uses AI to be consistent and scalable\n",
    "\n",
    "**How it works:**\n",
    "1. We define **evaluation criteria** (what to look for)\n",
    "2. We provide the **close note** and **incident context** to the LLM\n",
    "3. The LLM **assesses** the close note against each criterion\n",
    "4. The LLM **selects an option** (e.g., \"Excellent\", \"Acceptable\", \"Bad\")\n",
    "5. We get a **score** (0.0 to 1.0) and **reasoning** (why that score was given)\n",
    "\n",
    "**Why this matters:**\n",
    "- Provides **consistent evaluation** (same criteria applied to all notes)\n",
    "- Gives **explainable scores** (we know why a score was given)\n",
    "- Can **scale** to evaluate many close notes automatically\n",
    "- Helps **identify** what makes a close note good or bad\n",
    "\n",
    "### What are Evaluation Criteria?\n",
    "\n",
    "**Evaluation criteria** are specific questions we ask about a close note's quality.\n",
    "\n",
    "**Our 5 criteria:**\n",
    "1. **Informativeness** - Does it provide useful information?\n",
    "2. **Specificity** - Does it include specific details?\n",
    "3. **Completeness** - Does it cover all key aspects?\n",
    "4. **No Generic Statements** - Does it avoid generic phrases?\n",
    "5. **Clarity** - Is it well-written and clear?\n",
    "\n",
    "**Each criterion has options:**\n",
    "- **Excellent** (score: 1.0) - Meets the criterion perfectly\n",
    "- **Acceptable** (score: 0.75) - Good but could be better\n",
    "- **Could be Improved** (score: 0.4-0.6) - Needs improvement\n",
    "- **Bad** (score: 0.0-0.2) - Doesn't meet the criterion\n",
    "\n",
    "**Why multiple criteria?**\n",
    "- One score isn't enough - we need to understand **what** makes a close note good\n",
    "- Different close notes may be strong in different areas\n",
    "- Helps identify **specific improvements** needed\n",
    "\n",
    "### How Does Scoring Work?\n",
    "\n",
    "**Scoring process:**\n",
    "1. LLM reads the close note and incident context\n",
    "2. For each criterion, LLM evaluates the close note\n",
    "3. LLM selects an option (e.g., \"Excellent\")\n",
    "4. Option is converted to a numeric score (e.g., 1.0)\n",
    "5. We get scores for all 5 criteria\n",
    "6. We calculate an **average score** across all criteria\n",
    "\n",
    "**Score interpretation:**\n",
    "- **0.8 - 1.0** = Excellent close note (high quality)\n",
    "- **0.6 - 0.8** = Good close note (acceptable quality)\n",
    "- **0.4 - 0.6** = Needs improvement\n",
    "- **0.0 - 0.4** = Poor close note (low quality)\n",
    "\n",
    "**Example:**\n",
    "- Close note scores: Informativeness=1.0, Specificity=0.8, Completeness=1.0, No Generic=1.0, Clarity=0.9\n",
    "- Average: **0.94** = Excellent quality close note!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Objectives\n",
    "\n",
    "This notebook will:\n",
    "1. **Load** reference and other incidents datasets\n",
    "2. **Set up evaluation criteria** - Define 5 quality dimensions\n",
    "3. **Configure LLM-as-a-Judge** - Set up Ollama and Unitxt\n",
    "4. **Evaluate close notes** - Score close notes from both datasets\n",
    "5. **Compare results** - Analyze differences between good and bad close notes\n",
    "6. **Visualize scores** - Create charts showing score distributions\n",
    "7. **Interpret results** - Understand what the scores mean\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ What We're Evaluating\n",
    "\n",
    "**Datasets:**\n",
    "- **Reference Dataset** (`reference_close_notes.csv`) - Good close notes (ground truth)\n",
    "- **Other Incidents Dataset** (`other_incidents.csv`) - Bad/regular close notes\n",
    "\n",
    "**What we'll evaluate:**\n",
    "- Close notes from both datasets\n",
    "- Using 5 evaluation criteria\n",
    "- With incident context (`content` field) for better evaluation\n",
    "- Get scores and reasoning for each evaluation\n",
    "\n",
    "**Expected results:**\n",
    "- Reference close notes should score **higher** (0.7-1.0 average)\n",
    "- Other incidents should score **lower** (0.3-0.6 average)\n",
    "- Differences should be most obvious in **Specificity** and **No Generic Statements**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Getting Started\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our evaluation environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "**What we're doing:** Importing the libraries we need for LLM-as-a-Judge evaluation.\n",
    "\n",
    "**Libraries:**\n",
    "- `pandas` - For working with datasets\n",
    "- `unitxt` - For LLM-as-a-Judge evaluation framework\n",
    "- `matplotlib` and `seaborn` - For visualizations\n",
    "- `numpy` - For numerical operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Unitxt imports for LLM-as-a-Judge\n",
    "from unitxt.api import create_dataset, evaluate\n",
    "from unitxt.inference import CrossProviderInferenceEngine\n",
    "from unitxt.llm_as_judge import LLMJudgeDirect\n",
    "from unitxt.llm_as_judge_constants import CriteriaWithOptions\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ðŸ“Š Working directory: {Path.cwd()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Datasets\n",
    "\n",
    "**What we're doing:** Loading the reference dataset (good close notes) and other incidents dataset (bad/regular close notes) that we created in Notebook 02.\n",
    "\n",
    "**Files:**\n",
    "- `data/reference_close_notes.csv` - High-quality close notes (ground truth)\n",
    "- `data/other_incidents.csv` - Standard close notes (for comparison)\n",
    "\n",
    "**Key fields:**\n",
    "- `content` - Incident description (context for evaluation)\n",
    "- `close_notes_ref` or `close_notes` - The close note to evaluate\n",
    "- `category` - Incident category (for filtering/grouping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "data_dir = Path(\"../data\")\n",
    "\n",
    "reference_df = pd.read_csv(data_dir / \"reference_close_notes.csv\")\n",
    "other_incidents_df = pd.read_csv(data_dir / \"other_incidents.csv\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASETS LOADED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“Š Reference Dataset (Good Close Notes):\")\n",
    "print(f\"   - Total records: {len(reference_df)}\")\n",
    "print(f\"   - Columns: {list(reference_df.columns)}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Other Incidents Dataset (Bad/Regular Close Notes):\")\n",
    "print(f\"   - Total records: {len(other_incidents_df)}\")\n",
    "print(f\"   - Columns: {list(other_incidents_df.columns)}\")\n",
    "\n",
    "# Check for required fields\n",
    "print(f\"\\nðŸ” Checking required fields...\")\n",
    "required_ref_fields = ['content', 'close_notes_ref']\n",
    "required_other_fields = ['content', 'close_notes']\n",
    "\n",
    "missing_ref = [f for f in required_ref_fields if f not in reference_df.columns]\n",
    "missing_other = [f for f in required_other_fields if f not in other_incidents_df.columns]\n",
    "\n",
    "if missing_ref:\n",
    "    print(f\"   âš ï¸  Missing in reference dataset: {missing_ref}\")\n",
    "else:\n",
    "    print(f\"   âœ… Reference dataset has all required fields\")\n",
    "\n",
    "if missing_other:\n",
    "    print(f\"   âš ï¸  Missing in other incidents dataset: {missing_other}\")\n",
    "else:\n",
    "    print(f\"   âœ… Other incidents dataset has all required fields\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Sample Data for Evaluation\n",
    "\n",
    "**What we're doing:** Selecting a sample of close notes from both datasets to evaluate. We'll start with a small sample to test the evaluation, then can expand.\n",
    "\n",
    "**Why sample?**\n",
    "- LLM evaluation takes time and resources\n",
    "- Starting small helps us verify everything works\n",
    "- We can evaluate more later if needed\n",
    "\n",
    "**Selection strategy:**\n",
    "- Take a diverse sample (different categories)\n",
    "- Ensure we have incident context (`content` field)\n",
    "- Filter out empty or very short close notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sample data for evaluation\n",
    "# Start with a small sample (e.g., 5-10 from each dataset) for testing\n",
    "SAMPLE_SIZE = 5\n",
    "\n",
    "# Filter reference dataset: ensure we have content and close_notes_ref\n",
    "reference_sample = reference_df[\n",
    "    (reference_df['content'].notna()) & \n",
    "    (reference_df['content'].astype(str).str.strip() != '') &\n",
    "    (reference_df['close_notes_ref'].notna()) & \n",
    "    (reference_df['close_notes_ref'].astype(str).str.strip() != '') &\n",
    "    (reference_df['close_notes_ref'].astype(str).str.len() > 20)  # Minimum length\n",
    "].head(SAMPLE_SIZE).copy()\n",
    "\n",
    "# Filter other incidents dataset: ensure we have content and close_notes\n",
    "other_sample = other_incidents_df[\n",
    "    (other_incidents_df['content'].notna()) & \n",
    "    (other_incidents_df['content'].astype(str).str.strip() != '') &\n",
    "    (other_incidents_df['close_notes'].notna()) & \n",
    "    (other_incidents_df['close_notes'].astype(str).str.strip() != '') &\n",
    "    (other_incidents_df['close_notes'].astype(str).str.len() > 10)  # Minimum length\n",
    "].head(SAMPLE_SIZE).copy()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE DATA PREPARED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“Š Reference Sample (Good Close Notes):\")\n",
    "print(f\"   - Sample size: {len(reference_sample)}\")\n",
    "if 'category' in reference_sample.columns:\n",
    "    print(f\"   - Categories: {reference_sample['category'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Other Incidents Sample (Bad/Regular Close Notes):\")\n",
    "print(f\"   - Sample size: {len(other_sample)}\")\n",
    "if 'category' in other_sample.columns:\n",
    "    print(f\"   - Categories: {other_sample['category'].value_counts().to_dict()}\")\n",
    "\n",
    "# Show example\n",
    "print(f\"\\nðŸ“ Example Reference Close Note:\")\n",
    "if len(reference_sample) > 0:\n",
    "    example_idx = 0\n",
    "    print(f\"   Content (first 150 chars): {reference_sample.iloc[example_idx]['content'][:150]}...\")\n",
    "    print(f\"   Close Note: {reference_sample.iloc[example_idx]['close_notes_ref'][:200]}...\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Criteria\n",
    "\n",
    "**What we're doing:** Defining the 5 evaluation criteria that will be used to judge close note quality.\n",
    "\n",
    "**Each criterion:**\n",
    "- Has a **name** and **description** (what we're looking for)\n",
    "- Has **options** (e.g., \"Excellent\", \"Acceptable\", \"Bad\")\n",
    "- Has an **option_map** (converts options to numeric scores 0.0-1.0)\n",
    "\n",
    "**Why these criteria?**\n",
    "- They cover the key aspects of a good close note\n",
    "- They're specific enough to be evaluated consistently\n",
    "- They help identify what makes a close note good or bad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 5 evaluation criteria for close notes quality\n",
    "\n",
    "informativeness = CriteriaWithOptions.from_obj({\n",
    "    \"name\": \"Informativeness\",\n",
    "    \"description\": \"Does the close note provide useful, specific information about what happened and how it was resolved?\",\n",
    "    \"options\": [\n",
    "        {\"name\": \"Excellent\", \"description\": \"Highly informative with specific details about problem, cause, and resolution.\"},\n",
    "        {\"name\": \"Acceptable\", \"description\": \"Provides useful information but could include more specific details.\"},\n",
    "        {\"name\": \"Could be Improved\", \"description\": \"Some information present but vague or incomplete.\"},\n",
    "        {\"name\": \"Bad\", \"description\": \"Little or no useful information (e.g., just 'Issue resolved').\"},\n",
    "    ],\n",
    "    \"option_map\": {\"Excellent\": 1.0, \"Acceptable\": 0.75, \"Could be Improved\": 0.4, \"Bad\": 0.0},\n",
    "})\n",
    "\n",
    "specificity = CriteriaWithOptions.from_obj({\n",
    "    \"name\": \"Specificity\",\n",
    "    \"description\": \"Does the close note include specific details such as error messages, specific actions taken, browser versions, or exact resolutions?\",\n",
    "    \"options\": [\n",
    "        {\"name\": \"Highly Specific\", \"description\": \"Includes concrete details like error codes, specific steps taken, browser versions, or exact outcomes.\"},\n",
    "        {\"name\": \"Somewhat Specific\", \"description\": \"Includes some details but could be more precise.\"},\n",
    "        {\"name\": \"Vague\", \"description\": \"Lacks specific details; too general.\"},\n",
    "    ],\n",
    "    \"option_map\": {\"Highly Specific\": 1.0, \"Somewhat Specific\": 0.6, \"Vague\": 0.2},\n",
    "})\n",
    "\n",
    "completeness = CriteriaWithOptions.from_obj({\n",
    "    \"name\": \"Completeness\",\n",
    "    \"description\": \"Does the close note cover the key aspects: what the problem was, what was done to resolve it, and the outcome?\",\n",
    "    \"options\": [\n",
    "        {\"name\": \"Complete\", \"description\": \"Covers problem, actions taken, and outcome clearly.\"},\n",
    "        {\"name\": \"Partially Complete\", \"description\": \"Covers some aspects but missing important details.\"},\n",
    "        {\"name\": \"Incomplete\", \"description\": \"Significant gaps in information; missing key aspects.\"},\n",
    "    ],\n",
    "    \"option_map\": {\"Complete\": 1.0, \"Partially Complete\": 0.5, \"Incomplete\": 0.0},\n",
    "})\n",
    "\n",
    "no_generic_statements = CriteriaWithOptions.from_obj({\n",
    "    \"name\": \"No Generic Statements\",\n",
    "    \"description\": \"Does the close note avoid generic, unhelpful phrases like 'Issue resolved', 'No changes noted', or 'Resolved per user' without explanation?\",\n",
    "    \"options\": [\n",
    "        {\"name\": \"No Generic Phrases\", \"description\": \"No generic statements; all content is specific and informative.\"},\n",
    "        {\"name\": \"Few Generic Phrases\", \"description\": \"Mostly specific but includes some generic statements.\"},\n",
    "        {\"name\": \"Too Generic\", \"description\": \"Primarily or entirely generic statements without explanation.\"},\n",
    "    ],\n",
    "    \"option_map\": {\"No Generic Phrases\": 1.0, \"Few Generic Phrases\": 0.4, \"Too Generic\": 0.0},\n",
    "})\n",
    "\n",
    "clarity = CriteriaWithOptions.from_obj({\n",
    "    \"name\": \"Clarity\",\n",
    "    \"description\": \"Is the close note well-written, clear, and easy to understand?\",\n",
    "    \"options\": [\n",
    "        {\"name\": \"Clear\", \"description\": \"Well-structured, easy to follow, and professional.\"},\n",
    "        {\"name\": \"Somewhat Clear\", \"description\": \"Understandable but could be better organized or more concise.\"},\n",
    "        {\"name\": \"Unclear\", \"description\": \"Difficult to understand or poorly structured.\"},\n",
    "    ],\n",
    "    \"option_map\": {\"Clear\": 1.0, \"Somewhat Clear\": 0.6, \"Unclear\": 0.0},\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATION CRITERIA DEFINED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nâœ… Created {5} evaluation criteria:\")\n",
    "print(\"   1. Informativeness - Does it provide useful information?\")\n",
    "print(\"   2. Specificity - Does it include specific details?\")\n",
    "print(\"   3. Completeness - Does it cover all key aspects?\")\n",
    "print(\"   4. No Generic Statements - Does it avoid generic phrases?\")\n",
    "print(\"   5. Clarity - Is it well-written and clear?\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure LLM-as-a-Judge\n",
    "\n",
    "**What we're doing:** Setting up the LLM judge using Ollama (local LLM) and Unitxt framework.\n",
    "\n",
    "**Configuration:**\n",
    "- **Model:** Llama 3.2 3B Instruct (via Ollama)\n",
    "- **Provider:** Ollama (runs locally)\n",
    "- **Context:** We'll pass the incident description (`content`) as context\n",
    "- **Metrics:** One metric per criterion (5 metrics total)\n",
    "\n",
    "**Important:** Make sure Ollama is running (`ollama serve`) and the model is pulled (`ollama pull llama3.2:3b`) before running this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics for each criterion\n",
    "# Each criterion gets its own LLMJudgeDirect metric instance\n",
    "\n",
    "metrics = [\n",
    "    LLMJudgeDirect(\n",
    "        inference_engine=CrossProviderInferenceEngine(\n",
    "            model=\"llama3.2:3b\",  # Ollama model name\n",
    "            max_tokens=1024,\n",
    "            data_classification_policy=[\"private\"],\n",
    "            provider=\"ollama\",\n",
    "        ),\n",
    "        criteria=informativeness,\n",
    "        context_fields=[\"question\"],  # Will contain incident context\n",
    "        criteria_field=\"criteria\",\n",
    "    ),\n",
    "    LLMJudgeDirect(\n",
    "        inference_engine=CrossProviderInferenceEngine(\n",
    "            model=\"llama3.2:3b\",\n",
    "            max_tokens=1024,\n",
    "            data_classification_policy=[\"private\"],\n",
    "            provider=\"ollama\",\n",
    "        ),\n",
    "        criteria=specificity,\n",
    "        context_fields=[\"question\"],\n",
    "        criteria_field=\"criteria\",\n",
    "    ),\n",
    "    LLMJudgeDirect(\n",
    "        inference_engine=CrossProviderInferenceEngine(\n",
    "            model=\"llama3.2:3b\",\n",
    "            max_tokens=1024,\n",
    "            data_classification_policy=[\"private\"],\n",
    "            provider=\"ollama\",\n",
    "        ),\n",
    "        criteria=completeness,\n",
    "        context_fields=[\"question\"],\n",
    "        criteria_field=\"criteria\",\n",
    "    ),\n",
    "    LLMJudgeDirect(\n",
    "        inference_engine=CrossProviderInferenceEngine(\n",
    "            model=\"llama3.2:3b\",\n",
    "            max_tokens=1024,\n",
    "            data_classification_policy=[\"private\"],\n",
    "            provider=\"ollama\",\n",
    "        ),\n",
    "        criteria=no_generic_statements,\n",
    "        context_fields=[\"question\"],\n",
    "        criteria_field=\"criteria\",\n",
    "    ),\n",
    "    LLMJudgeDirect(\n",
    "        inference_engine=CrossProviderInferenceEngine(\n",
    "            model=\"llama3.2:3b\",\n",
    "            max_tokens=1024,\n",
    "            data_classification_policy=[\"private\"],\n",
    "            provider=\"ollama\",\n",
    "        ),\n",
    "        criteria=clarity,\n",
    "        context_fields=[\"question\"],\n",
    "        criteria_field=\"criteria\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LLM-AS-A-JUDGE CONFIGURED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nâœ… Created {len(metrics)} evaluation metrics (one per criterion)\")\n",
    "print(f\"   Model: llama3.2:3b (via Ollama)\")\n",
    "print(f\"   Provider: Ollama (local)\")\n",
    "print(f\"\\nâš ï¸  Prerequisites:\")\n",
    "print(f\"   1. Ollama server running: ollama serve\")\n",
    "print(f\"   2. Model pulled: ollama pull llama3.2:3b\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for Evaluation\n",
    "\n",
    "**What we're doing:** Formatting the close notes and incident context into the format expected by Unitxt.\n",
    "\n",
    "**Format:**\n",
    "- Each item needs a `question` field containing:\n",
    "  - Instruction to write a close note\n",
    "  - Full incident context (from `content` field)\n",
    "  - This gives the LLM judge all the information it needs\n",
    "\n",
    "**We'll prepare:**\n",
    "- Reference dataset close notes (good examples)\n",
    "- Other incidents close notes (bad/regular examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for evaluation\n",
    "# Format: question field contains instruction + incident context\n",
    "\n",
    "def prepare_evaluation_data(df, close_notes_col='close_notes_ref', dataset_type='reference'):\n",
    "    \"\"\"Prepare data in format expected by Unitxt.\"\"\"\n",
    "    data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        content = str(row['content']) if pd.notna(row['content']) else \"\"\n",
    "        close_note = str(row[close_notes_col]) if pd.notna(row[close_notes_col]) else \"\"\n",
    "        \n",
    "        # Create question with incident context\n",
    "        question = f\"\"\"Write a close note for this incident:\n",
    "\n",
    "{content}\n",
    "\n",
    "The close note being evaluated is:\n",
    "{close_note}\"\"\"\n",
    "        \n",
    "        data.append({\n",
    "            \"question\": question,\n",
    "            \"dataset_type\": dataset_type,\n",
    "            \"incident_id\": row.get('number', f\"INC-{idx}\"),\n",
    "            \"category\": row.get('category', 'Unknown'),\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Prepare reference dataset\n",
    "reference_data = prepare_evaluation_data(\n",
    "    reference_sample, \n",
    "    close_notes_col='close_notes_ref',\n",
    "    dataset_type='reference'\n",
    ")\n",
    "\n",
    "# Prepare other incidents dataset\n",
    "other_data = prepare_evaluation_data(\n",
    "    other_sample,\n",
    "    close_notes_col='close_notes',\n",
    "    dataset_type='other'\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA PREPARED FOR EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“Š Reference Data:\")\n",
    "print(f\"   - Records: {len(reference_data)}\")\n",
    "print(f\"   - Example question length: {len(reference_data[0]['question'])} chars\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Other Incidents Data:\")\n",
    "print(f\"   - Records: {len(other_data)}\")\n",
    "print(f\"   - Example question length: {len(other_data[0]['question'])} chars\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Close Notes\n",
    "\n",
    "**What we're doing:** Running the LLM-as-a-Judge evaluation on our close notes.\n",
    "\n",
    "**Process:**\n",
    "1. Create a dataset with our close notes\n",
    "2. For each close note, the LLM evaluates it against all 5 criteria\n",
    "3. We get scores (0.0-1.0) and reasoning for each criterion\n",
    "4. Results are stored for analysis\n",
    "\n",
    "**Note:** This may take a few minutes as the LLM processes each close note for each criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data for evaluation\n",
    "all_data = reference_data + other_data\n",
    "\n",
    "# Create dataset\n",
    "print(\"Creating dataset...\")\n",
    "dataset = create_dataset(\n",
    "    task=\"tasks.qa.open\",\n",
    "    test_set=all_data,\n",
    "    metrics=metrics,\n",
    "    split=\"test\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dataset created with {len(dataset)} examples\")\n",
    "\n",
    "# Prepare predictions (the close notes to evaluate)\n",
    "# For each item in data, extract the close note from the question\n",
    "predictions = []\n",
    "for item in all_data:\n",
    "    # Extract close note from question (it's after \"The close note being evaluated is:\")\n",
    "    question_parts = item['question'].split(\"The close note being evaluated is:\")\n",
    "    if len(question_parts) > 1:\n",
    "        close_note = question_parts[1].strip()\n",
    "    else:\n",
    "        # Fallback: extract from original data\n",
    "        if item['dataset_type'] == 'reference':\n",
    "            idx = reference_data.index(item) if item in reference_data else 0\n",
    "            close_note = reference_sample.iloc[idx]['close_notes_ref']\n",
    "        else:\n",
    "            idx = other_data.index(item) if item in other_data else 0\n",
    "            close_note = other_sample.iloc[idx]['close_notes']\n",
    "    predictions.append(close_note)\n",
    "\n",
    "print(f\"âœ… Prepared {len(predictions)} predictions for evaluation\")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING LLM-AS-A-JUDGE EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"â³ This may take a few minutes...\")\n",
    "print(\"   Evaluating each close note against 5 criteria...\\n\")\n",
    "\n",
    "results = evaluate(predictions=predictions, data=dataset)\n",
    "\n",
    "print(\"âœ… Evaluation completed!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Extract and Analyze Results\n",
    "\n",
    "**What we're doing:** Extracting scores from the evaluation results and organizing them for analysis.\n",
    "\n",
    "**We'll extract:**\n",
    "- Score for each criterion (0.0-1.0)\n",
    "- Selected option for each criterion (e.g., \"Excellent\", \"Acceptable\")\n",
    "- Reasoning for each evaluation (why that score was given)\n",
    "- Overall average score across all criteria\n",
    "\n",
    "**Then we'll:**\n",
    "- Compare scores between reference (good) and other (bad) close notes\n",
    "- Identify which criteria show the biggest differences\n",
    "- Visualize the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results and organize into a DataFrame\n",
    "results_list = []\n",
    "\n",
    "if hasattr(results, 'instance_scores') and isinstance(results.instance_scores, list):\n",
    "    for i, instance in enumerate(results.instance_scores):\n",
    "        if isinstance(instance, dict):\n",
    "            # Get metadata from original data\n",
    "            metadata = all_data[i]\n",
    "            \n",
    "            # Extract scores for each criterion\n",
    "            result_row = {\n",
    "                'dataset_type': metadata['dataset_type'],\n",
    "                'incident_id': metadata['incident_id'],\n",
    "                'category': metadata.get('category', 'Unknown'),\n",
    "            }\n",
    "            \n",
    "            # Find all criteria scores\n",
    "            score_keys = [k for k in instance.keys() if k.endswith('_selected_option')]\n",
    "            all_scores = []\n",
    "            \n",
    "            for score_key in score_keys:\n",
    "                base_name = score_key.replace('_selected_option', '')\n",
    "                score = instance.get(base_name, None)\n",
    "                selected_option = instance.get(f'{base_name}_selected_option', 'N/A')\n",
    "                \n",
    "                if score is not None:\n",
    "                    # Clean criterion name (remove underscores, title case)\n",
    "                    criterion_name = base_name.replace('_', ' ').title()\n",
    "                    result_row[f'{criterion_name}_score'] = score\n",
    "                    result_row[f'{criterion_name}_option'] = selected_option\n",
    "                    all_scores.append(score)\n",
    "            \n",
    "            # Calculate average score\n",
    "            if all_scores:\n",
    "                result_row['average_score'] = sum(all_scores) / len(all_scores)\n",
    "            else:\n",
    "                result_row['average_score'] = None\n",
    "            \n",
    "            results_list.append(result_row)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESULTS EXTRACTED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“Š Total evaluations: {len(results_df)}\")\n",
    "print(f\"   - Reference (good): {len(results_df[results_df['dataset_type'] == 'reference'])}\")\n",
    "print(f\"   - Other (bad/regular): {len(results_df[results_df['dataset_type'] == 'other'])}\")\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    print(f\"\\nðŸ“‹ Sample results:\")\n",
    "    print(results_df[['dataset_type', 'average_score']].head())\n",
    "    \n",
    "    # Show average scores by dataset type\n",
    "    print(f\"\\nðŸ“ˆ Average Scores by Dataset Type:\")\n",
    "    avg_by_type = results_df.groupby('dataset_type')['average_score'].agg(['mean', 'std', 'min', 'max'])\n",
    "    print(avg_by_type)\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Results\n",
    "\n",
    "**What we're doing:** Creating visualizations to compare scores between good and bad close notes.\n",
    "\n",
    "**Charts we'll create:**\n",
    "1. **Average Score Comparison** - Box plot showing score distributions\n",
    "2. **Criterion-by-Criterion Comparison** - See which criteria show biggest differences\n",
    "3. **Score Distribution** - Histogram showing how scores are distributed\n",
    "\n",
    "**What to look for:**\n",
    "- Reference (good) close notes should score **higher** overall\n",
    "- Biggest differences likely in **Specificity** and **No Generic Statements**\n",
    "- Scores should cluster: good notes 0.7-1.0, bad notes 0.3-0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('LLM-as-a-Judge Evaluation Results: Good vs Bad Close Notes', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Average Score Comparison\n",
    "ax1 = axes[0, 0]\n",
    "results_df.boxplot(column='average_score', by='dataset_type', ax=ax1)\n",
    "ax1.set_title('Average Score Distribution')\n",
    "ax1.set_xlabel('Dataset Type')\n",
    "ax1.set_ylabel('Average Score (0.0 - 1.0)')\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Criterion Scores Comparison\n",
    "ax2 = axes[0, 1]\n",
    "criterion_cols = [col for col in results_df.columns if col.endswith('_score')]\n",
    "if criterion_cols:\n",
    "    comparison_data = []\n",
    "    for criterion in criterion_cols:\n",
    "        criterion_name = criterion.replace('_score', '')\n",
    "        for dataset_type in ['reference', 'other']:\n",
    "            subset = results_df[results_df['dataset_type'] == dataset_type]\n",
    "            if len(subset) > 0:\n",
    "                comparison_data.append({\n",
    "                    'Criterion': criterion_name,\n",
    "                    'Dataset': 'Good (Reference)' if dataset_type == 'reference' else 'Bad/Regular (Other)',\n",
    "                    'Score': subset[criterion].mean()\n",
    "                })\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_pivot = comparison_df.pivot(index='Criterion', columns='Dataset', values='Score')\n",
    "        comparison_pivot.plot(kind='bar', ax=ax2, color=['#2ecc71', '#e74c3c'])\n",
    "        ax2.set_title('Average Scores by Criterion')\n",
    "        ax2.set_xlabel('Criterion')\n",
    "        ax2.set_ylabel('Average Score')\n",
    "        ax2.legend(title='Dataset Type')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Score Distribution Histogram\n",
    "ax3 = axes[1, 0]\n",
    "reference_scores = results_df[results_df['dataset_type'] == 'reference']['average_score'].dropna()\n",
    "other_scores = results_df[results_df['dataset_type'] == 'other']['average_score'].dropna()\n",
    "\n",
    "if len(reference_scores) > 0 and len(other_scores) > 0:\n",
    "    ax3.hist(reference_scores, bins=10, alpha=0.6, label='Good (Reference)', color='#2ecc71')\n",
    "    ax3.hist(other_scores, bins=10, alpha=0.6, label='Bad/Regular (Other)', color='#e74c3c')\n",
    "    ax3.set_title('Score Distribution')\n",
    "    ax3.set_xlabel('Average Score')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Summary Statistics Table\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for dataset_type in ['reference', 'other']:\n",
    "    subset = results_df[results_df['dataset_type'] == dataset_type]\n",
    "    if len(subset) > 0 and 'average_score' in subset.columns:\n",
    "        summary_data.append({\n",
    "            'Dataset': 'Good (Reference)' if dataset_type == 'reference' else 'Bad/Regular (Other)',\n",
    "            'Count': len(subset),\n",
    "            'Mean': subset['average_score'].mean(),\n",
    "            'Std': subset['average_score'].std(),\n",
    "            'Min': subset['average_score'].min(),\n",
    "            'Max': subset['average_score'].max()\n",
    "        })\n",
    "\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    table = ax4.table(cellText=summary_df.values,\n",
    "                     colLabels=summary_df.columns,\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     bbox=[0, 0, 1, 1])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    ax4.set_title('Summary Statistics', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualizations created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9b. Additional Visualizations: Heatmap and Radar Chart\n",
    "\n",
    "**What we're doing:** Creating two additional charts that provide deeper insights into the evaluation results.\n",
    "\n",
    "**New charts:**\n",
    "1. **Heatmap** - Visual comparison of scores across all criteria (quick overview)\n",
    "2. **Radar Chart** - Shows the \"profile\" of good vs bad notes (strengths/weaknesses)\n",
    "\n",
    "**Why these charts help:**\n",
    "- **Heatmap:** See all scores at once - which criteria show the biggest gaps?\n",
    "- **Radar Chart:** Understand the \"shape\" of quality - are good notes strong across all criteria or just some?\n",
    "\n",
    "**Think of it like:** \n",
    "- Heatmap = A color-coded report card showing where good notes excel\n",
    "- Radar Chart = A \"spider web\" showing the quality profile of each dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional visualizations: Heatmap and Radar Chart\n",
    "# These charts provide deeper insights into the evaluation results\n",
    "\n",
    "# Extract criterion columns (exclude average_score)\n",
    "criterion_cols = [\n",
    "    col\n",
    "    for col in results_df.columns\n",
    "    if col.endswith(\"_score\") and col != \"average_score\"\n",
    "]\n",
    "criterion_names = [\n",
    "    col.replace(\"_score\", \"\").replace(\"_\", \" \").title() for col in criterion_cols\n",
    "]\n",
    "\n",
    "if len(criterion_cols) > 0:\n",
    "    # Create figure with 2 subplots\n",
    "    fig2, (ax5, ax6) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    fig2.suptitle(\n",
    "        \"Additional Insights: Heatmap and Criterion Profile\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.02,\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # CHART 1: HEATMAP - Visual comparison across all criteria\n",
    "    # ========================================================================\n",
    "    # Prepare data for heatmap\n",
    "    heatmap_data = []\n",
    "    for dataset_type in [\"reference\", \"other\"]:\n",
    "        subset = results_df[results_df[\"dataset_type\"] == dataset_type]\n",
    "        if len(subset) > 0:\n",
    "            row_data = []\n",
    "            for criterion in criterion_cols:\n",
    "                row_data.append(subset[criterion].mean())\n",
    "            heatmap_data.append(row_data)\n",
    "\n",
    "    if heatmap_data:\n",
    "        heatmap_df = pd.DataFrame(\n",
    "            heatmap_data,\n",
    "            index=[\"Good (Reference)\", \"Bad/Regular (Other)\"],\n",
    "            columns=criterion_names,\n",
    "        )\n",
    "\n",
    "        # Create heatmap with better colormap\n",
    "        im = ax5.imshow(heatmap_df.values, cmap=\"RdYlGn\", aspect=\"auto\", vmin=0, vmax=1)\n",
    "\n",
    "        # Set ticks and labels\n",
    "        ax5.set_xticks(np.arange(len(heatmap_df.columns)))\n",
    "        ax5.set_yticks(np.arange(len(heatmap_df.index)))\n",
    "        ax5.set_xticklabels(heatmap_df.columns, rotation=45, ha=\"right\", fontsize=11)\n",
    "        ax5.set_yticklabels(heatmap_df.index, fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "        # Add text annotations with better formatting\n",
    "        for i in range(len(heatmap_df.index)):\n",
    "            for j in range(len(heatmap_df.columns)):\n",
    "                score = heatmap_df.iloc[i, j]\n",
    "                # Use white text for low scores, black for high scores\n",
    "                text_color = \"white\" if score < 0.5 else \"black\"\n",
    "                ax5.text(\n",
    "                    j,\n",
    "                    i,\n",
    "                    f\"{score:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=text_color,\n",
    "                    fontsize=11,\n",
    "                    fontweight=\"bold\",\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.7),\n",
    "                )\n",
    "\n",
    "        ax5.set_title(\n",
    "            \"Score Heatmap Across All Criteria\\n(Darker Green = Higher Score)\",\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "            pad=15,\n",
    "        )\n",
    "\n",
    "        # Add colorbar with better styling\n",
    "        cbar = plt.colorbar(im, ax=ax5, fraction=0.046, pad=0.04)\n",
    "        cbar.set_label(\n",
    "            \"Score (0.0 = Poor, 1.0 = Excellent)\", fontsize=11, fontweight=\"bold\"\n",
    "        )\n",
    "        cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "    # ========================================================================\n",
    "    # CHART 2: RADAR CHART - Criterion Profile Comparison\n",
    "    # ========================================================================\n",
    "    if len(criterion_cols) >= 3:\n",
    "        try:\n",
    "            # Calculate average scores for each criterion\n",
    "            ref_means = [\n",
    "                results_df[results_df[\"dataset_type\"] == \"reference\"][col].mean()\n",
    "                for col in criterion_cols\n",
    "            ]\n",
    "            other_means = [\n",
    "                results_df[results_df[\"dataset_type\"] == \"other\"][col].mean()\n",
    "                for col in criterion_cols\n",
    "            ]\n",
    "\n",
    "            # Number of criteria\n",
    "            N = len(criterion_cols)\n",
    "\n",
    "            # Compute angle for each criterion\n",
    "            angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "            angles += angles[:1]  # Complete the circle\n",
    "\n",
    "            # Add values to complete the circle\n",
    "            ref_means += ref_means[:1]\n",
    "            other_means += other_means[:1]\n",
    "\n",
    "            # Create radar chart\n",
    "            ax6 = plt.subplot(1, 2, 2, projection=\"polar\")\n",
    "            ax6.plot(\n",
    "                angles,\n",
    "                ref_means,\n",
    "                \"o-\",\n",
    "                linewidth=3,\n",
    "                label=\"Good (Reference)\",\n",
    "                color=\"#2ecc71\",\n",
    "                markersize=8,\n",
    "            )\n",
    "            ax6.fill(angles, ref_means, alpha=0.25, color=\"#2ecc71\")\n",
    "            ax6.plot(\n",
    "                angles,\n",
    "                other_means,\n",
    "                \"o-\",\n",
    "                linewidth=3,\n",
    "                label=\"Bad/Regular (Other)\",\n",
    "                color=\"#e74c3c\",\n",
    "                markersize=8,\n",
    "            )\n",
    "            ax6.fill(angles, other_means, alpha=0.25, color=\"#e74c3c\")\n",
    "\n",
    "            # Add criterion labels with better positioning\n",
    "            ax6.set_xticks(angles[:-1])\n",
    "            ax6.set_xticklabels(criterion_names, fontsize=10)\n",
    "            ax6.set_ylim(0, 1)\n",
    "            ax6.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "            ax6.set_yticklabels([\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], fontsize=9)\n",
    "            ax6.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "            ax6.set_title(\n",
    "                \"Criterion Profile Comparison\\n(Radar Chart - Shows Strengths/Weaknesses)\",\n",
    "                fontsize=14,\n",
    "                fontweight=\"bold\",\n",
    "                pad=20,\n",
    "            )\n",
    "            ax6.legend(\n",
    "                loc=\"upper right\",\n",
    "                bbox_to_anchor=(1.3, 1.1),\n",
    "                fontsize=11,\n",
    "                framealpha=0.9,\n",
    "            )\n",
    "\n",
    "            # Add score annotations at each point\n",
    "            for angle, ref_val, other_val in zip(\n",
    "                angles[:-1], ref_means[:-1], other_means[:-1]\n",
    "            ):\n",
    "                ax6.text(\n",
    "                    angle,\n",
    "                    ref_val + 0.1,\n",
    "                    f\"{ref_val:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=8,\n",
    "                    color=\"#2ecc71\",\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "                ax6.text(\n",
    "                    angle,\n",
    "                    other_val - 0.1,\n",
    "                    f\"{other_val:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=8,\n",
    "                    color=\"#e74c3c\",\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "        except Exception as e:\n",
    "            ax6.text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                f\"Radar chart unavailable\\n({str(e)[:40]})\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax6.transAxes,\n",
    "                fontsize=11,\n",
    "            )\n",
    "            ax6.set_title(\n",
    "                \"Criterion Profile (Radar Chart)\", fontsize=14, fontweight=\"bold\"\n",
    "            )\n",
    "    else:\n",
    "        ax6.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Radar chart requires at least 3 criteria\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=ax6.transAxes,\n",
    "            fontsize=11,\n",
    "        )\n",
    "        ax6.set_title(\"Criterion Profile (Radar Chart)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nâœ… Additional visualizations created!\")\n",
    "    print(\"   ðŸ“Š Charts: Heatmap (all criteria at once), Radar Chart (quality profile)\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"âš ï¸  No criterion columns found for additional visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Detailed Results Per Close Note\n",
    "\n",
    "**What we're doing:** Showing detailed evaluation results for each close note, including scores and reasoning for each criterion.\n",
    "\n",
    "**This helps us:**\n",
    "- Understand why each close note scored the way it did\n",
    "- See which criteria are strengths/weaknesses for each note\n",
    "- Learn what makes a close note good or bad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results for each close note\n",
    "print(\"=\"*80)\n",
    "print(\"DETAILED EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if hasattr(results, 'instance_scores') and isinstance(results.instance_scores, list):\n",
    "    for i, instance in enumerate(results.instance_scores):\n",
    "        metadata = all_data[i]\n",
    "        prediction = predictions[i]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CLOSE NOTE {i+1} - {metadata['dataset_type'].upper()}\")\n",
    "        print('='*80)\n",
    "        print(f\"\\nðŸ“ Close Note:\")\n",
    "        print(f\"{prediction[:300]}...\" if len(prediction) > 300 else prediction)\n",
    "        \n",
    "        if isinstance(instance, dict):\n",
    "            # Extract all criteria scores\n",
    "            score_keys = [k for k in instance.keys() if k.endswith('_selected_option')]\n",
    "            \n",
    "            if score_keys:\n",
    "                print(f\"\\nðŸ“Š Scores Across All Criteria:\")\n",
    "                print(\"-\" * 80)\n",
    "                \n",
    "                all_scores = []\n",
    "                for score_key in score_keys:\n",
    "                    base_name = score_key.replace('_selected_option', '')\n",
    "                    score = instance.get(base_name, None)\n",
    "                    selected_option = instance.get(f'{base_name}_selected_option', 'N/A')\n",
    "                    assessment = instance.get(f'{base_name}_assessment', '')\n",
    "                    \n",
    "                    if score is not None:\n",
    "                        criterion_name = base_name.replace('_', ' ').title()\n",
    "                        print(f\"\\nðŸ” {criterion_name}:\")\n",
    "                        print(f\"   Score: {score:.2f} ({selected_option})\")\n",
    "                        \n",
    "                        if assessment:\n",
    "                            # Show first 200 chars of reasoning\n",
    "                            if len(assessment) > 200:\n",
    "                                print(f\"   Reasoning: {assessment[:200]}...\")\n",
    "                            else:\n",
    "                                print(f\"   Reasoning: {assessment}\")\n",
    "                        \n",
    "                        all_scores.append(score)\n",
    "                \n",
    "                # Show average\n",
    "                if all_scores:\n",
    "                    avg_score = sum(all_scores) / len(all_scores)\n",
    "                    print(f\"\\n{'='*80}\")\n",
    "                    print(f\"ðŸ“ˆ Overall Average Score: {avg_score:.2f} / 1.0\")\n",
    "                    print(f\"   Quality Level: \", end=\"\")\n",
    "                    if avg_score >= 0.8:\n",
    "                        print(\"âœ… Excellent\")\n",
    "                    elif avg_score >= 0.6:\n",
    "                        print(\"âœ… Good\")\n",
    "                    elif avg_score >= 0.4:\n",
    "                        print(\"âš ï¸  Needs Improvement\")\n",
    "                    else:\n",
    "                        print(\"âŒ Poor\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… DETAILED RESULTS DISPLAYED\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Comparison and Interpretation\n",
    "\n",
    "**What we're doing:** Comparing results between good (reference) and bad (other) close notes to understand the differences.\n",
    "\n",
    "**Key questions:**\n",
    "- Do good close notes score higher? (Expected: Yes)\n",
    "- Which criteria show the biggest differences?\n",
    "- What can we learn about what makes a close note good?\n",
    "\n",
    "**Interpretation guide:**\n",
    "- **Large difference (0.3+)** = This criterion strongly distinguishes good from bad\n",
    "- **Small difference (<0.2)** = This criterion doesn't distinguish well\n",
    "- **Consistent pattern** = Good notes score higher across all criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results between reference and other datasets\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARISON: GOOD vs BAD CLOSE NOTES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    reference_scores = results_df[results_df['dataset_type'] == 'reference']\n",
    "    other_scores = results_df[results_df['dataset_type'] == 'other']\n",
    "    \n",
    "    if len(reference_scores) > 0 and len(other_scores) > 0:\n",
    "        print(f\"\\nðŸ“Š Overall Average Scores:\")\n",
    "        print(f\"   Good (Reference): {reference_scores['average_score'].mean():.2f}\")\n",
    "        print(f\"   Bad/Regular (Other): {other_scores['average_score'].mean():.2f}\")\n",
    "        print(f\"   Difference: {reference_scores['average_score'].mean() - other_scores['average_score'].mean():.2f}\")\n",
    "        \n",
    "        # Compare by criterion\n",
    "        criterion_cols = [col for col in results_df.columns if col.endswith('_score')]\n",
    "        if criterion_cols:\n",
    "            print(f\"\\nðŸ“‹ Criterion-by-Criterion Comparison:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for criterion_col in criterion_cols:\n",
    "                criterion_name = criterion_col.replace('_score', '').replace('_', ' ').title()\n",
    "                ref_mean = reference_scores[criterion_col].mean()\n",
    "                other_mean = other_scores[criterion_col].mean()\n",
    "                difference = ref_mean - other_mean\n",
    "                \n",
    "                print(f\"\\n{criterion_name}:\")\n",
    "                print(f\"   Good: {ref_mean:.2f}\")\n",
    "                print(f\"   Bad/Regular: {other_mean:.2f}\")\n",
    "                print(f\"   Difference: {difference:.2f}\", end=\"\")\n",
    "                \n",
    "                if difference > 0.3:\n",
    "                    print(\" âœ… Large difference - strongly distinguishes quality\")\n",
    "                elif difference > 0.15:\n",
    "                    print(\" âœ… Moderate difference - distinguishes quality\")\n",
    "                else:\n",
    "                    print(\" âš ï¸  Small difference - less distinguishing\")\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"ðŸ“ˆ SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nâœ… Key Findings:\")\n",
    "        print(f\"   - Good close notes score {'higher' if reference_scores['average_score'].mean() > other_scores['average_score'].mean() else 'lower'} overall\")\n",
    "        print(f\"   - Average difference: {abs(reference_scores['average_score'].mean() - other_scores['average_score'].mean()):.2f} points\")\n",
    "        \n",
    "        if reference_scores['average_score'].mean() > other_scores['average_score'].mean():\n",
    "            print(f\"\\nðŸ’¡ Interpretation:\")\n",
    "            print(f\"   LLM-as-a-Judge successfully distinguishes between good and bad close notes.\")\n",
    "            print(f\"   This evaluation method can be used to assess close note quality.\")\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸  Note:\")\n",
    "            print(f\"   Unexpected results - good notes should score higher.\")\n",
    "            print(f\"   This may indicate issues with the evaluation or sample selection.\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion and Next Steps\n",
    "\n",
    "**What we learned:**\n",
    "- âœ… LLM-as-a-Judge provides structured, explainable evaluation\n",
    "- âœ… Good close notes score higher across multiple criteria\n",
    "- âœ… Specific criteria (like Specificity and No Generic Statements) strongly distinguish quality\n",
    "- âœ… This method can be used to evaluate AI-generated close notes\n",
    "\n",
    "**Next steps:**\n",
    "- **Improvement:** Use evaluation results to guide LLM prompt engineering\n",
    "- **Scaling:** Evaluate more close notes to build a comprehensive quality assessment\n",
    "\n",
    "**How to use this:**\n",
    "- Generate close notes using an LLM (Notebook 06)\n",
    "- Evaluate them using this same method\n",
    "- Compare scores to identify areas for improvement\n",
    "- Iterate on prompts to improve quality\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
