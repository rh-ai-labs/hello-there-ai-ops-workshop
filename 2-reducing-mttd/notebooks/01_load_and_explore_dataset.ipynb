{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: Load and Explore Dataset\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. **Load** the synthetic IT call center tickets dataset from Hugging Face\n",
    "2. **Explore** dataset structure, data quality, and completeness\n",
    "3. **Analyze** incident characteristics (categories, types, contact channels)\n",
    "4. **Examine** content quality and ground truth availability\n",
    "5. **Visualize** key metrics and distributions\n",
    "6. **Prepare** data for enrichment experiments\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Dataset Overview\n",
    "\n",
    "**Source:** [Hugging Face - KameronB/synthetic-it-callcenter-tickets](https://huggingface.co/datasets/KameronB/synthetic-it-callcenter-tickets)\n",
    "\n",
    "This dataset contains synthetic IT support tickets simulating real-world incidents and requests, ideal for demonstrating LLM-based incident enrichment tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(str(Path(\"../src\").resolve()))\n",
    "\n",
    "from utils import load_incident_dataset, calculate_basic_stats, prepare_incident_for_enrichment\n",
    "\n",
    "# Set up plotting style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn')\n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "We'll load the synthetic IT call center tickets dataset from Hugging Face. For faster experimentation, we can sample a subset of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset - adjust sample_size as needed\n",
    "# Set sample_size=None to load full dataset, or specify a number (e.g., 200)\n",
    "SAMPLE_SIZE = 200  # Use smaller sample for faster experiments\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "df = load_incident_dataset(sample_size=SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Dataset Overview\n",
    "\n",
    "Let's examine the first few rows and check data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Statistics\n",
    "\n",
    "Calculate and display basic statistics about the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic statistics\n",
    "stats = calculate_basic_stats(df)\n",
    "\n",
    "print(\"üìä Dataset Statistics:\")\n",
    "print(f\"Total incidents: {stats['total_incidents']}\")\n",
    "print(f\"Incidents: {stats['incidents']}\")\n",
    "print(f\"Requests: {stats['requests']}\")\n",
    "if stats['avg_resolution_time']:\n",
    "    print(f\"Average resolution time: {stats['avg_resolution_time']:.2f} minutes\")\n",
    "print(f\"\\nCategories distribution:\")\n",
    "for category, count in stats['categories'].items():\n",
    "    print(f\"  - {category}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Incident Characteristics Analysis\n",
    "\n",
    "Create comprehensive visualizations to understand the dataset distribution across multiple dimensions:\n",
    "- **Categories & Subcategories**: Distribution of incident types\n",
    "- **Contact Channels**: How users report incidents\n",
    "- **Incident Types**: Incident vs Request breakdown\n",
    "- **Resolution Metrics**: Time to resolution and reassignments\n",
    "- **Content Quality**: Text length and ground truth quality scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization dashboard\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# 1. Category Distribution (Pie Chart)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "if 'category' in df.columns:\n",
    "    category_counts = df['category'].value_counts()\n",
    "    colors = sns.color_palette(\"husl\", len(category_counts))\n",
    "    wedges, texts, autotexts = ax1.pie(\n",
    "        category_counts.values, \n",
    "        labels=category_counts.index, \n",
    "        autopct='%1.1f%%',\n",
    "        colors=colors,\n",
    "        startangle=90\n",
    "    )\n",
    "    ax1.set_title('Incident Categories Distribution', fontsize=12, fontweight='bold')\n",
    "    # Improve text readability\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "\n",
    "# 2. Category Distribution (Bar Chart with counts)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "if 'category' in df.columns:\n",
    "    category_counts = df['category'].value_counts()\n",
    "    bars = ax2.bar(range(len(category_counts)), category_counts.values, color=colors)\n",
    "    ax2.set_xticks(range(len(category_counts)))\n",
    "    ax2.set_xticklabels(category_counts.index, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Count', fontsize=10)\n",
    "    ax2.set_title('Categories by Count', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Top 10 Subcategories\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "if 'subcategory' in df.columns:\n",
    "    subcat_counts = df['subcategory'].value_counts().head(10)\n",
    "    ax3.barh(range(len(subcat_counts)), subcat_counts.values, \n",
    "             color=sns.color_palette(\"viridis\", len(subcat_counts)))\n",
    "    ax3.set_yticks(range(len(subcat_counts)))\n",
    "    ax3.set_yticklabels(subcat_counts.index)\n",
    "    ax3.set_xlabel('Count', fontsize=10)\n",
    "    ax3.set_title('Top 10 Subcategories', fontsize=12, fontweight='bold')\n",
    "    ax3.grid(axis='x', alpha=0.3)\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(subcat_counts.values):\n",
    "        ax3.text(v + 0.5, i, str(v), va='center', fontsize=9)\n",
    "\n",
    "# 4. Contact Type Distribution\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "if 'contact_type' in df.columns:\n",
    "    contact_counts = df['contact_type'].value_counts()\n",
    "    bars = ax4.bar(contact_counts.index, contact_counts.values, \n",
    "                   color=sns.color_palette(\"muted\", len(contact_counts)))\n",
    "    ax4.set_ylabel('Count', fontsize=10)\n",
    "    ax4.set_title('Contact Channel Distribution', fontsize=12, fontweight='bold')\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 5. Type Distribution (Incident vs Request)\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "if 'type' in df.columns:\n",
    "    type_counts = df['type'].value_counts()\n",
    "    colors_type = sns.color_palette(\"Set2\", len(type_counts))\n",
    "    wedges, texts, autotexts = ax5.pie(\n",
    "        type_counts.values,\n",
    "        labels=type_counts.index,\n",
    "        autopct='%1.1f%%',\n",
    "        colors=colors_type,\n",
    "        startangle=90\n",
    "    )\n",
    "    ax5.set_title('Incident vs Request Distribution', fontsize=12, fontweight='bold')\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "\n",
    "# 6. Resolution Time Distribution\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "if 'resolution_time' in df.columns and df['resolution_time'].notna().any():\n",
    "    resolution_times = df['resolution_time'].dropna()\n",
    "    ax6.hist(resolution_times, bins=40, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    ax6.set_xlabel('Resolution Time (minutes)', fontsize=10)\n",
    "    ax6.set_ylabel('Frequency', fontsize=10)\n",
    "    ax6.set_title('Resolution Time Distribution', fontsize=12, fontweight='bold')\n",
    "    ax6.set_yscale('log')\n",
    "    ax6.grid(axis='y', alpha=0.3)\n",
    "    # Add statistics\n",
    "    ax6.axvline(resolution_times.median(), color='red', linestyle='--', \n",
    "                label=f'Median: {resolution_times.median():.1f} min')\n",
    "    ax6.axvline(resolution_times.mean(), color='orange', linestyle='--', \n",
    "                label=f'Mean: {resolution_times.mean():.1f} min')\n",
    "    ax6.legend(fontsize=8)\n",
    "\n",
    "# 7. Content Length Analysis\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "if 'content' in df.columns:\n",
    "    df['content_length'] = df['content'].astype(str).str.len()\n",
    "    ax7.hist(df['content_length'], bins=30, edgecolor='black', alpha=0.7, color='teal')\n",
    "    ax7.set_xlabel('Content Length (characters)', fontsize=10)\n",
    "    ax7.set_ylabel('Frequency', fontsize=10)\n",
    "    ax7.set_title('Incident Content Length Distribution', fontsize=12, fontweight='bold')\n",
    "    ax7.grid(axis='y', alpha=0.3)\n",
    "    ax7.axvline(df['content_length'].median(), color='red', linestyle='--', \n",
    "                label=f'Median: {df[\"content_length\"].median():.0f} chars')\n",
    "    ax7.legend(fontsize=8)\n",
    "\n",
    "# 8. Ground Truth Quality (Info Score)\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "if 'info_score_close_notes' in df.columns:\n",
    "    info_scores = df['info_score_close_notes'].dropna()\n",
    "    if len(info_scores) > 0:\n",
    "        ax8.hist(info_scores, bins=20, edgecolor='black', alpha=0.7, color='purple')\n",
    "        ax8.set_xlabel('Info Score', fontsize=10)\n",
    "        ax8.set_ylabel('Frequency', fontsize=10)\n",
    "        ax8.set_title('Ground Truth Quality Score\\n(close_notes info_score)', fontsize=12, fontweight='bold')\n",
    "        ax8.grid(axis='y', alpha=0.3)\n",
    "        ax8.axvline(info_scores.mean(), color='red', linestyle='--', \n",
    "                    label=f'Mean: {info_scores.mean():.2f}')\n",
    "        ax8.legend(fontsize=8)\n",
    "\n",
    "# 9. Reassignment Analysis\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "if 'reassigned_count' in df.columns:\n",
    "    reassign_counts = df['reassigned_count'].value_counts().sort_index()\n",
    "    bars = ax9.bar(reassign_counts.index, reassign_counts.values, \n",
    "                   color=sns.color_palette(\"rocket\", len(reassign_counts)))\n",
    "    ax9.set_xlabel('Number of Reassignments', fontsize=10)\n",
    "    ax9.set_ylabel('Count', fontsize=10)\n",
    "    ax9.set_title('Incident Reassignment Frequency', fontsize=12, fontweight='bold')\n",
    "    ax9.grid(axis='y', alpha=0.3)\n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            ax9.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('Dataset Overview Dashboard', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "if 'category' in df.columns:\n",
    "    print(f\"\\nüìä Categories: {df['category'].nunique()} unique categories\")\n",
    "    print(f\"   Most common: {df['category'].value_counts().index[0]} ({df['category'].value_counts().iloc[0]} incidents)\")\n",
    "if 'subcategory' in df.columns:\n",
    "    print(f\"\\nüìã Subcategories: {df['subcategory'].nunique()} unique subcategories\")\n",
    "    print(f\"   Most common: {df['subcategory'].value_counts().index[0]} ({df['subcategory'].value_counts().iloc[0]} incidents)\")\n",
    "if 'contact_type' in df.columns:\n",
    "    print(f\"\\nüìû Contact Channels: {df['contact_type'].nunique()} channels\")\n",
    "    print(f\"   Most used: {df['contact_type'].value_counts().index[0]} ({df['contact_type'].value_counts().iloc[0]} incidents)\")\n",
    "if 'resolution_time' in df.columns and df['resolution_time'].notna().any():\n",
    "    rt = df['resolution_time'].dropna()\n",
    "    print(f\"\\n‚è±Ô∏è  Resolution Time:\")\n",
    "    print(f\"   Mean: {rt.mean():.1f} minutes ({rt.mean()/60:.1f} hours)\")\n",
    "    print(f\"   Median: {rt.median():.1f} minutes ({rt.median()/60:.1f} hours)\")\n",
    "    print(f\"   Range: {rt.min():.1f} - {rt.max():.1f} minutes\")\n",
    "if 'reassigned_count' in df.columns:\n",
    "    print(f\"\\nüîÑ Reassignments:\")\n",
    "    print(f\"   Mean: {df['reassigned_count'].mean():.2f} reassignments per incident\")\n",
    "    print(f\"   Max: {df['reassigned_count'].max()} reassignments\")\n",
    "    no_reassign = (df['reassigned_count'] == 0).sum()\n",
    "    print(f\"   {no_reassign} incidents ({no_reassign/len(df)*100:.1f}%) had no reassignments\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examine Sample Incidents\n",
    "\n",
    "Let's look at a few sample incidents to understand the structure and content quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample incident in detail\n",
    "sample_incident = df.sample(1).iloc[0]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE INCIDENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìã Number: {sample_incident.get('number', 'N/A')}\")\n",
    "print(f\"üìÖ Date: {sample_incident.get('date', 'N/A')}\")\n",
    "print(f\"üìû Contact Type: {sample_incident.get('contact_type', 'N/A')}\")\n",
    "print(f\"üè∑Ô∏è  Category: {sample_incident.get('category', 'N/A')}\")\n",
    "print(f\"üè∑Ô∏è  Subcategory: {sample_incident.get('subcategory', 'N/A')}\")\n",
    "print(f\"üë§ Customer: {sample_incident.get('customer', 'N/A')}\")\n",
    "print(f\"\\nüìù Short Description:\")\n",
    "print(f\"   {sample_incident.get('short_description', 'N/A')}\")\n",
    "print(f\"\\nüìÑ Content:\")\n",
    "print(f\"   {sample_incident.get('content', 'N/A')[:500]}...\")\n",
    "if 'close_notes' in sample_incident and pd.notna(sample_incident.get('close_notes')):\n",
    "    print(f\"\\n‚úÖ Close Notes (Ground Truth):\")\n",
    "    print(f\"   {sample_incident.get('close_notes', 'N/A')[:500]}...\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Content Quality Analysis\n",
    "\n",
    "Analyze the text content characteristics to understand:\n",
    "- **Content length**: Input text size for LLM processing\n",
    "- **Ground truth availability**: Quality and completeness of close_notes\n",
    "- **Content vs Resolution**: Compare input content with resolution notes length\n",
    "- **Information quality scores**: Assess the informational value of ground truth data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive content analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Content Quality Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Calculate content metrics\n",
    "if 'content' in df.columns:\n",
    "    df['content_length'] = df['content'].astype(str).str.len()\n",
    "    df['content_word_count'] = df['content'].astype(str).str.split().str.len()\n",
    "    \n",
    "    # 1. Content Length Distribution\n",
    "    axes[0, 0].hist(df['content_length'], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[0, 0].axvline(df['content_length'].median(), color='red', linestyle='--', \n",
    "                       label=f'Median: {df[\"content_length\"].median():.0f} chars')\n",
    "    axes[0, 0].set_xlabel('Content Length (characters)', fontsize=10)\n",
    "    axes[0, 0].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[0, 0].set_title('Input Content Length Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    axes[0, 0].legend(fontsize=9)\n",
    "    \n",
    "    # 2. Word Count Distribution\n",
    "    axes[0, 1].hist(df['content_word_count'], bins=30, edgecolor='black', alpha=0.7, color='teal')\n",
    "    axes[0, 1].axvline(df['content_word_count'].median(), color='red', linestyle='--', \n",
    "                       label=f'Median: {df[\"content_word_count\"].median():.0f} words')\n",
    "    axes[0, 1].set_xlabel('Word Count', fontsize=10)\n",
    "    axes[0, 1].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[0, 1].set_title('Content Word Count Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    axes[0, 1].legend(fontsize=9)\n",
    "    \n",
    "    # Check if close_notes exist (ground truth)\n",
    "    if 'close_notes' in df.columns:\n",
    "        has_close_notes = df['close_notes'].notna()\n",
    "        df_with_gt = df[has_close_notes].copy()\n",
    "        \n",
    "        if len(df_with_gt) > 0:\n",
    "            df_with_gt['close_notes_length'] = df_with_gt['close_notes'].astype(str).str.len()\n",
    "            df_with_gt['close_notes_word_count'] = df_with_gt['close_notes'].astype(str).str.split().str.len()\n",
    "            \n",
    "            # 3. Content vs Close Notes Length Comparison\n",
    "            axes[1, 0].scatter(df_with_gt['content_length'], df_with_gt['close_notes_length'], \n",
    "                              alpha=0.6, color='purple', s=50)\n",
    "            axes[1, 0].plot([0, max(df_with_gt['content_length'].max(), df_with_gt['close_notes_length'].max())],\n",
    "                            [0, max(df_with_gt['content_length'].max(), df_with_gt['close_notes_length'].max())],\n",
    "                            'r--', alpha=0.5, label='y=x line')\n",
    "            axes[1, 0].set_xlabel('Content Length (chars)', fontsize=10)\n",
    "            axes[1, 0].set_ylabel('Close Notes Length (chars)', fontsize=10)\n",
    "            axes[1, 0].set_title('Content vs Resolution Notes Length', fontsize=12, fontweight='bold')\n",
    "            axes[1, 0].grid(alpha=0.3)\n",
    "            axes[1, 0].legend(fontsize=9)\n",
    "            \n",
    "            # 4. Info Score Distribution\n",
    "            if 'info_score_close_notes' in df_with_gt.columns:\n",
    "                info_scores = df_with_gt['info_score_close_notes'].dropna()\n",
    "                if len(info_scores) > 0:\n",
    "                    axes[1, 1].hist(info_scores, bins=20, edgecolor='black', alpha=0.7, color='orange')\n",
    "                    axes[1, 1].axvline(info_scores.mean(), color='red', linestyle='--', \n",
    "                                      label=f'Mean: {info_scores.mean():.2f}')\n",
    "                    axes[1, 1].axvline(info_scores.median(), color='blue', linestyle='--', \n",
    "                                      label=f'Median: {info_scores.median():.2f}')\n",
    "                    axes[1, 1].set_xlabel('Info Score', fontsize=10)\n",
    "                    axes[1, 1].set_ylabel('Frequency', fontsize=10)\n",
    "                    axes[1, 1].set_title('Ground Truth Quality Score Distribution', fontsize=12, fontweight='bold')\n",
    "                    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "                    axes[1, 1].legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONTENT QUALITY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "if 'content' in df.columns:\n",
    "    print(f\"\\nüìù Input Content (for LLM enrichment):\")\n",
    "    print(f\"   Average length: {df['content_length'].mean():.0f} characters\")\n",
    "    print(f\"   Median length: {df['content_length'].median():.0f} characters\")\n",
    "    print(f\"   Average word count: {df['content_word_count'].mean():.0f} words\")\n",
    "    print(f\"   Range: {df['content_length'].min()} - {df['content_length'].max()} characters\")\n",
    "    \n",
    "    if 'close_notes' in df.columns:\n",
    "        has_close_notes = df['close_notes'].notna().sum()\n",
    "        print(f\"\\n‚úÖ Ground Truth (close_notes) Availability:\")\n",
    "        print(f\"   Incidents with close_notes: {has_close_notes} ({has_close_notes/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        if has_close_notes > 0:\n",
    "            df_with_gt = df[df['close_notes'].notna()].copy()\n",
    "            df_with_gt['close_notes_length'] = df_with_gt['close_notes'].astype(str).str.len()\n",
    "            df_with_gt['close_notes_word_count'] = df_with_gt['close_notes'].astype(str).str.split().str.len()\n",
    "            \n",
    "            print(f\"\\nüìã Resolution Notes (close_notes) Statistics:\")\n",
    "            print(f\"   Average length: {df_with_gt['close_notes_length'].mean():.0f} characters\")\n",
    "            print(f\"   Median length: {df_with_gt['close_notes_length'].median():.0f} characters\")\n",
    "            print(f\"   Average word count: {df_with_gt['close_notes_word_count'].mean():.0f} words\")\n",
    "            print(f\"   Range: {df_with_gt['close_notes_length'].min()} - {df_with_gt['close_notes_length'].max()} characters\")\n",
    "            \n",
    "            # Expansion ratio\n",
    "            expansion_ratio = df_with_gt['close_notes_length'].mean() / df_with_gt['content_length'].mean()\n",
    "            print(f\"\\nüìà Content Expansion:\")\n",
    "            print(f\"   Resolution notes are {expansion_ratio:.2f}x longer than input content on average\")\n",
    "            \n",
    "            if 'info_score_close_notes' in df_with_gt.columns:\n",
    "                info_scores = df_with_gt['info_score_close_notes'].dropna()\n",
    "                if len(info_scores) > 0:\n",
    "                    print(f\"\\n‚≠ê Information Quality Score:\")\n",
    "                    print(f\"   Mean: {info_scores.mean():.2f}\")\n",
    "                    print(f\"   Median: {info_scores.median():.2f}\")\n",
    "                    print(f\"   Range: {info_scores.min():.2f} - {info_scores.max():.2f}\")\n",
    "                    high_quality = (info_scores >= 0.8).sum()\n",
    "                    print(f\"   High quality (‚â•0.8): {high_quality} ({high_quality/len(info_scores)*100:.1f}%)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Data for Experiments\n",
    "\n",
    "Select and prepare incidents for enrichment experiments. We'll focus on incidents that have ground truth (close_notes) for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter incidents that have close_notes (ground truth) for evaluation\n",
    "if 'close_notes' in df.columns:\n",
    "    df_with_ground_truth = df[df['close_notes'].notna()].copy()\n",
    "    print(f\"Incidents with ground truth: {len(df_with_ground_truth)}\")\n",
    "    print(f\"Incidents without ground truth: {len(df) - len(df_with_ground_truth)}\")\n",
    "    \n",
    "    # For experiments, we'll use incidents with ground truth\n",
    "    df_experiments = df_with_ground_truth.copy()\n",
    "else:\n",
    "    print(\"No close_notes column found - will use all incidents\")\n",
    "    df_experiments = df.copy()\n",
    "\n",
    "print(f\"\\nTotal incidents prepared for experiments: {len(df_experiments)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Prepared Dataset\n",
    "\n",
    "Save the prepared dataset for use in subsequent notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "data_dir = Path(\"../data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save the prepared dataset\n",
    "output_path = data_dir / \"incidents_prepared.csv\"\n",
    "df_experiments.to_csv(output_path, index=False)\n",
    "print(f\"‚úÖ Saved prepared dataset to: {output_path}\")\n",
    "print(f\"   Total records: {len(df_experiments)}\")\n",
    "\n",
    "# Also save a sample of incidents for quick testing\n",
    "df_sample = df_experiments.sample(min(10, len(df_experiments)), random_state=42)\n",
    "sample_path = data_dir / \"incidents_sample.csv\"\n",
    "df_sample.to_csv(sample_path, index=False)\n",
    "print(f\"‚úÖ Saved sample dataset to: {sample_path}\")\n",
    "print(f\"   Sample records: {len(df_sample)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "This notebook has:\n",
    "- ‚úÖ Loaded the synthetic IT call center tickets dataset\n",
    "- ‚úÖ Explored dataset structure and characteristics\n",
    "- ‚úÖ Analyzed content and ground truth availability\n",
    "- ‚úÖ Prepared data for enrichment experiments\n",
    "- ‚úÖ Saved prepared datasets for next steps\n",
    "\n",
    "**Next Steps:**\n",
    "- Move to `02_llm_eval_with_trustyai.ipynb` to test different prompts and LLM models\n",
    "- Use the prepared dataset to generate enriched incident reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary\n",
    "print(\"=\"*80)\n",
    "print(\"NOTEBOOK SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä Dataset loaded: {len(df)} total records\")\n",
    "print(f\"üìù Prepared for experiments: {len(df_experiments)} records\")\n",
    "print(f\"üíæ Saved to: {output_path}\")\n",
    "print(\"\\n‚úÖ Ready for prompt experiments in the next notebook!\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
