{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Define Quality and Separate Datasets\n",
    "\n",
    "## üéØ What is This Notebook About?\n",
    "\n",
    "This notebook helps you understand what makes a \"good\" close note and separates your incident dataset into two groups:\n",
    "- **Reference Dataset** (high-quality close notes) - These will serve as examples of what good close notes look like\n",
    "- **Other Incidents Dataset** - All remaining incidents (for comparison)\n",
    "\n",
    "**Why this matters:**\n",
    "- We need clear examples of good close notes to compare against\n",
    "- These reference examples will be used to evaluate other close notes (existing or AI-generated)\n",
    "- By separating them, we can see the difference between high-quality and standard close notes\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Key Concepts Explained\n",
    "\n",
    "### What Makes a \"Good\" Close Note?\n",
    "\n",
    "A good close note should:\n",
    "1. **Be informative** - Contains specific details about the problem and solution\n",
    "2. **Be complete** - Covers what happened, what was done, and the outcome\n",
    "3. **Avoid generic phrases** - Doesn't just say \"Issue resolved\" without explanation\n",
    "4. **Be professional** - Clear, well-structured, and easy to understand\n",
    "\n",
    "**Example of a GOOD close note:**\n",
    "> \"Investigated the reported issue with Workday crashing when saving files. Cleared browser cache and cookies, updated browser to latest version. Verified user can now save files successfully. Issue resolved and confirmed with user.\"\n",
    "\n",
    "**Example of a BAD/Generic close note:**\n",
    "> \"Issue resolved.\"\n",
    "\n",
    "**Why the difference matters:**\n",
    "- Good close notes help others understand what happened and how it was fixed\n",
    "- Generic close notes don't provide useful information\n",
    "- We'll use good examples as \"reference\" to evaluate other close notes\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "This notebook will:\n",
    "1. **Define quality criteria** - Explain what makes a close note \"good\"\n",
    "2. **Examine quality scores** - Look at how close notes are scored\n",
    "3. **Filter high-quality examples** - Identify close notes that meet our criteria\n",
    "4. **Separate into two datasets:**\n",
    "   - **Reference Dataset** - High-quality close notes (ground truth)\n",
    "   - **Other Incidents Dataset** - Remaining incidents (for comparison)\n",
    "5. **Save datasets** - Save both for use in later notebooks\n",
    "\n",
    "---\n",
    "\n",
    "## üìã How We'll Separate the Data\n",
    "\n",
    "**Separation Strategy:**\n",
    "- **Reference Dataset** (`reference_close_notes.csv`): \n",
    "  - High-quality close notes that meet all criteria\n",
    "  - Will be used as examples/references for evaluation\n",
    "  \n",
    "- **Other Incidents Dataset**:\n",
    "  - All incidents that don't meet the \"high-quality\" criteria\n",
    "  - Will be used for comparison in later notebooks\n",
    "\n",
    "**Selection Criteria for Reference Dataset:**\n",
    "- High information score (`info_score_close_notes` ‚â• 0.8)\n",
    "- Low generic content score (`info_score_poor_close_notes` ‚â§ 0.1)\n",
    "- Complete and informative text (not just \"Issue resolved\")\n",
    "- Minimum length (ensures sufficient detail)\n",
    "- Diverse examples across different categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "**What we're doing:** Loading the tools we need to work with data and create visualizations.\n",
    "\n",
    "**Why:** Just like a carpenter needs a hammer and saw, we need specific tools (libraries) to work with data.\n",
    "\n",
    "**What to expect:** You'll see a success message when everything is loaded correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# Think of these like tools in a toolbox - each one does a specific job\n",
    "\n",
    "import pandas as pd  # For working with data tables (like Excel spreadsheets)\n",
    "import numpy as np   # For doing math calculations\n",
    "import matplotlib.pyplot as plt  # For creating charts and graphs\n",
    "import seaborn as sns  # For making prettier charts\n",
    "from pathlib import Path  # For handling file paths\n",
    "import sys\n",
    "import re  # For text pattern matching (finding generic phrases)\n",
    "\n",
    "# Add src directory to path so we can use our helper functions\n",
    "sys.path.append(str(Path(\"../src\").resolve()))\n",
    "\n",
    "# Import our custom helper functions\n",
    "from utils import load_incident_dataset, calculate_basic_stats\n",
    "\n",
    "# Set up plotting style (makes our charts look nicer)\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn')\n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# This makes charts appear in the notebook\n",
    "%matplotlib inline  \n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"üìö Ready to start analyzing close notes quality!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Prepared Dataset\n",
    "\n",
    "**What we're doing:** Loading the incident dataset from Notebook 01.\n",
    "\n",
    "**Why:** We need all incidents with their close notes so we can identify which ones are high-quality and separate them.\n",
    "\n",
    "**What to expect:** The dataset contains incidents with various quality levels of close notes. We'll filter to find the best ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared dataset from notebook 01\n",
    "# This is the data we prepared in the previous notebook\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "prepared_path = data_dir / \"incidents_prepared.csv\"\n",
    "\n",
    "if prepared_path.exists():\n",
    "    # Load the prepared dataset (faster - we already processed it)\n",
    "    df = pd.read_csv(prepared_path)\n",
    "    print(f\"‚úÖ Loaded prepared dataset: {len(df)} records\")\n",
    "else:\n",
    "    # If the prepared file doesn't exist, load from scratch\n",
    "    print(\"‚ö†Ô∏è Prepared dataset not found. Loading from Hugging Face...\")\n",
    "    df = load_incident_dataset(sample_size=200, random_state=42)\n",
    "    # Filter for records with close_notes (we need these!)\n",
    "    if 'close_notes' in df.columns:\n",
    "        df = df[df['close_notes'].notna()].copy()\n",
    "    print(f\"‚úÖ Loaded dataset: {len(df)} records with close_notes\")\n",
    "\n",
    "# Let's see what we're working with\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"   Total incidents: {len(df)}\")\n",
    "print(f\"   Columns (pieces of information): {df.shape[1]}\")\n",
    "print(f\"\\nüìã Key columns we'll use:\")\n",
    "key_columns = ['close_notes', 'info_score_close_notes', 'info_score_poor_close_notes', 'category']\n",
    "for col in key_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"   ‚úÖ {col}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {col} (missing!)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understand Quality Scores\n",
    "\n",
    "**What we're doing:** Looking at how close notes are scored for quality.\n",
    "\n",
    "**What are quality scores?**\n",
    "- **`info_score_close_notes`**: Measures how informative a close note is (0.0 to 1.0)\n",
    "  - **High score (‚â•0.8)**: Contains detailed, useful information\n",
    "  - **Low score (<0.8)**: Lacks detail or information\n",
    "  \n",
    "- **`info_score_poor_close_notes`**: Measures how \"generic\" a close note is (0.0 to 1.0)\n",
    "  - **Low score (‚â§0.1)**: Not generic, contains specific information\n",
    "  - **High score (>0.1)**: Generic phrases like \"Issue resolved\"\n",
    "\n",
    "**Why this matters:** We want close notes that are informative (high score) and not generic (low poor score).\n",
    "\n",
    "**What to look for:** \n",
    "- How many close notes have high information scores?\n",
    "- How many have low generic scores?\n",
    "- This tells us how many \"good\" examples we can use as references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check quality score columns\n",
    "# This shows us how the close notes are scored for quality\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä QUALITY SCORES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüí° Understanding the scores:\")\n",
    "print(\"   ‚Ä¢ Higher info_score_close_notes = More informative (better!)\")\n",
    "print(\"   ‚Ä¢ Lower info_score_poor_close_notes = Less generic (better!)\")\n",
    "print(\"   ‚Ä¢ We want: High info score (‚â•0.8) AND Low poor score (‚â§0.1)\")\n",
    "\n",
    "if 'info_score_close_notes' in df.columns:\n",
    "    high_quality_count = (df['info_score_close_notes'] >= 0.8).sum()\n",
    "    high_quality_pct = (high_quality_count / len(df)) * 100\n",
    "    print(f\"\\nüìä info_score_close_notes (How informative?):\")\n",
    "    print(f\"   Mean (average): {df['info_score_close_notes'].mean():.3f}\")\n",
    "    print(f\"   Median (middle value): {df['info_score_close_notes'].median():.3f}\")\n",
    "    print(f\"   Range: {df['info_score_close_notes'].min():.3f} to {df['info_score_close_notes'].max():.3f}\")\n",
    "    print(f\"   ‚úÖ High quality (‚â•0.8): {high_quality_count} incidents ({high_quality_pct:.1f}%)\")\n",
    "\n",
    "if 'info_score_poor_close_notes' in df.columns:\n",
    "    low_generic_count = (df['info_score_poor_close_notes'] <= 0.1).sum()\n",
    "    low_generic_pct = (low_generic_count / len(df)) * 100\n",
    "    print(f\"\\nüìä info_score_poor_close_notes (How generic?):\")\n",
    "    print(f\"   Mean (average): {df['info_score_poor_close_notes'].mean():.3f}\")\n",
    "    print(f\"   Median (middle value): {df['info_score_poor_close_notes'].median():.3f}\")\n",
    "    print(f\"   ‚úÖ Low generic (‚â§0.1): {low_generic_count} incidents ({low_generic_pct:.1f}%)\")\n",
    "\n",
    "# Visualize score distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "if 'info_score_close_notes' in df.columns:\n",
    "    axes[0].hist(df['info_score_close_notes'].dropna(), bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[0].axvline(0.8, color='red', linestyle='--', label='Threshold (‚â•0.8)')\n",
    "    axes[0].set_xlabel('Info Score (close_notes)', fontsize=11)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[0].set_title('Close Notes Quality Score Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    axes[0].legend()\n",
    "\n",
    "if 'info_score_poor_close_notes' in df.columns:\n",
    "    axes[1].hist(df['info_score_poor_close_notes'].dropna(), bins=20, edgecolor='black', alpha=0.7, color='coral')\n",
    "    axes[1].axvline(0.1, color='red', linestyle='--', label='Threshold (‚â§0.1)')\n",
    "    axes[1].set_xlabel('Info Score (poor_close_notes)', fontsize=11)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[1].set_title('Poor Close Notes Score Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Quality Criteria and Filter\n",
    "\n",
    "**What we're doing:** Applying filters to identify high-quality close notes that will become our \"Reference Dataset\".\n",
    "\n",
    "**Our quality criteria:**\n",
    "1. **High information score** (`info_score_close_notes` ‚â• 0.8)\n",
    "   - *Why?* Ensures the close note contains useful, detailed information\n",
    "   \n",
    "2. **Low generic score** (`info_score_poor_close_notes` ‚â§ 0.1)\n",
    "   - *Why?* Ensures it's not just generic phrases like \"Issue resolved\"\n",
    "   \n",
    "3. **Not generic phrases** - Excludes notes that only say things like:\n",
    "   - \"No changes noted\"\n",
    "   - \"Issue resolved\"\n",
    "   - \"Resolved per user\"\n",
    "   \n",
    "4. **Minimum length** (‚â•100 characters)\n",
    "   - *Why?* Ensures there's enough detail to be useful\n",
    "\n",
    "**What happens:**\n",
    "- We'll filter the dataset step by step\n",
    "- Records that pass all filters ‚Üí **Reference Dataset** (high-quality examples)\n",
    "- Records that don't pass ‚Üí **Other Incidents Dataset** (for comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generic phrases to exclude\n",
    "# These are phrases that don't provide useful information\n",
    "# Think of them like \"filler words\" - they don't tell us anything useful\n",
    "\n",
    "GENERIC_PHRASES = [\n",
    "    'no changes noted',\n",
    "    'issue resolved',\n",
    "    'resolved',\n",
    "    'closed',\n",
    "    'no further action',\n",
    "    'resolved per user',\n",
    "    'user confirmed resolved'\n",
    "]\n",
    "\n",
    "def is_generic_close_note(text):\n",
    "    \"\"\"\n",
    "    Check if close note contains only generic phrases.\n",
    "    \n",
    "    This function looks for close notes that are too short or only contain\n",
    "    generic phrases like \"Issue resolved\" without any details.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return True  # Missing or not text = consider it generic\n",
    "    \n",
    "    text_lower = text.lower().strip()\n",
    "    \n",
    "    # Check if text is too short (likely generic)\n",
    "    # Good close notes need detail, so very short ones are probably generic\n",
    "    if len(text_lower) < 50:\n",
    "        return True\n",
    "    \n",
    "    # Check if text contains only generic phrases\n",
    "    words = set(text_lower.split())\n",
    "    generic_words = set()\n",
    "    for phrase in GENERIC_PHRASES:\n",
    "        generic_words.update(phrase.split())\n",
    "    \n",
    "    # If most words are generic, likely a generic note\n",
    "    # (e.g., \"Issue resolved\" = only 2 words, both generic)\n",
    "    if len(words) <= 5 and words.issubset(generic_words):\n",
    "        return True\n",
    "    \n",
    "    return False  # Not generic - has useful information!\n",
    "\n",
    "# Apply filters step by step\n",
    "# We'll filter the data multiple times, keeping only the best close notes\n",
    "print(\"üîç Applying quality filters...\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üì¶ Initial records: {len(df)}\")\n",
    "\n",
    "# Filter 1: Must have close_notes\n",
    "# We can't evaluate quality if there's no close note!\n",
    "df_filtered = df[df['close_notes'].notna()].copy()\n",
    "print(f\"‚úÖ Filter 1 - Has close_notes: {len(df_filtered)} records\")\n",
    "\n",
    "# Filter 2: High quality score (informative)\n",
    "# We want close notes that are informative (score ‚â• 0.8)\n",
    "if 'info_score_close_notes' in df_filtered.columns:\n",
    "    before = len(df_filtered)\n",
    "    df_filtered = df_filtered[df_filtered['info_score_close_notes'] >= 0.8].copy()\n",
    "    removed = before - len(df_filtered)\n",
    "    print(f\"‚úÖ Filter 2 - High info score (‚â•0.8): {len(df_filtered)} records (removed {removed})\")\n",
    "\n",
    "# Filter 3: Low poor quality score (not generic)\n",
    "# We want close notes that are NOT generic (score ‚â§ 0.1)\n",
    "if 'info_score_poor_close_notes' in df_filtered.columns:\n",
    "    before = len(df_filtered)\n",
    "    df_filtered = df_filtered[df_filtered['info_score_poor_close_notes'] <= 0.1].copy()\n",
    "    removed = before - len(df_filtered)\n",
    "    print(f\"‚úÖ Filter 3 - Low generic score (‚â§0.1): {len(df_filtered)} records (removed {removed})\")\n",
    "\n",
    "# Filter 4: Exclude generic notes (text-based check)\n",
    "# Double-check: remove any that are just generic phrases\n",
    "df_filtered['is_generic'] = df_filtered['close_notes'].apply(is_generic_close_note)\n",
    "before = len(df_filtered)\n",
    "df_filtered = df_filtered[~df_filtered['is_generic']].copy()\n",
    "removed = before - len(df_filtered)\n",
    "print(f\"‚úÖ Filter 4 - Exclude generic phrases: {len(df_filtered)} records (removed {removed})\")\n",
    "\n",
    "# Filter 5: Minimum text length (ensure informative)\n",
    "# Very short close notes probably don't have enough detail\n",
    "df_filtered['close_notes_length'] = df_filtered['close_notes'].astype(str).str.len()\n",
    "before = len(df_filtered)\n",
    "df_filtered = df_filtered[df_filtered['close_notes_length'] >= 100].copy()\n",
    "removed = before - len(df_filtered)\n",
    "print(f\"‚úÖ Filter 5 - Minimum length (‚â•100 chars): {len(df_filtered)} records (removed {removed})\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüéØ Final filtered dataset: {len(df_filtered)} high-quality records\")\n",
    "reduction_pct = ((len(df) - len(df_filtered))/len(df)*100)\n",
    "print(f\"   üìâ Filtered out: {len(df) - len(df_filtered)} records ({reduction_pct:.1f}%)\")\n",
    "print(f\"   ‚úÖ Kept: {len(df_filtered)} records ({100-reduction_pct:.1f}%)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Diversity of Filtered Dataset\n",
    "\n",
    "**What we're doing:** Checking how diverse our filtered high-quality close notes are across different categories.\n",
    "\n",
    "**Why this matters:**\n",
    "- We want examples from different types of incidents (not just SOFTWARE)\n",
    "- Diverse examples help when evaluating different categories later\n",
    "- This shows us if we need to balance the dataset better\n",
    "\n",
    "**What we'll check:**\n",
    "- **Categories**: How many incidents per category (SOFTWARE, ACCOUNT, etc.)\n",
    "- **Subcategories**: How many per subcategory (ERROR, MALFUNCTION, etc.)\n",
    "- **Contact Types**: How incidents were reported (Email, Phone, Chat, etc.)\n",
    "\n",
    "**What to look for:**\n",
    "- If one category dominates (e.g., 90% SOFTWARE), we may want to balance\n",
    "- Good diversity means we have examples across different incident types\n",
    "- This helps when evaluating close notes for different types of problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze diversity\n",
    "print(\"=\"*80)\n",
    "print(\"DIVERSITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'category' in df_filtered.columns:\n",
    "    print(f\"\\nüìä Categories:\")\n",
    "    category_counts = df_filtered['category'].value_counts()\n",
    "    for cat, count in category_counts.items():\n",
    "        print(f\"   {cat}: {count} ({count/len(df_filtered)*100:.1f}%)\")\n",
    "\n",
    "if 'subcategory' in df_filtered.columns:\n",
    "    print(f\"\\nüìã Subcategories:\")\n",
    "    subcat_counts = df_filtered['subcategory'].value_counts()\n",
    "    print(f\"   Total unique: {df_filtered['subcategory'].nunique()}\")\n",
    "    for subcat, count in subcat_counts.head(10).items():\n",
    "        print(f\"   {subcat}: {count}\")\n",
    "\n",
    "if 'contact_type' in df_filtered.columns:\n",
    "    print(f\"\\nüìû Contact Types:\")\n",
    "    contact_counts = df_filtered['contact_type'].value_counts()\n",
    "    for contact, count in contact_counts.items():\n",
    "        print(f\"   {contact}: {count} ({count/len(df_filtered)*100:.1f}%)\")\n",
    "\n",
    "# Visualize diversity\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "if 'category' in df_filtered.columns:\n",
    "    category_counts = df_filtered['category'].value_counts()\n",
    "    axes[0].bar(range(len(category_counts)), category_counts.values, \n",
    "                color=sns.color_palette(\"husl\", len(category_counts)))\n",
    "    axes[0].set_xticks(range(len(category_counts)))\n",
    "    axes[0].set_xticklabels(category_counts.index, rotation=45, ha='right')\n",
    "    axes[0].set_ylabel('Count', fontsize=10)\n",
    "    axes[0].set_title('Category Distribution (Filtered)', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(category_counts.values):\n",
    "        axes[0].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "if 'subcategory' in df_filtered.columns:\n",
    "    subcat_counts = df_filtered['subcategory'].value_counts().head(10)\n",
    "    axes[1].barh(range(len(subcat_counts)), subcat_counts.values,\n",
    "                color=sns.color_palette(\"viridis\", len(subcat_counts)))\n",
    "    axes[1].set_yticks(range(len(subcat_counts)))\n",
    "    axes[1].set_yticklabels(subcat_counts.index)\n",
    "    axes[1].set_xlabel('Count', fontsize=10)\n",
    "    axes[1].set_title('Top 10 Subcategories (Filtered)', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "if 'contact_type' in df_filtered.columns:\n",
    "    contact_counts = df_filtered['contact_type'].value_counts()\n",
    "    axes[2].bar(contact_counts.index, contact_counts.values,\n",
    "               color=sns.color_palette(\"muted\", len(contact_counts)))\n",
    "    axes[2].set_ylabel('Count', fontsize=10)\n",
    "    axes[2].set_title('Contact Type Distribution (Filtered)', fontsize=12, fontweight='bold')\n",
    "    axes[2].grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(contact_counts.values):\n",
    "        axes[2].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensure Balanced Sampling\n",
    "\n",
    "**What we're doing:** Making sure our Reference Dataset has examples from different categories.\n",
    "\n",
    "**Why this matters:**\n",
    "- We want diverse examples (not just SOFTWARE incidents)\n",
    "- This helps when evaluating different types of incidents later\n",
    "- Balanced sampling ensures we have good examples across categories\n",
    "\n",
    "**How it works:**\n",
    "- We try to get a target number of samples per category (e.g., 20)\n",
    "- If a category has fewer samples, we take what's available\n",
    "- If a category has too few samples (< 5), we skip it\n",
    "\n",
    "**Result:** A balanced Reference Dataset with diverse examples.\n",
    "\n",
    "**Note:** After sampling, the remaining high-quality records (that didn't make it into the balanced sample) will still be part of the \"Other Incidents Dataset\" - they're still good quality, just not selected for the reference set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy: Sample balanced examples if one category dominates\n",
    "TARGET_SAMPLES_PER_CATEGORY = 20  # Adjust based on dataset size\n",
    "MIN_SAMPLES_PER_CATEGORY = 5     # Minimum samples per category\n",
    "\n",
    "print(\"Applying balanced sampling strategy...\")\n",
    "\n",
    "if 'category' in df_filtered.columns:\n",
    "    sampled_records = []\n",
    "    \n",
    "    for category in df_filtered['category'].unique():\n",
    "        category_df = df_filtered[df_filtered['category'] == category].copy()\n",
    "        \n",
    "        if len(category_df) >= TARGET_SAMPLES_PER_CATEGORY:\n",
    "            # Sample TARGET_SAMPLES_PER_CATEGORY records\n",
    "            sampled = category_df.sample(\n",
    "                min(TARGET_SAMPLES_PER_CATEGORY, len(category_df)),\n",
    "                random_state=42\n",
    "            )\n",
    "        elif len(category_df) >= MIN_SAMPLES_PER_CATEGORY:\n",
    "            # Take all available if less than target but above minimum\n",
    "            sampled = category_df\n",
    "        else:\n",
    "            # Skip categories with too few samples\n",
    "            print(f\"   ‚ö†Ô∏è Skipping {category}: only {len(category_df)} samples\")\n",
    "            continue\n",
    "        \n",
    "        sampled_records.append(sampled)\n",
    "        print(f\"   ‚úÖ {category}: {len(sampled)} samples\")\n",
    "    \n",
    "    df_ground_truth = pd.concat(sampled_records, ignore_index=True)\n",
    "    print(f\"\\n‚úÖ Balanced ground truth dataset: {len(df_ground_truth)} records\")\n",
    "else:\n",
    "    # If no category column, use all filtered records\n",
    "    df_ground_truth = df_filtered.copy()\n",
    "    print(f\"\\n‚úÖ Using all filtered records: {len(df_ground_truth)} records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Reference Dataset Structure\n",
    "\n",
    "**What we're doing:** Preparing the Reference Dataset with the columns we need.\n",
    "\n",
    "**Structure we need:**\n",
    "- `number` - Incident identifier\n",
    "- `content` - Original incident description\n",
    "- `close_notes_ref` - High-quality close note (reference example)\n",
    "- Metadata: category, subcategory, contact_type, info_score\n",
    "\n",
    "**Why these columns:**\n",
    "- We need both `content` and `close_notes_ref` from the same incident (for Notebook 03)\n",
    "- Metadata helps us find similar incidents for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ground truth dataset with required structure\n",
    "gt_dataset = pd.DataFrame({\n",
    "    'number': df_ground_truth['number'].values,\n",
    "    'content': df_ground_truth['content'].values,\n",
    "    'close_notes_ref': df_ground_truth['close_notes'].values\n",
    "})\n",
    "\n",
    "# Add optional metadata columns for reference\n",
    "if 'category' in df_ground_truth.columns:\n",
    "    gt_dataset['category'] = df_ground_truth['category'].values\n",
    "if 'subcategory' in df_ground_truth.columns:\n",
    "    gt_dataset['subcategory'] = df_ground_truth['subcategory'].values\n",
    "if 'contact_type' in df_ground_truth.columns:\n",
    "    gt_dataset['contact_type'] = df_ground_truth['contact_type'].values\n",
    "if 'info_score_close_notes' in df_ground_truth.columns:\n",
    "    gt_dataset['info_score'] = df_ground_truth['info_score_close_notes'].values\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GROUND TRUTH DATASET STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal records: {len(gt_dataset)}\")\n",
    "print(f\"\\nColumns: {list(gt_dataset.columns)}\")\n",
    "print(f\"\\nFirst few records:\")\n",
    "print(gt_dataset.head())\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Separate into Two Datasets\n",
    "\n",
    "**What we're doing:** Splitting our incidents into two groups - the \"good examples\" and everything else.\n",
    "\n",
    "**Why separate:**\n",
    "- **Reference Dataset** (the \"good\" ones): High-quality close notes that will serve as our standards\n",
    "  - Think of these like \"model answers\" - examples of what good close notes should look like\n",
    "  - We'll use these to evaluate other close notes (existing or AI-generated)\n",
    "  \n",
    "- **Other Incidents Dataset** (the rest): All remaining incidents that didn't meet our high-quality criteria\n",
    "  - These are standard close notes - not bad, just not exceptional\n",
    "  - We'll compare these against the reference to see the difference\n",
    "\n",
    "**How we separate:**\n",
    "- Records that passed all quality filters ‚Üí **Reference Dataset** ‚úÖ\n",
    "- All other records ‚Üí **Other Incidents Dataset** üìã\n",
    "\n",
    "**Think of it like:**\n",
    "- Sorting apples into \"premium\" (perfect, beautiful) and \"regular\" (good, but not perfect)\n",
    "- The premium ones become our reference for what \"good\" looks like\n",
    "- The regular ones help us see the difference\n",
    "\n",
    "**This separation allows us to:**\n",
    "- Compare how \"good\" close notes differ from standard ones\n",
    "- Use references to evaluate other close notes (like grading against a rubric)\n",
    "- Understand quality differences between datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into two datasets\n",
    "# This is the key step: creating Reference Dataset and Other Incidents Dataset\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET SEPARATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Reference Dataset: High-quality close notes (what we've filtered)\n",
    "# Contains: number, content, close_notes_ref, and metadata\n",
    "reference_dataset = gt_dataset.copy()\n",
    "\n",
    "# Other Incidents Dataset: All incidents NOT in reference dataset\n",
    "# These are incidents that didn't meet the high-quality criteria\n",
    "reference_numbers = set(reference_dataset['number'].values)\n",
    "other_incidents = df[~df['number'].isin(reference_numbers)].copy()\n",
    "\n",
    "# Keep same structure for other incidents (for consistency)\n",
    "# Keep: number, content, close_notes (if available), and metadata\n",
    "other_incidents_dataset = other_incidents[[\n",
    "    'number', 'content', 'close_notes', 'category', 'subcategory', 'contact_type'\n",
    "]].copy() if 'close_notes' in other_incidents.columns else other_incidents[['number', 'content', 'category', 'subcategory', 'contact_type']].copy()\n",
    "\n",
    "print(f\"\\nüìä Separation Summary:\")\n",
    "print(f\"   Total incidents: {len(df)}\")\n",
    "print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(f\"   ‚úÖ Reference Dataset (high-quality): {len(reference_dataset)} incidents\")\n",
    "print(f\"      - These are our 'good' examples\")\n",
    "print(f\"      - Will be used as references for evaluation\")\n",
    "print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(f\"   üìã Other Incidents Dataset: {len(other_incidents_dataset)} incidents\")\n",
    "print(f\"      - Remaining incidents (standard quality)\")\n",
    "print(f\"      - Will be used for comparison\")\n",
    "print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(f\"   Total: {len(reference_dataset) + len(other_incidents_dataset)} incidents\")\n",
    "print(f\"   Match check: {'‚úÖ Match' if len(reference_dataset) + len(other_incidents_dataset) == len(df) else '‚ö†Ô∏è Mismatch'}\")\n",
    "\n",
    "# Show examples of the difference\n",
    "print(f\"\\nüìù Example from Reference Dataset (High-Quality):\")\n",
    "if len(reference_dataset) > 0:\n",
    "    example_ref = reference_dataset.iloc[0]\n",
    "    print(f\"   Incident: {example_ref['number']}\")\n",
    "    print(f\"   Close Note: {str(example_ref['close_notes_ref'])[:200]}...\")\n",
    "    print(f\"   Quality Score: {example_ref.get('info_score', 'N/A')}\")\n",
    "\n",
    "print(f\"\\nüìù Example from Other Incidents Dataset (Standard):\")\n",
    "if len(other_incidents_dataset) > 0 and 'close_notes' in other_incidents_dataset.columns:\n",
    "    example_other = other_incidents_dataset[other_incidents_dataset['close_notes'].notna()].iloc[0] if len(other_incidents_dataset[other_incidents_dataset['close_notes'].notna()]) > 0 else other_incidents_dataset.iloc[0]\n",
    "    print(f\"   Incident: {example_other['number']}\")\n",
    "    if 'close_notes' in example_other and pd.notna(example_other['close_notes']):\n",
    "        print(f\"   Close Note: {str(example_other['close_notes'])[:200]}...\")\n",
    "    else:\n",
    "        print(f\"   Close Note: (not available)\")\n",
    "\n",
    "print(\"\\nüí° Key Difference:\")\n",
    "print(\"   - Reference Dataset: Detailed, informative, complete close notes\")\n",
    "print(\"   - Other Incidents: Standard close notes (may be shorter, less detailed, or generic)\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Display Sample Examples\n",
    "\n",
    "**What we're doing:** Showing examples from both datasets so you can see the difference.\n",
    "\n",
    "**What to look for:**\n",
    "- Reference examples are detailed and informative\n",
    "- Other incidents may be shorter or less detailed\n",
    "- This visual comparison helps understand what makes a \"good\" close note\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample examples\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE GROUND TRUTH EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in gt_dataset.head(3).iterrows():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {idx + 1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nüìã Incident Number: {row['number']}\")\n",
    "    if 'category' in row:\n",
    "        print(f\"üè∑Ô∏è  Category: {row['category']}\")\n",
    "    if 'subcategory' in row:\n",
    "        print(f\"üè∑Ô∏è  Subcategory: {row['subcategory']}\")\n",
    "    if 'info_score' in row:\n",
    "        print(f\"‚≠ê Quality Score: {row['info_score']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüìù Original Content (Input):\")\n",
    "    print(f\"   {row['content'][:300]}...\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Reference Close Notes (Ground Truth):\")\n",
    "    print(f\"   {row['close_notes_ref'][:400]}...\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Both Datasets\n",
    "\n",
    "**What we're doing:** Saving both datasets so we can use them in later notebooks.\n",
    "\n",
    "**What gets saved:**\n",
    "1. **Reference Dataset** (`reference_close_notes.csv`) - **Good Samples**\n",
    "   - High-quality close notes with both `content` and `close_notes_ref`\n",
    "   - Used as references for evaluation\n",
    "   \n",
    "2. **Other Incidents Dataset** (`other_incidents.csv`) - **Remaining Samples**\n",
    "   - All remaining incidents (those that didn't meet high-quality criteria)\n",
    "   - Includes `content` and `close_notes` (if available)\n",
    "   - Can be used for comparison in Notebook 03\n",
    "\n",
    "**Why save both:**\n",
    "- Reference Dataset: Used in Notebooks 03, 04, and 05 for evaluation\n",
    "- Other Incidents Dataset: Used for comparison to see quality differences\n",
    "- Saving both allows us to work with consistent data across notebooks\n",
    "- Both files are ready to use without re-running this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Reference Dataset (high-quality close notes)\n",
    "# This is the main dataset we'll use as references for evaluation\n",
    "\n",
    "reference_final = reference_dataset[[\n",
    "    'number',\n",
    "    'content',\n",
    "    'close_notes_ref'\n",
    "]].copy()\n",
    "\n",
    "# Add metadata columns if they exist\n",
    "for col in ['category', 'subcategory', 'contact_type', 'info_score']:\n",
    "    if col in reference_dataset.columns:\n",
    "        reference_final[col] = reference_dataset[col]\n",
    "\n",
    "# Save Reference Dataset (good samples)\n",
    "reference_path = data_dir / \"reference_close_notes.csv\"\n",
    "reference_final.to_csv(reference_path, index=False)\n",
    "\n",
    "# Prepare Other Incidents Dataset for saving\n",
    "# This dataset already has the structure we need: number, content, close_notes, and metadata\n",
    "other_incidents_final = other_incidents_dataset.copy()\n",
    "\n",
    "# Save Other Incidents Dataset (remaining incidents - not high-quality)\n",
    "other_incidents_path = data_dir / \"other_incidents.csv\"\n",
    "other_incidents_final.to_csv(other_incidents_path, index=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASETS SAVED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ Reference Dataset (Good Samples) saved:\")\n",
    "print(f\"   File: {reference_path}\")\n",
    "print(f\"   Total records: {len(reference_final)}\")\n",
    "print(f\"   File size: {reference_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"   Columns: {list(reference_final.columns)}\")\n",
    "print(f\"   Use: High-quality examples for evaluation (Notebooks 03, 04, 05)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Other Incidents Dataset (Remaining Samples) saved:\")\n",
    "print(f\"   File: {other_incidents_path}\")\n",
    "print(f\"   Total records: {len(other_incidents_final)}\")\n",
    "print(f\"   File size: {other_incidents_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"   Columns: {list(other_incidents_final.columns)}\")\n",
    "print(f\"   Use: For comparison in Notebook 03\")\n",
    "\n",
    "print(\"\\nüí° Summary:\")\n",
    "print(f\"   - Reference Dataset (good): {len(reference_final)} high-quality examples ‚Üí {reference_path.name}\")\n",
    "print(f\"   - Other Incidents (remaining): {len(other_incidents_final)} remaining incidents ‚Üí {other_incidents_path.name}\")\n",
    "print(f\"   - Total: {len(reference_final) + len(other_incidents_final)} incidents\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary Statistics\n",
    "\n",
    "**What we're doing:** Looking at the final results - what we created and how the two datasets compare.\n",
    "\n",
    "**What we'll show:**\n",
    "- **Size of each dataset** - How many incidents in each group?\n",
    "- **Distribution across categories** - Do we have examples from different types of problems?\n",
    "- **Quality differences** - How do the scores compare between \"good\" and \"regular\"?\n",
    "\n",
    "**This helps us understand:**\n",
    "- How many reference examples we have (enough to evaluate with?)\n",
    "- Whether we have good coverage across categories (not just SOFTWARE?)\n",
    "- The difference between reference and other incidents (is the quality gap clear?)\n",
    "\n",
    "**Think of it like:** A final report card showing what we accomplished and what we have to work with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary comparing both datasets\n",
    "# This shows us what we accomplished and what we have to work with\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ FINAL SUMMARY: DATASET SEPARATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ Reference Dataset (High-Quality Examples):\")\n",
    "print(f\"   üì¶ Total records: {len(reference_final)}\")\n",
    "print(f\"   üéØ Purpose: Examples of good close notes for evaluation\")\n",
    "print(f\"   üí° Think of these as 'model answers' - what good close notes should look like\")\n",
    "\n",
    "if 'category' in reference_final.columns:\n",
    "    print(f\"\\n   üìä Category Distribution:\")\n",
    "    for cat, count in reference_final['category'].value_counts().items():\n",
    "        pct = count/len(reference_final)*100\n",
    "        print(f\"      ‚Ä¢ {cat}: {count} incidents ({pct:.1f}%)\")\n",
    "\n",
    "if 'info_score' in reference_final.columns:\n",
    "    print(f\"\\n   ‚≠ê Quality Scores:\")\n",
    "    print(f\"      Mean (average): {reference_final['info_score'].mean():.3f}\")\n",
    "    print(f\"      Range: {reference_final['info_score'].min():.3f} to {reference_final['info_score'].max():.3f}\")\n",
    "    print(f\"      üí° All scores are ‚â• 0.8 (high quality!)\")\n",
    "\n",
    "print(f\"\\nüìã Other Incidents Dataset (Remaining Samples):\")\n",
    "print(f\"   üì¶ Total records: {len(other_incidents_dataset)}\")\n",
    "print(f\"   üéØ Purpose: Remaining incidents for comparison\")\n",
    "print(f\"   üí° These are standard close notes - not bad, just not exceptional\")\n",
    "\n",
    "if 'category' in other_incidents_dataset.columns:\n",
    "    print(f\"\\n   üìä Category Distribution:\")\n",
    "    for cat, count in other_incidents_dataset['category'].value_counts().items():\n",
    "        pct = count/len(other_incidents_dataset)*100\n",
    "        print(f\"      ‚Ä¢ {cat}: {count} incidents ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä Overall Statistics:\")\n",
    "print(f\"   üì¶ Total incidents: {len(df)}\")\n",
    "ref_pct = len(reference_final)/len(df)*100\n",
    "other_pct = len(other_incidents_dataset)/len(df)*100\n",
    "print(f\"   ‚úÖ Reference Dataset: {len(reference_final)} incidents ({ref_pct:.1f}%)\")\n",
    "print(f\"   üìã Other Incidents: {len(other_incidents_dataset)} incidents ({other_pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüíæ Files Saved:\")\n",
    "print(f\"   ‚úÖ Reference Dataset: data/reference_close_notes.csv\")\n",
    "print(f\"   ‚úÖ Other Incidents: data/other_incidents.csv\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   ‚Üí Notebook 03: Compare n-gram scores between reference and other incidents\")\n",
    "print(f\"   ‚Üí Notebook 04: Analyze semantic similarity using embeddings\")\n",
    "print(f\"   ‚Üí Notebook 05: Use LLM-as-a-Judge to evaluate close notes\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
