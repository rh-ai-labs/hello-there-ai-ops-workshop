{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Define Quality and Separate Datasets\n",
    "\n",
    "## üéØ What is This Notebook About?\n",
    "\n",
    "This notebook helps you understand what makes a \"good\" close note and separates your incident dataset into two groups:\n",
    "- **Reference Dataset** (high-quality close notes) - These will serve as examples of what good close notes look like\n",
    "- **Other Incidents Dataset** - All remaining incidents (for comparison)\n",
    "\n",
    "**Why this matters:**\n",
    "- We need clear examples of good close notes to compare against\n",
    "- These reference examples will be used to evaluate other close notes (existing or AI-generated)\n",
    "- By separating them, we can see the difference between high-quality and standard close notes\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Key Concepts Explained\n",
    "\n",
    "### What Makes a \"Good\" Close Note?\n",
    "\n",
    "A good close note should:\n",
    "1. **Be informative** - Contains specific details about the problem and solution\n",
    "2. **Be complete** - Covers what happened, what was done, and the outcome\n",
    "3. **Avoid generic phrases** - Doesn't just say \"Issue resolved\" without explanation\n",
    "4. **Be professional** - Clear, well-structured, and easy to understand\n",
    "\n",
    "**Example of a GOOD close note:**\n",
    "> \"Investigated the reported issue with Workday crashing when saving files. Cleared browser cache and cookies, updated browser to latest version. Verified user can now save files successfully. Issue resolved and confirmed with user.\"\n",
    "\n",
    "**Example of a BAD/Generic close note:**\n",
    "> \"Issue resolved.\"\n",
    "\n",
    "**Why the difference matters:**\n",
    "- Good close notes help others understand what happened and how it was fixed\n",
    "- Generic close notes don't provide useful information\n",
    "- We'll use good examples as \"reference\" to evaluate other close notes\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "This notebook will:\n",
    "1. **Define quality criteria** - Explain what makes a close note \"good\"\n",
    "2. **Examine quality scores** - Look at how close notes are scored\n",
    "3. **Filter high-quality examples** - Identify close notes that meet our criteria\n",
    "4. **Separate into two datasets:**\n",
    "   - **Reference Dataset** - High-quality close notes (ground truth)\n",
    "   - **Other Incidents Dataset** - Remaining incidents (for comparison)\n",
    "5. **Save datasets** - Save both for use in later notebooks\n",
    "\n",
    "---\n",
    "\n",
    "## üìã How We'll Separate the Data\n",
    "\n",
    "**Separation Strategy:**\n",
    "- **Reference Dataset** (`reference_close_notes.csv`): \n",
    "  - High-quality close notes that meet all criteria\n",
    "  - Will be used as examples/references for evaluation\n",
    "  \n",
    "- **Other Incidents Dataset**:\n",
    "  - All incidents that don't meet the \"high-quality\" criteria\n",
    "  - Will be used for comparison in later notebooks\n",
    "\n",
    "**Selection Criteria for Reference Dataset:**\n",
    "- High information score (`info_score_close_notes` ‚â• 0.8)\n",
    "- Low generic content score (`info_score_poor_close_notes` ‚â§ 0.1)\n",
    "- Complete and informative text (not just \"Issue resolved\")\n",
    "- Minimum length (ensures sufficient detail)\n",
    "- Diverse examples across different categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "**What we're doing:** Loading the tools we need to work with data and create visualizations.\n",
    "\n",
    "**Why:** Just like a carpenter needs a hammer and saw, we need specific tools (libraries) to work with data.\n",
    "\n",
    "**What to expect:** You'll see a success message when everything is loaded correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# Think of these like tools in a toolbox - each one does a specific job\n",
    "\n",
    "import pandas as pd  # For working with data tables (like Excel spreadsheets)\n",
    "import numpy as np   # For doing math calculations\n",
    "import matplotlib.pyplot as plt  # For creating charts and graphs\n",
    "import seaborn as sns  # For making prettier charts\n",
    "from pathlib import Path  # For handling file paths\n",
    "import sys\n",
    "import re  # For text pattern matching (finding generic phrases)\n",
    "\n",
    "# Add src directory to path so we can use our helper functions\n",
    "sys.path.append(str(Path(\"../src\").resolve()))\n",
    "\n",
    "# Import our custom helper functions\n",
    "from utils import load_incident_dataset, calculate_basic_stats\n",
    "\n",
    "# Set up plotting style (makes our charts look nicer)\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn')\n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline  # This makes charts appear in the notebook\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"üìö Ready to start analyzing close notes quality!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Prepared Dataset\n",
    "\n",
    "**What we're doing:** Loading the incident dataset from Notebook 01.\n",
    "\n",
    "**Why:** We need all incidents with their close notes so we can identify which ones are high-quality and separate them.\n",
    "\n",
    "**What to expect:** The dataset contains incidents with various quality levels of close notes. We'll filter to find the best ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared dataset from notebook 01\n",
    "# This is the data we prepared in the previous notebook\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "prepared_path = data_dir / \"incidents_prepared.csv\"\n",
    "\n",
    "if prepared_path.exists():\n",
    "    # Load the prepared dataset (faster - we already processed it)\n",
    "    df = pd.read_csv(prepared_path)\n",
    "    print(f\"‚úÖ Loaded prepared dataset: {len(df)} records\")\n",
    "else:\n",
    "    # If the prepared file doesn't exist, load from scratch\n",
    "    print(\"‚ö†Ô∏è Prepared dataset not found. Loading from Hugging Face...\")\n",
    "    df = load_incident_dataset(sample_size=200, random_state=42)\n",
    "    # Filter for records with close_notes (we need these!)\n",
    "    if 'close_notes' in df.columns:\n",
    "        df = df[df['close_notes'].notna()].copy()\n",
    "    print(f\"‚úÖ Loaded dataset: {len(df)} records with close_notes\")\n",
    "\n",
    "# Let's see what we're working with\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"   Total incidents: {len(df)}\")\n",
    "print(f\"   Columns (pieces of information): {df.shape[1]}\")\n",
    "print(f\"\\nüìã Key columns we'll use:\")\n",
    "key_columns = ['close_notes', 'info_score_close_notes', 'info_score_poor_close_notes', 'category']\n",
    "for col in key_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"   ‚úÖ {col}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {col} (missing!)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understand Quality Scores\n",
    "\n",
    "**What we're doing:** Looking at how close notes are scored for quality.\n",
    "\n",
    "**What are quality scores?**\n",
    "- **`info_score_close_notes`**: Measures how informative a close note is (0.0 to 1.0)\n",
    "  - **High score (‚â•0.8)**: Contains detailed, useful information\n",
    "  - **Low score (<0.8)**: Lacks detail or information\n",
    "  \n",
    "- **`info_score_poor_close_notes`**: Measures how \"generic\" a close note is (0.0 to 1.0)\n",
    "  - **Low score (‚â§0.1)**: Not generic, contains specific information\n",
    "  - **High score (>0.1)**: Generic phrases like \"Issue resolved\"\n",
    "\n",
    "**Why this matters:** We want close notes that are informative (high score) and not generic (low poor score).\n",
    "\n",
    "**What to look for:** \n",
    "- How many close notes have high information scores?\n",
    "- How many have low generic scores?\n",
    "- This tells us how many \"good\" examples we can use as references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check quality score columns\n",
    "# This shows us how the close notes are scored for quality\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä QUALITY SCORES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüí° Understanding the scores:\")\n",
    "print(\"   ‚Ä¢ Higher info_score_close_notes = More informative (better!)\")\n",
    "print(\"   ‚Ä¢ Lower info_score_poor_close_notes = Less generic (better!)\")\n",
    "print(\"   ‚Ä¢ We want: High info score (‚â•0.8) AND Low poor score (‚â§0.1)\")\n",
    "\n",
    "if 'info_score_close_notes' in df.columns:\n",
    "    high_quality_count = (df['info_score_close_notes'] >= 0.8).sum()\n",
    "    high_quality_pct = (high_quality_count / len(df)) * 100\n",
    "    print(f\"\\nüìä info_score_close_notes (How informative?):\")\n",
    "    print(f\"   Mean (average): {df['info_score_close_notes'].mean():.3f}\")\n",
    "    print(f\"   Median (middle value): {df['info_score_close_notes'].median():.3f}\")\n",
    "    print(f\"   Range: {df['info_score_close_notes'].min():.3f} to {df['info_score_close_notes'].max():.3f}\")\n",
    "    print(f\"   ‚úÖ High quality (‚â•0.8): {high_quality_count} incidents ({high_quality_pct:.1f}%)\")\n",
    "\n",
    "if 'info_score_poor_close_notes' in df.columns:\n",
    "    low_generic_count = (df['info_score_poor_close_notes'] <= 0.1).sum()\n",
    "    low_generic_pct = (low_generic_count / len(df)) * 100\n",
    "    print(f\"\\nüìä info_score_poor_close_notes (How generic?):\")\n",
    "    print(f\"   Mean (average): {df['info_score_poor_close_notes'].mean():.3f}\")\n",
    "    print(f\"   Median (middle value): {df['info_score_poor_close_notes'].median():.3f}\")\n",
    "    print(f\"   ‚úÖ Low generic (‚â§0.1): {low_generic_count} incidents ({low_generic_pct:.1f}%)\")\n",
    "\n",
    "# Visualize score distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "if 'info_score_close_notes' in df.columns:\n",
    "    axes[0].hist(df['info_score_close_notes'].dropna(), bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[0].axvline(0.8, color='red', linestyle='--', label='Threshold (‚â•0.8)')\n",
    "    axes[0].set_xlabel('Info Score (close_notes)', fontsize=11)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[0].set_title('Close Notes Quality Score Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    axes[0].legend()\n",
    "\n",
    "if 'info_score_poor_close_notes' in df.columns:\n",
    "    axes[1].hist(df['info_score_poor_close_notes'].dropna(), bins=20, edgecolor='black', alpha=0.7, color='coral')\n",
    "    axes[1].axvline(0.1, color='red', linestyle='--', label='Threshold (‚â§0.1)')\n",
    "    axes[1].set_xlabel('Info Score (poor_close_notes)', fontsize=11)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[1].set_title('Poor Close Notes Score Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Quality Criteria and Filter\n",
    "\n",
    "**What we're doing:** Applying filters to identify high-quality close notes that will become our \"Reference Dataset\".\n",
    "\n",
    "**Our quality criteria:**\n",
    "1. **High information score** (`info_score_close_notes` ‚â• 0.8)\n",
    "   - *Why?* Ensures the close note contains useful, detailed information\n",
    "   \n",
    "2. **Low generic score** (`info_score_poor_close_notes` ‚â§ 0.1)\n",
    "   - *Why?* Ensures it's not just generic phrases like \"Issue resolved\"\n",
    "   \n",
    "3. **Not generic phrases** - Excludes notes that only say things like:\n",
    "   - \"No changes noted\"\n",
    "   - \"Issue resolved\"\n",
    "   - \"Resolved per user\"\n",
    "   \n",
    "4. **Minimum length** (‚â•100 characters)\n",
    "   - *Why?* Ensures there's enough detail to be useful\n",
    "\n",
    "**What happens:**\n",
    "- We'll filter the dataset step by step\n",
    "- Records that pass all filters ‚Üí **Reference Dataset** (high-quality examples)\n",
    "- Records that don't pass ‚Üí **Other Incidents Dataset** (for comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generic phrases to exclude\n",
    "# These are phrases that don't provide useful information\n",
    "# Think of them like \"filler words\" - they don't tell us anything useful\n",
    "\n",
    "GENERIC_PHRASES = [\n",
    "    'no changes noted',\n",
    "    'issue resolved',\n",
    "    'resolved',\n",
    "    'closed',\n",
    "    'no further action',\n",
    "    'resolved per user',\n",
    "    'user confirmed resolved'\n",
    "]\n",
    "\n",
    "def is_generic_close_note(text):\n",
    "    \"\"\"\n",
    "    Check if close note contains only generic phrases.\n",
    "    \n",
    "    This function looks for close notes that are too short or only contain\n",
    "    generic phrases like \"Issue resolved\" without any details.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return True  # Missing or not text = consider it generic\n",
    "    \n",
    "    text_lower = text.lower().strip()\n",
    "    \n",
    "    # Check if text is too short (likely generic)\n",
    "    # Good close notes need detail, so very short ones are probably generic\n",
    "    if len(text_lower) < 50:\n",
    "        return True\n",
    "    \n",
    "    # Check if text contains only generic phrases\n",
    "    words = set(text_lower.split())\n",
    "    generic_words = set()\n",
    "    for phrase in GENERIC_PHRASES:\n",
    "        generic_words.update(phrase.split())\n",
    "    \n",
    "    # If most words are generic, likely a generic note\n",
    "    # (e.g., \"Issue resolved\" = only 2 words, both generic)\n",
    "    if len(words) <= 5 and words.issubset(generic_words):\n",
    "        return True\n",
    "    \n",
    "    return False  # Not generic - has useful information!\n",
    "\n",
    "# Apply filters step by step\n",
    "# We'll filter the data multiple times, keeping only the best close notes\n",
    "print(\"üîç Applying quality filters...\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üì¶ Initial records: {len(df)}\")\n",
    "\n",
    "# Filter 1: Must have close_notes\n",
    "# We can't evaluate quality if there's no close note!\n",
    "df_filtered = df[df['close_notes'].notna()].copy()\n",
    "print(f\"‚úÖ Filter 1 - Has close_notes: {len(df_filtered)} records\")\n",
    "\n",
    "# Filter 2: High quality score (informative)\n",
    "# We want close notes that are informative (score ‚â• 0.8)\n",
    "if 'info_score_close_notes' in df_filtered.columns:\n",
    "    before = len(df_filtered)\n",
    "    df_filtered = df_filtered[df_filtered['info_score_close_notes'] >= 0.8].copy()\n",
    "    removed = before - len(df_filtered)\n",
    "    print(f\"‚úÖ Filter 2 - High info score (‚â•0.8): {len(df_filtered)} records (removed {removed})\")\n",
    "\n",
    "# Filter 3: Low poor quality score (not generic)\n",
    "# We want close notes that are NOT generic (score ‚â§ 0.1)\n",
    "if 'info_score_poor_close_notes' in df_filtered.columns:\n",
    "    before = len(df_filtered)\n",
    "    df_filtered = df_filtered[df_filtered['info_score_poor_close_notes'] <= 0.1].copy()\n",
    "    removed = before - len(df_filtered)\n",
    "    print(f\"‚úÖ Filter 3 - Low generic score (‚â§0.1): {len(df_filtered)} records (removed {removed})\")\n",
    "\n",
    "# Filter 4: Exclude generic notes (text-based check)\n",
    "# Double-check: remove any that are just generic phrases\n",
    "df_filtered['is_generic'] = df_filtered['close_notes'].apply(is_generic_close_note)\n",
    "before = len(df_filtered)\n",
    "df_filtered = df_filtered[~df_filtered['is_generic']].copy()\n",
    "removed = before - len(df_filtered)\n",
    "print(f\"‚úÖ Filter 4 - Exclude generic phrases: {len(df_filtered)} records (removed {removed})\")\n",
    "\n",
    "# Filter 5: Minimum text length (ensure informative)\n",
    "# Very short close notes probably don't have enough detail\n",
    "df_filtered['close_notes_length'] = df_filtered['close_notes'].astype(str).str.len()\n",
    "before = len(df_filtered)\n",
    "df_filtered = df_filtered[df_filtered['close_notes_length'] >= 100].copy()\n",
    "removed = before - len(df_filtered)\n",
    "print(f\"‚úÖ Filter 5 - Minimum length (‚â•100 chars): {len(df_filtered)} records (removed {removed})\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüéØ Final filtered dataset: {len(df_filtered)} high-quality records\")\n",
    "reduction_pct = ((len(df) - len(df_filtered))/len(df)*100)\n",
    "print(f\"   üìâ Filtered out: {len(df) - len(df_filtered)} records ({reduction_pct:.1f}%)\")\n",
    "print(f\"   ‚úÖ Kept: {len(df_filtered)} records ({100-reduction_pct:.1f}%)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Diversity of Filtered Dataset\n",
    "\n",
    "**What we're doing:** Checking how diverse our filtered high-quality close notes are across different categories.\n",
    "\n",
    "**Why this matters:**\n",
    "- We want examples from different types of incidents (not just SOFTWARE)\n",
    "- Diverse examples help when evaluating different categories later\n",
    "- This shows us if we need to balance the dataset better\n",
    "\n",
    "**What we'll check:**\n",
    "- **Categories**: How many incidents per category (SOFTWARE, ACCOUNT, etc.)\n",
    "- **Subcategories**: How many per subcategory (ERROR, MALFUNCTION, etc.)\n",
    "- **Contact Types**: How incidents were reported (Email, Phone, Chat, etc.)\n",
    "\n",
    "**What to look for:**\n",
    "- If one category dominates (e.g., 90% SOFTWARE), we may want to balance\n",
    "- Good diversity means we have examples across different incident types\n",
    "- This helps when evaluating close notes for different types of problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze diversity\n",
    "print(\"=\"*80)\n",
    "print(\"DIVERSITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'category' in df_filtered.columns:\n",
    "    print(f\"\\nüìä Categories:\")\n",
    "    category_counts = df_filtered['category'].value_counts()\n",
    "    for cat, count in category_counts.items():\n",
    "        print(f\"   {cat}: {count} ({count/len(df_filtered)*100:.1f}%)\")\n",
    "\n",
    "if 'subcategory' in df_filtered.columns:\n",
    "    print(f\"\\nüìã Subcategories:\")\n",
    "    subcat_counts = df_filtered['subcategory'].value_counts()\n",
    "    print(f\"   Total unique: {df_filtered['subcategory'].nunique()}\")\n",
    "    for subcat, count in subcat_counts.head(10).items():\n",
    "        print(f\"   {subcat}: {count}\")\n",
    "\n",
    "if 'contact_type' in df_filtered.columns:\n",
    "    print(f\"\\nüìû Contact Types:\")\n",
    "    contact_counts = df_filtered['contact_type'].value_counts()\n",
    "    for contact, count in contact_counts.items():\n",
    "        print(f\"   {contact}: {count} ({count/len(df_filtered)*100:.1f}%)\")\n",
    "\n",
    "# Visualize diversity\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "if 'category' in df_filtered.columns:\n",
    "    category_counts = df_filtered['category'].value_counts()\n",
    "    axes[0].bar(range(len(category_counts)), category_counts.values, \n",
    "                color=sns.color_palette(\"husl\", len(category_counts)))\n",
    "    axes[0].set_xticks(range(len(category_counts)))\n",
    "    axes[0].set_xticklabels(category_counts.index, rotation=45, ha='right')\n",
    "    axes[0].set_ylabel('Count', fontsize=10)\n",
    "    axes[0].set_title('Category Distribution (Filtered)', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(category_counts.values):\n",
    "        axes[0].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "if 'subcategory' in df_filtered.columns:\n",
    "    subcat_counts = df_filtered['subcategory'].value_counts().head(10)\n",
    "    axes[1].barh(range(len(subcat_counts)), subcat_counts.values,\n",
    "                color=sns.color_palette(\"viridis\", len(subcat_counts)))\n",
    "    axes[1].set_yticks(range(len(subcat_counts)))\n",
    "    axes[1].set_yticklabels(subcat_counts.index)\n",
    "    axes[1].set_xlabel('Count', fontsize=10)\n",
    "    axes[1].set_title('Top 10 Subcategories (Filtered)', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "if 'contact_type' in df_filtered.columns:\n",
    "    contact_counts = df_filtered['contact_type'].value_counts()\n",
    "    axes[2].bar(contact_counts.index, contact_counts.values,\n",
    "               color=sns.color_palette(\"muted\", len(contact_counts)))\n",
    "    axes[2].set_ylabel('Count', fontsize=10)\n",
    "    axes[2].set_title('Contact Type Distribution (Filtered)', fontsize=12, fontweight='bold')\n",
    "    axes[2].grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(contact_counts.values):\n",
    "        axes[2].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensure Balanced Sampling\n",
    "\n",
    "**What we're doing:** Making sure our Reference Dataset has examples from different categories.\n",
    "\n",
    "**Why this matters:**\n",
    "- We want diverse examples (not just SOFTWARE incidents)\n",
    "- This helps when evaluating different types of incidents later\n",
    "- Balanced sampling ensures we have good examples across categories\n",
    "\n",
    "**How it works:**\n",
    "- We try to get a target number of samples per category (e.g., 20)\n",
    "- If a category has fewer samples, we take what's available\n",
    "- If a category has too few samples (< 5), we skip it\n",
    "\n",
    "**Result:** A balanced Reference Dataset with diverse examples.\n",
    "\n",
    "**Note:** After sampling, the remaining high-quality records (that didn't make it into the balanced sample) will still be part of the \"Other Incidents Dataset\" - they're still good quality, just not selected for the reference set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy: Sample balanced examples if one category dominates\n",
    "TARGET_SAMPLES_PER_CATEGORY = 20  # Adjust based on dataset size\n",
    "MIN_SAMPLES_PER_CATEGORY = 5     # Minimum samples per category\n",
    "\n",
    "print(\"Applying balanced sampling strategy...\")\n",
    "\n",
    "if 'category' in df_filtered.columns:\n",
    "    sampled_records = []\n",
    "    \n",
    "    for category in df_filtered['category'].unique():\n",
    "        category_df = df_filtered[df_filtered['category'] == category].copy()\n",
    "        \n",
    "        if len(category_df) >= TARGET_SAMPLES_PER_CATEGORY:\n",
    "            # Sample TARGET_SAMPLES_PER_CATEGORY records\n",
    "            sampled = category_df.sample(\n",
    "                min(TARGET_SAMPLES_PER_CATEGORY, len(category_df)),\n",
    "                random_state=42\n",
    "            )\n",
    "        elif len(category_df) >= MIN_SAMPLES_PER_CATEGORY:\n",
    "            # Take all available if less than target but above minimum\n",
    "            sampled = category_df\n",
    "        else:\n",
    "            # Skip categories with too few samples\n",
    "            print(f\"   ‚ö†Ô∏è Skipping {category}: only {len(category_df)} samples\")\n",
    "            continue\n",
    "        \n",
    "        sampled_records.append(sampled)\n",
    "        print(f\"   ‚úÖ {category}: {len(sampled)} samples\")\n",
    "    \n",
    "    df_ground_truth = pd.concat(sampled_records, ignore_index=True)\n",
    "    print(f\"\\n‚úÖ Balanced ground truth dataset: {len(df_ground_truth)} records\")\n",
    "else:\n",
    "    # If no category column, use all filtered records\n",
    "    df_ground_truth = df_filtered.copy()\n",
    "    print(f\"\\n‚úÖ Using all filtered records: {len(df_ground_truth)} records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Reference Dataset Structure\n",
    "\n",
    "**What we're doing:** Preparing the Reference Dataset with the columns we need.\n",
    "\n",
    "**Structure we need:**\n",
    "- `number` - Incident identifier\n",
    "- `content` - Original incident description\n",
    "- `close_notes_ref` - High-quality close note (reference example)\n",
    "- Metadata: category, subcategory, contact_type, info_score\n",
    "\n",
    "**Why these columns:**\n",
    "- We need both `content` and `close_notes_ref` from the same incident (for Notebook 03)\n",
    "- Metadata helps us find similar incidents for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ground truth dataset with required structure\n",
    "gt_dataset = pd.DataFrame({\n",
    "    'number': df_ground_truth['number'].values,\n",
    "    'content': df_ground_truth['content'].values,\n",
    "    'close_notes_ref': df_ground_truth['close_notes'].values\n",
    "})\n",
    "\n",
    "# Add optional metadata columns for reference\n",
    "if 'category' in df_ground_truth.columns:\n",
    "    gt_dataset['category'] = df_ground_truth['category'].values\n",
    "if 'subcategory' in df_ground_truth.columns:\n",
    "    gt_dataset['subcategory'] = df_ground_truth['subcategory'].values\n",
    "if 'contact_type' in df_ground_truth.columns:\n",
    "    gt_dataset['contact_type'] = df_ground_truth['contact_type'].values\n",
    "if 'info_score_close_notes' in df_ground_truth.columns:\n",
    "    gt_dataset['info_score'] = df_ground_truth['info_score_close_notes'].values\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GROUND TRUTH DATASET STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal records: {len(gt_dataset)}\")\n",
    "print(f\"\\nColumns: {list(gt_dataset.columns)}\")\n",
    "print(f\"\\nFirst few records:\")\n",
    "print(gt_dataset.head())\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Separate into Two Datasets\n",
    "\n",
    "**What we're doing:** Splitting our incidents into two groups - the \"good examples\" and everything else.\n",
    "\n",
    "**Why separate:**\n",
    "- **Reference Dataset** (the \"good\" ones): High-quality close notes that will serve as our standards\n",
    "  - Think of these like \"model answers\" - examples of what good close notes should look like\n",
    "  - We'll use these to evaluate other close notes (existing or AI-generated)\n",
    "  \n",
    "- **Other Incidents Dataset** (the rest): All remaining incidents that didn't meet our high-quality criteria\n",
    "  - These are standard close notes - not bad, just not exceptional\n",
    "  - We'll compare these against the reference to see the difference\n",
    "\n",
    "**How we separate:**\n",
    "- Records that passed all quality filters ‚Üí **Reference Dataset** ‚úÖ\n",
    "- All other records ‚Üí **Other Incidents Dataset** üìã\n",
    "\n",
    "**Think of it like:**\n",
    "- Sorting apples into \"premium\" (perfect, beautiful) and \"regular\" (good, but not perfect)\n",
    "- The premium ones become our reference for what \"good\" looks like\n",
    "- The regular ones help us see the difference\n",
    "\n",
    "**This separation allows us to:**\n",
    "- Compare how \"good\" close notes differ from standard ones\n",
    "- Use references to evaluate other close notes (like grading against a rubric)\n",
    "- Understand quality differences between datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into two datasets\n",
    "# This is the key step: creating Reference Dataset and Other Incidents Dataset\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET SEPARATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Reference Dataset: High-quality close notes (what we've filtered)\n",
    "# Contains: number, content, close_notes_ref, and metadata\n",
    "reference_dataset = gt_dataset.copy()\n",
    "\n",
    "# Other Incidents Dataset: All incidents NOT in reference dataset\n",
    "# These are incidents that didn't meet the high-quality criteria\n",
    "reference_numbers = set(reference_dataset['number'].values)\n",
    "other_incidents = df[~df['number'].isin(reference_numbers)].copy()\n",
    "\n",
    "# Keep same structure for other incidents (for consistency)\n",
    "# Keep: number, content, close_notes (if available), and metadata\n",
    "other_incidents_dataset = other_incidents[[\n",
    "    'number', 'content', 'close_notes', 'category', 'subcategory', 'contact_type'\n",
    "]].copy() if 'close_notes' in other_incidents.columns else other_incidents[['number', 'content', 'category', 'subcategory', 'contact_type']].copy()\n",
    "\n",
    "print(f\"\\nüìä Separation Summary:\")\n",
    "print(f\"   Total incidents: {len(df)}\")\n",
    "print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(f\"   ‚úÖ Reference Dataset (high-quality): {len(reference_dataset)} incidents\")\n",
    "print(f\"      - These are our 'good' examples\")\n",
    "print(f\"      - Will be used as references for evaluation\")\n",
    "print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(f\"   üìã Other Incidents Dataset: {len(other_incidents_dataset)} incidents\")\n",
    "print(f\"      - Remaining incidents (standard quality)\")\n",
    "print(f\"      - Will be used for comparison\")\n",
    "print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(f\"   Total: {len(reference_dataset) + len(other_incidents_dataset)} incidents\")\n",
    "print(f\"   Match check: {'‚úÖ Match' if len(reference_dataset) + len(other_incidents_dataset) == len(df) else '‚ö†Ô∏è Mismatch'}\")\n",
    "\n",
    "# Show examples of the difference\n",
    "print(f\"\\nüìù Example from Reference Dataset (High-Quality):\")\n",
    "if len(reference_dataset) > 0:\n",
    "    example_ref = reference_dataset.iloc[0]\n",
    "    print(f\"   Incident: {example_ref['number']}\")\n",
    "    print(f\"   Close Note: {str(example_ref['close_notes_ref'])[:200]}...\")\n",
    "    print(f\"   Quality Score: {example_ref.get('info_score', 'N/A')}\")\n",
    "\n",
    "print(f\"\\nüìù Example from Other Incidents Dataset (Standard):\")\n",
    "if len(other_incidents_dataset) > 0 and 'close_notes' in other_incidents_dataset.columns:\n",
    "    example_other = other_incidents_dataset[other_incidents_dataset['close_notes'].notna()].iloc[0] if len(other_incidents_dataset[other_incidents_dataset['close_notes'].notna()]) > 0 else other_incidents_dataset.iloc[0]\n",
    "    print(f\"   Incident: {example_other['number']}\")\n",
    "    if 'close_notes' in example_other and pd.notna(example_other['close_notes']):\n",
    "        print(f\"   Close Note: {str(example_other['close_notes'])[:200]}...\")\n",
    "    else:\n",
    "        print(f\"   Close Note: (not available)\")\n",
    "\n",
    "print(\"\\nüí° Key Difference:\")\n",
    "print(\"   - Reference Dataset: Detailed, informative, complete close notes\")\n",
    "print(\"   - Other Incidents: Standard close notes (may be shorter, less detailed, or generic)\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Display Sample Examples\n",
    "\n",
    "**What we're doing:** Showing examples from both datasets so you can see the difference.\n",
    "\n",
    "**What to look for:**\n",
    "- Reference examples are detailed and informative\n",
    "- Other incidents may be shorter or less detailed\n",
    "- This visual comparison helps understand what makes a \"good\" close note\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample examples\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE GROUND TRUTH EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in gt_dataset.head(3).iterrows():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {idx + 1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nüìã Incident Number: {row['number']}\")\n",
    "    if 'category' in row:\n",
    "        print(f\"üè∑Ô∏è  Category: {row['category']}\")\n",
    "    if 'subcategory' in row:\n",
    "        print(f\"üè∑Ô∏è  Subcategory: {row['subcategory']}\")\n",
    "    if 'info_score' in row:\n",
    "        print(f\"‚≠ê Quality Score: {row['info_score']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüìù Original Content (Input):\")\n",
    "    print(f\"   {row['content'][:300]}...\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Reference Close Notes (Ground Truth):\")\n",
    "    print(f\"   {row['close_notes_ref'][:400]}...\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Both Datasets\n",
    "\n",
    "**What we're doing:** Saving both datasets so we can use them in later notebooks.\n",
    "\n",
    "**What gets saved:**\n",
    "1. **Reference Dataset** (`reference_close_notes.csv`) - **Good Samples**\n",
    "   - High-quality close notes with both `content` and `close_notes_ref`\n",
    "   - Used as references for evaluation\n",
    "   \n",
    "2. **Other Incidents Dataset** (`other_incidents.csv`) - **Remaining Samples**\n",
    "   - All remaining incidents (those that didn't meet high-quality criteria)\n",
    "   - Includes `content` and `close_notes` (if available)\n",
    "   - Can be used for comparison in Notebook 03\n",
    "\n",
    "**Why save both:**\n",
    "- Reference Dataset: Used in Notebooks 03, 04, and 05 for evaluation\n",
    "- Other Incidents Dataset: Used for comparison to see quality differences\n",
    "- Saving both allows us to work with consistent data across notebooks\n",
    "- Both files are ready to use without re-running this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Reference Dataset (high-quality close notes)\n",
    "# This is the main dataset we'll use as references for evaluation\n",
    "\n",
    "reference_final = reference_dataset[[\n",
    "    'number',\n",
    "    'content',\n",
    "    'close_notes_ref'\n",
    "]].copy()\n",
    "\n",
    "# Add metadata columns if they exist\n",
    "for col in ['category', 'subcategory', 'contact_type', 'info_score']:\n",
    "    if col in reference_dataset.columns:\n",
    "        reference_final[col] = reference_dataset[col]\n",
    "\n",
    "# Save Reference Dataset (good samples)\n",
    "reference_path = data_dir / \"reference_close_notes.csv\"\n",
    "reference_final.to_csv(reference_path, index=False)\n",
    "\n",
    "# Prepare Other Incidents Dataset for saving\n",
    "# This dataset already has the structure we need: number, content, close_notes, and metadata\n",
    "other_incidents_final = other_incidents_dataset.copy()\n",
    "\n",
    "# Save Other Incidents Dataset (remaining incidents - not high-quality)\n",
    "other_incidents_path = data_dir / \"other_incidents.csv\"\n",
    "other_incidents_final.to_csv(other_incidents_path, index=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASETS SAVED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ Reference Dataset (Good Samples) saved:\")\n",
    "print(f\"   File: {reference_path}\")\n",
    "print(f\"   Total records: {len(reference_final)}\")\n",
    "print(f\"   File size: {reference_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"   Columns: {list(reference_final.columns)}\")\n",
    "print(f\"   Use: High-quality examples for evaluation (Notebooks 03, 04, 05)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Other Incidents Dataset (Remaining Samples) saved:\")\n",
    "print(f\"   File: {other_incidents_path}\")\n",
    "print(f\"   Total records: {len(other_incidents_final)}\")\n",
    "print(f\"   File size: {other_incidents_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"   Columns: {list(other_incidents_final.columns)}\")\n",
    "print(f\"   Use: For comparison in Notebook 03\")\n",
    "\n",
    "print(\"\\nüí° Summary:\")\n",
    "print(f\"   - Reference Dataset (good): {len(reference_final)} high-quality examples ‚Üí {reference_path.name}\")\n",
    "print(f\"   - Other Incidents (remaining): {len(other_incidents_final)} remaining incidents ‚Üí {other_incidents_path.name}\")\n",
    "print(f\"   - Total: {len(reference_final) + len(other_incidents_final)} incidents\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary Statistics\n",
    "\n",
    "**What we're doing:** Looking at the final results - what we created and how the two datasets compare.\n",
    "\n",
    "**What we'll show:**\n",
    "- **Size of each dataset** - How many incidents in each group?\n",
    "- **Distribution across categories** - Do we have examples from different types of problems?\n",
    "- **Quality differences** - How do the scores compare between \"good\" and \"regular\"?\n",
    "\n",
    "**This helps us understand:**\n",
    "- How many reference examples we have (enough to evaluate with?)\n",
    "- Whether we have good coverage across categories (not just SOFTWARE?)\n",
    "- The difference between reference and other incidents (is the quality gap clear?)\n",
    "\n",
    "**Think of it like:** A final report card showing what we accomplished and what we have to work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary comparing both datasets\n",
    "# This shows us what we accomplished and what we have to work with\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ FINAL SUMMARY: DATASET SEPARATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ Reference Dataset (High-Quality Examples):\")\n",
    "print(f\"   üì¶ Total records: {len(reference_final)}\")\n",
    "print(f\"   üéØ Purpose: Examples of good close notes for evaluation\")\n",
    "print(f\"   üí° Think of these as 'model answers' - what good close notes should look like\")\n",
    "\n",
    "if 'category' in reference_final.columns:\n",
    "    print(f\"\\n   üìä Category Distribution:\")\n",
    "    for cat, count in reference_final['category'].value_counts().items():\n",
    "        pct = count/len(reference_final)*100\n",
    "        print(f\"      ‚Ä¢ {cat}: {count} incidents ({pct:.1f}%)\")\n",
    "\n",
    "if 'info_score' in reference_final.columns:\n",
    "    print(f\"\\n   ‚≠ê Quality Scores:\")\n",
    "    print(f\"      Mean (average): {reference_final['info_score'].mean():.3f}\")\n",
    "    print(f\"      Range: {reference_final['info_score'].min():.3f} to {reference_final['info_score'].max():.3f}\")\n",
    "    print(f\"      üí° All scores are ‚â• 0.8 (high quality!)\")\n",
    "\n",
    "print(f\"\\nüìã Other Incidents Dataset (Remaining Samples):\")\n",
    "print(f\"   üì¶ Total records: {len(other_incidents_dataset)}\")\n",
    "print(f\"   üéØ Purpose: Remaining incidents for comparison\")\n",
    "print(f\"   üí° These are standard close notes - not bad, just not exceptional\")\n",
    "\n",
    "if 'category' in other_incidents_dataset.columns:\n",
    "    print(f\"\\n   üìä Category Distribution:\")\n",
    "    for cat, count in other_incidents_dataset['category'].value_counts().items():\n",
    "        pct = count/len(other_incidents_dataset)*100\n",
    "        print(f\"      ‚Ä¢ {cat}: {count} incidents ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä Overall Statistics:\")\n",
    "print(f\"   üì¶ Total incidents: {len(df)}\")\n",
    "ref_pct = len(reference_final)/len(df)*100\n",
    "other_pct = len(other_incidents_dataset)/len(df)*100\n",
    "print(f\"   ‚úÖ Reference Dataset: {len(reference_final)} incidents ({ref_pct:.1f}%)\")\n",
    "print(f\"   üìã Other Incidents: {len(other_incidents_dataset)} incidents ({other_pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüíæ Files Saved:\")\n",
    "print(f\"   ‚úÖ Reference Dataset: data/reference_close_notes.csv\")\n",
    "print(f\"   ‚úÖ Other Incidents: data/other_incidents.csv\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   ‚Üí Notebook 03: Compare n-gram scores between reference and other incidents\")\n",
    "print(f\"   ‚Üí Notebook 04: Analyze semantic similarity using embeddings\")\n",
    "print(f\"   ‚Üí Notebook 05: Use LLM-as-a-Judge to evaluate close notes\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Optional: Generate Embeddings for All Incidents\n",
    "\n",
    "**What we're doing:** (Optional) Creating semantic embeddings for ALL incidents (both reference and other) to validate quality scores.\n",
    "\n",
    "**What are embeddings?**\n",
    "- Mathematical representations of text that capture meaning\n",
    "- Similar meanings ‚Üí similar embeddings\n",
    "- Allows us to measure semantic similarity between close notes\n",
    "\n",
    "**Why generate embeddings for all incidents?**\n",
    "- **Validate quality scores**: Check if incidents with similar quality scores are semantically closer\n",
    "- **Understand relationships**: See how close notes relate to each other semantically\n",
    "- **Quality assurance**: Verify that our quality scoring makes sense (similar scores = similar content)\n",
    "\n",
    "**What we'll validate:**\n",
    "- Incidents with similar `info_score_close_notes` should be semantically similar\n",
    "- High-quality close notes should cluster together in semantic space\n",
    "- This confirms our quality criteria are meaningful\n",
    "\n",
    "**Note:** This is optional but useful for validating our quality scoring approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import embedding library\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sentence_transformers.util import cos_sim\n",
    "    EMBEDDINGS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è sentence-transformers not available. Install with: pip install sentence-transformers\")\n",
    "    EMBEDDINGS_AVAILABLE = False\n",
    "\n",
    "if EMBEDDINGS_AVAILABLE:\n",
    "    import os\n",
    "    print(\"Loading embedding model...\")\n",
    "    \n",
    "    # Model selection: Use BGE-M3 for multilingual, multi-granularity support\n",
    "    # Can be overridden via EMBEDDING_MODEL environment variable\n",
    "    DEFAULT_MODEL = 'BAAI/bge-m3'  # Multilingual, supports dense/sparse/multi-vector retrieval\n",
    "    # Alternative models:\n",
    "    # - 'BAAI/bge-small-en-v1.5' (faster, English-only)\n",
    "    # - 'BAAI/bge-base-en-v1.5' (slower, higher accuracy, English-only)\n",
    "    # - 'sentence-transformers/all-mpnet-base-v2' (proven alternative, English-only)\n",
    "    \n",
    "    embedding_model_name = os.getenv('EMBEDDING_MODEL', DEFAULT_MODEL)\n",
    "    print(f\"   Using model: {embedding_model_name}\")\n",
    "    \n",
    "    use_flag_embedding = False\n",
    "    try:\n",
    "        # Try sentence-transformers first\n",
    "        model = SentenceTransformer(embedding_model_name, trust_remote_code=True)\n",
    "        embedding_dim = model.get_sentence_embedding_dimension()\n",
    "        print(f\"‚úÖ Model loaded: {embedding_dim}-dimensional embeddings\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading with sentence-transformers: {e}\")\n",
    "        print(\"   Trying FlagEmbedding library...\")\n",
    "        try:\n",
    "            from FlagEmbedding import BGEM3FlagModel\n",
    "            model = BGEM3FlagModel(embedding_model_name)\n",
    "            use_flag_embedding = True\n",
    "            print(f\"‚úÖ Model loaded via FlagEmbedding (BGE-M3)\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è FlagEmbedding not installed. Install with: pip install FlagEmbedding\")\n",
    "            raise\n",
    "    \n",
    "    # Combine all incidents for embedding generation\n",
    "    # We want to generate embeddings for ALL incidents to validate quality scores\n",
    "    print(\"\\nPreparing all incidents for embedding generation...\")\n",
    "    \n",
    "    # Prepare reference dataset (has close_notes_ref)\n",
    "    reference_with_notes = reference_final.copy()\n",
    "    reference_with_notes['close_notes_for_embedding'] = reference_with_notes['close_notes_ref']\n",
    "    reference_with_notes['dataset_type'] = 'reference'\n",
    "    \n",
    "    # Prepare other incidents dataset (has close_notes)\n",
    "    other_with_notes = other_incidents_final.copy()\n",
    "    if 'close_notes' in other_with_notes.columns:\n",
    "        # Filter to only incidents that have close_notes\n",
    "        other_with_notes = other_with_notes[other_with_notes['close_notes'].notna()].copy()\n",
    "        other_with_notes['close_notes_for_embedding'] = other_with_notes['close_notes']\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Other incidents dataset doesn't have close_notes column\")\n",
    "        other_with_notes = pd.DataFrame()  # Empty if no close_notes\n",
    "    \n",
    "    other_with_notes['dataset_type'] = 'other'\n",
    "    \n",
    "    # Combine both datasets\n",
    "    all_incidents = pd.concat([reference_with_notes, other_with_notes], ignore_index=True)\n",
    "    print(f\"   Reference incidents: {len(reference_with_notes)}\")\n",
    "    print(f\"   Other incidents: {len(other_with_notes)}\")\n",
    "    print(f\"   Total incidents for embedding: {len(all_incidents)}\")\n",
    "    \n",
    "    # Generate embeddings for all close notes\n",
    "    print(\"\\nGenerating embeddings for all close notes...\")\n",
    "    close_notes_texts = all_incidents['close_notes_for_embedding'].astype(str).tolist()\n",
    "    \n",
    "    if use_flag_embedding:\n",
    "        # FlagEmbedding returns dict with 'dense_vecs', 'sparse', 'colbert_vecs'\n",
    "        output = model.encode(close_notes_texts, return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
    "        embeddings = output['dense_vecs']\n",
    "    else:\n",
    "        embeddings = model.encode(close_notes_texts, show_progress_bar=True, batch_size=32)\n",
    "    \n",
    "    print(f\"‚úÖ Generated embeddings for {len(embeddings)} close notes\")\n",
    "    print(f\"   Embedding dimensions: {embeddings.shape}\")\n",
    "    \n",
    "    # Store embeddings in the combined dataframe\n",
    "    all_incidents['embedding'] = embeddings.tolist()\n",
    "    \n",
    "    # Also store in original dataframes for convenience\n",
    "    # Split back into reference and other\n",
    "    reference_mask = all_incidents['dataset_type'] == 'reference'\n",
    "    reference_with_embeddings = all_incidents[reference_mask].copy()\n",
    "    other_with_embeddings = all_incidents[~reference_mask].copy()\n",
    "    \n",
    "    # Store embeddings in original dataframes\n",
    "    if len(reference_with_embeddings) > 0:\n",
    "        reference_final['embedding'] = reference_with_embeddings['embedding'].values\n",
    "    \n",
    "    print(\"\\n‚úÖ Embeddings generated and stored for all incidents!\")\n",
    "    print(f\"   Use 'all_incidents' dataframe for full analysis\")\n",
    "    print(f\"   Use 'reference_final' for reference dataset with embeddings\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping embeddings generation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Validate Quality Scores with Semantic Similarity\n",
    "\n",
    "**What we're doing:** Checking if incidents with similar quality scores are semantically closer to each other.\n",
    "\n",
    "**Why this matters:**\n",
    "- **Validates our scoring**: If quality scores are meaningful, similar scores should mean similar content\n",
    "- **Quality assurance**: Confirms that our filtering criteria make sense\n",
    "- **Understanding relationships**: See how close notes cluster based on quality\n",
    "\n",
    "**What we'll check:**\n",
    "- Incidents with similar `info_score_close_notes` should be semantically similar\n",
    "- High-quality close notes (reference dataset) should cluster together\n",
    "- This validates that our quality criteria capture meaningful differences\n",
    "\n",
    "**Note:** This analysis uses ALL incidents (both reference and other) to get a complete picture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate quality scores using semantic similarity\n",
    "# Check if incidents with similar quality scores are semantically closer\n",
    "\n",
    "if EMBEDDINGS_AVAILABLE and 'embedding' in all_incidents.columns:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"VALIDATING QUALITY SCORES WITH SEMANTIC SIMILARITY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Convert embeddings to numpy array\n",
    "    embedding_array = np.array(all_incidents['embedding'].tolist())\n",
    "    \n",
    "    # Calculate pairwise cosine similarities (measures how similar each pair is)\n",
    "    print(\"\\nCalculating semantic similarities between all incidents...\")\n",
    "    similarity_matrix = cosine_similarity(embedding_array)\n",
    "    \n",
    "    # Get quality scores if available\n",
    "    if 'info_score' in all_incidents.columns:\n",
    "        quality_scores = all_incidents['info_score'].values\n",
    "    elif 'info_score_close_notes' in all_incidents.columns:\n",
    "        quality_scores = all_incidents['info_score_close_notes'].values\n",
    "    else:\n",
    "        # Try to get from original df\n",
    "        quality_scores = None\n",
    "        print(\"‚ö†Ô∏è Quality scores not found in all_incidents\")\n",
    "    \n",
    "    if quality_scores is not None:\n",
    "        print(\"\\nüìä Validation: Do similar quality scores mean semantic similarity?\")\n",
    "        \n",
    "        # Group incidents by quality score ranges\n",
    "        score_ranges = [\n",
    "            (0.8, 1.0, \"High (0.8-1.0)\"),\n",
    "            (0.6, 0.8, \"Medium-High (0.6-0.8)\"),\n",
    "            (0.4, 0.6, \"Medium (0.4-0.6)\"),\n",
    "            (0.0, 0.4, \"Low (0.0-0.4)\")\n",
    "        ]\n",
    "        \n",
    "        within_group_similarities = []\n",
    "        between_group_similarities = []\n",
    "        \n",
    "        for low1, high1, label1 in score_ranges:\n",
    "            mask1 = (quality_scores >= low1) & (quality_scores < high1)\n",
    "            if mask1.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            indices1 = np.where(mask1)[0]\n",
    "            \n",
    "            # Within-group similarity (incidents in same score range)\n",
    "            if len(indices1) > 1:\n",
    "                within_sim = similarity_matrix[np.ix_(indices1, indices1)]\n",
    "                # Get upper triangle (avoid diagonal and duplicates)\n",
    "                within_sim_flat = within_sim[np.triu_indices(len(indices1), k=1)]\n",
    "                within_group_similarities.extend(within_sim_flat.tolist())\n",
    "            \n",
    "            # Between-group similarity (incidents in different score ranges)\n",
    "            for low2, high2, label2 in score_ranges:\n",
    "                if low2 <= low1:  # Avoid duplicate comparisons\n",
    "                    continue\n",
    "                mask2 = (quality_scores >= low2) & (quality_scores < high2)\n",
    "                if mask2.sum() == 0:\n",
    "                    continue\n",
    "                \n",
    "                indices2 = np.where(mask2)[0]\n",
    "                between_sim = similarity_matrix[np.ix_(indices1, indices2)]\n",
    "                between_group_similarities.extend(between_sim.flatten().tolist())\n",
    "        \n",
    "        if within_group_similarities and between_group_similarities:\n",
    "            avg_within = np.mean(within_group_similarities)\n",
    "            avg_between = np.mean(between_group_similarities)\n",
    "            \n",
    "            print(f\"\\n   Average similarity WITHIN same score range: {avg_within:.3f}\")\n",
    "            print(f\"   Average similarity BETWEEN different score ranges: {avg_between:.3f}\")\n",
    "            print(f\"   Difference: {avg_within - avg_between:.3f}\")\n",
    "            \n",
    "            if avg_within > avg_between:\n",
    "                print(f\"\\n   ‚úÖ Validation PASSED: Similar scores = similar content\")\n",
    "                print(f\"      (Incidents with similar quality scores are semantically closer)\")\n",
    "            else:\n",
    "                print(f\"\\n   ‚ö†Ô∏è Validation WARNING: Similar scores don't mean similar content\")\n",
    "                print(f\"      (This might indicate issues with quality scoring)\")\n",
    "    \n",
    "    # Category analysis (if available)\n",
    "    if 'category' in all_incidents.columns:\n",
    "        print(\"\\nüìä Category-wise Semantic Similarity:\")\n",
    "        categories = all_incidents['category'].unique()\n",
    "    \n",
    "    for cat in categories:\n",
    "        cat_indices = reference_final[reference_final['category'] == cat].index\n",
    "        if len(cat_indices) > 1:\n",
    "            # Get similarity matrix for this category\n",
    "            cat_similarity = similarity_matrix[np.ix_(cat_indices, cat_indices)]\n",
    "            # Exclude diagonal (self-similarity = 1.0)\n",
    "            mask = np.ones_like(cat_similarity, dtype=bool)\n",
    "            np.fill_diagonal(mask, False)\n",
    "            within_similarity = cat_similarity[mask].mean()\n",
    "            print(f\"   {cat}: Mean within-category similarity: {within_similarity:.3f} (n={len(cat_indices)})\")\n",
    "    \n",
    "    # Analyze between-category similarity\n",
    "    if len(categories) > 1:\n",
    "        print(f\"\\nüìä Between-Category Semantic Similarity:\")\n",
    "        for i, cat1 in enumerate(categories):\n",
    "            for cat2 in categories[i+1:]:\n",
    "                cat1_indices = reference_final[reference_final['category'] == cat1].index\n",
    "                cat2_indices = reference_final[reference_final['category'] == cat2].index\n",
    "                \n",
    "                # Get similarity between categories\n",
    "                between_similarity = similarity_matrix[np.ix_(cat1_indices, cat2_indices)].mean()\n",
    "                print(f\"   {cat1} ‚Üî {cat2}: {between_similarity:.3f}\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nüìä Overall Statistics:\")\n",
    "    print(f\"   Mean similarity (all pairs): {similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)].mean():.3f}\")\n",
    "    print(f\"   Min similarity: {similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)].min():.3f}\")\n",
    "    print(f\"   Max similarity: {similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)].max():.3f}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Embeddings not available or category column missing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualize All Incidents with t-SNE\n",
    "\n",
    "**What we're doing:** Visualizing all incidents in 2D space to see how they cluster based on quality scores.\n",
    "\n",
    "**Why this matters:**\n",
    "- **Visual validation**: See if high-quality incidents cluster together\n",
    "- **Understand patterns**: Identify groups of similar incidents\n",
    "- **Quality assurance**: Verify that our scoring creates meaningful groupings\n",
    "\n",
    "**What you'll see:**\n",
    "- 2D visualization of all incidents (both reference and other)\n",
    "- Color-coded by quality score to see if similar scores cluster together\n",
    "- Category information to see if incident types group together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDINGS_AVAILABLE and 'embedding' in all_incidents.columns:\n",
    "    try:\n",
    "        from sklearn.manifold import TSNE\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        print(\"Generating t-SNE visualization...\")\n",
    "        \n",
    "        # Prepare embeddings for ALL incidents\n",
    "        embedding_array = np.array(all_incidents['embedding'].tolist())\n",
    "        \n",
    "        # Apply t-SNE (reduce to 2D for visualization)\n",
    "        print(\"   Running t-SNE for all incidents (this may take a moment)...\")\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_incidents)-1), max_iter=1000)\n",
    "        embeddings_2d = tsne.fit_transform(embedding_array)\n",
    "        \n",
    "        # Create visualization with 2 plots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "        \n",
    "        # Get quality scores\n",
    "        if 'info_score' in all_incidents.columns:\n",
    "            quality_scores = all_incidents['info_score'].values\n",
    "        elif 'info_score_close_notes' in all_incidents.columns:\n",
    "            quality_scores = all_incidents['info_score_close_notes'].values\n",
    "        else:\n",
    "            quality_scores = None\n",
    "        \n",
    "        # Plot 1: Color by dataset type (reference vs other)\n",
    "        if 'dataset_type' in all_incidents.columns:\n",
    "            ref_mask = all_incidents['dataset_type'] == 'reference'\n",
    "            axes[0].scatter(\n",
    "                embeddings_2d[ref_mask, 0],\n",
    "                embeddings_2d[ref_mask, 1],\n",
    "                label='Reference (High-Quality)',\n",
    "                color='green',\n",
    "                alpha=0.6,\n",
    "                s=100\n",
    "            )\n",
    "            axes[0].scatter(\n",
    "                embeddings_2d[~ref_mask, 0],\n",
    "                embeddings_2d[~ref_mask, 1],\n",
    "                label='Other Incidents',\n",
    "                color='orange',\n",
    "                alpha=0.6,\n",
    "                s=100\n",
    "            )\n",
    "            axes[0].set_title('t-SNE: Reference vs Other Incidents', fontsize=14, fontweight='bold')\n",
    "            axes[0].set_xlabel('t-SNE Dimension 1', fontsize=11)\n",
    "            axes[0].set_ylabel('t-SNE Dimension 2', fontsize=11)\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Color by quality score\n",
    "        if quality_scores is not None:\n",
    "            scatter = axes[1].scatter(\n",
    "                embeddings_2d[:, 0],\n",
    "                embeddings_2d[:, 1],\n",
    "                c=quality_scores,\n",
    "                cmap='viridis',\n",
    "                alpha=0.6,\n",
    "                s=100\n",
    "            )\n",
    "            axes[1].set_title('t-SNE: Colored by Quality Score', fontsize=14, fontweight='bold')\n",
    "            axes[1].set_xlabel('t-SNE Dimension 1', fontsize=11)\n",
    "            axes[1].set_ylabel('t-SNE Dimension 2', fontsize=11)\n",
    "            plt.colorbar(scatter, ax=axes[1], label='Quality Score')\n",
    "            axes[1].grid(alpha=0.3)\n",
    "        else:\n",
    "            # If no quality score, just show all points\n",
    "            axes[1].scatter(\n",
    "                embeddings_2d[:, 0],\n",
    "                embeddings_2d[:, 1],\n",
    "                alpha=0.6,\n",
    "                s=100,\n",
    "                color='steelblue'\n",
    "            )\n",
    "            axes[1].set_title('t-SNE: All Close Notes', fontsize=14, fontweight='bold')\n",
    "            axes[1].set_xlabel('t-SNE Dimension 1', fontsize=11)\n",
    "            axes[1].set_ylabel('t-SNE Dimension 2', fontsize=11)\n",
    "            axes[1].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Store 2D embeddings in all_incidents for potential future use\n",
    "        all_incidents['tsne_x'] = embeddings_2d[:, 0]\n",
    "        all_incidents['tsne_y'] = embeddings_2d[:, 1]\n",
    "        \n",
    "        print(\"\\nüí° Interpretation:\")\n",
    "        print(\"   - If high-quality incidents cluster together ‚Üí Quality scores are meaningful\")\n",
    "        print(\"   - If reference incidents are close ‚Üí Our filtering worked well\")\n",
    "        print(\"   - Color gradient in Plot 2 shows quality score distribution\")\n",
    "        \n",
    "        print(\"‚úÖ t-SNE visualization complete!\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è scikit-learn not available. Install with: pip install scikit-learn\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error generating t-SNE visualization: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Embeddings not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Embeddings for Future Use\n",
    "\n",
    "Save the embeddings (or embeddings metadata) for use in evaluation notebooks. Since embeddings are large, we'll save them separately or include metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if EMBEDDINGS_AVAILABLE and 'embedding' in reference_final.columns:\n",
    "    print(\"=\"*80)\n",
    "    print(\"SAVING EMBEDDINGS (OPTIONAL)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Save embeddings as numpy array (more efficient than storing in CSV)\n",
    "    embeddings_path = data_dir / \"gt_close_notes_embeddings.npy\"\n",
    "    embedding_array = np.array(reference_final['embedding'].tolist())\n",
    "    np.save(embeddings_path, embedding_array)\n",
    "    print(f\"‚úÖ Saved embeddings array to: {embeddings_path}\")\n",
    "    print(f\"   Shape: {embedding_array.shape}\")\n",
    "    print(f\"   Size: {embeddings_path.stat().st_size / 1024:.1f} KB\")\n",
    "    \n",
    "    # Save mapping between indices and incident numbers\n",
    "    import os\n",
    "    current_model = os.getenv('EMBEDDING_MODEL', 'BAAI/bge-m3')\n",
    "    embeddings_metadata = {\n",
    "        'indices': reference_final['number'].tolist(),\n",
    "        'model_name': current_model,\n",
    "        'embedding_dimension': embedding_array.shape[1],\n",
    "        'num_samples': len(embedding_array)\n",
    "    }\n",
    "    \n",
    "    metadata_path = data_dir / \"gt_close_notes_embeddings_metadata.pkl\"\n",
    "    with open(metadata_path, 'wb') as f:\n",
    "        pickle.dump(embeddings_metadata, f)\n",
    "    print(f\"‚úÖ Saved embeddings metadata to: {metadata_path}\")\n",
    "    \n",
    "    # Save updated CSV with t-SNE coordinates (if available)\n",
    "    if 'tsne_x' in reference_final.columns:\n",
    "        # Save a version without embeddings column (too large for CSV)\n",
    "        reference_final_export = reference_final.drop(columns=['embedding']).copy()\n",
    "        output_path_with_coords = data_dir / \"gt_close_notes_with_coords.csv\"\n",
    "        reference_final_export.to_csv(output_path_with_coords, index=False)\n",
    "        print(f\"‚úÖ Saved CSV with t-SNE coordinates to: {output_path_with_coords}\")\n",
    "    \n",
    "    print(\"\\nüí° To load embeddings later:\")\n",
    "    print(\"   embeddings = np.load('data/gt_close_notes_embeddings.npy')\")\n",
    "    print(\"   with open('data/gt_close_notes_embeddings_metadata.pkl', 'rb') as f:\")\n",
    "    print(\"       metadata = pickle.load(f)\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Embeddings not available - skipping save\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
