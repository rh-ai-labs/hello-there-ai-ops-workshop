{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Create Ground Truth Dataset\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. **Load** the prepared dataset from notebook 01\n",
    "2. **Filter** high-quality `close_notes` based on quality scores\n",
    "3. **Select** diverse examples across categories and subcategories\n",
    "4. **Create** a ground truth dataset for evaluation\n",
    "5. **Save** the ground truth dataset for use in evaluation notebooks\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Ground Truth Overview\n",
    "\n",
    "**Purpose:** Establish a reference dataset of high-quality close notes that will serve as \"ground truth\" for evaluating LLM-generated enrichments.\n",
    "\n",
    "**Selection Criteria:**\n",
    "- `info_score_close_notes` ‚â• 0.8 (high quality)\n",
    "- `info_score_poor_close_notes` ‚â§ 0.1 (low generic content)\n",
    "- Complete and informative `close_notes` text\n",
    "- Avoid generic phrases like \"No changes noted\" or \"Issue resolved\"\n",
    "- Diverse representation across categories, subcategories, and contact types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(str(Path(\"../src\").resolve()))\n",
    "\n",
    "from utils import load_incident_dataset, calculate_basic_stats\n",
    "\n",
    "# Set up plotting style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn')\n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Prepared Dataset\n",
    "\n",
    "Load the dataset prepared in notebook 01, which already contains incidents with `close_notes` (ground truth available).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared dataset from notebook 01\n",
    "data_dir = Path(\"../data\")\n",
    "prepared_path = data_dir / \"incidents_prepared.csv\"\n",
    "\n",
    "if prepared_path.exists():\n",
    "    df = pd.read_csv(prepared_path)\n",
    "    print(f\"‚úÖ Loaded prepared dataset: {len(df)} records\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Prepared dataset not found. Loading from Hugging Face...\")\n",
    "    df = load_incident_dataset(sample_size=200, random_state=42)\n",
    "    # Filter for records with close_notes\n",
    "    if 'close_notes' in df.columns:\n",
    "        df = df[df['close_notes'].notna()].copy()\n",
    "    print(f\"‚úÖ Loaded dataset: {len(df)} records with close_notes\")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examine Data Quality Scores\n",
    "\n",
    "Check the distribution of quality scores to understand the dataset characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check quality score columns\n",
    "print(\"=\"*80)\n",
    "print(\"QUALITY SCORES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'info_score_close_notes' in df.columns:\n",
    "    print(f\"\\nüìä info_score_close_notes:\")\n",
    "    print(f\"   Mean: {df['info_score_close_notes'].mean():.3f}\")\n",
    "    print(f\"   Median: {df['info_score_close_notes'].median():.3f}\")\n",
    "    print(f\"   Min: {df['info_score_close_notes'].min():.3f}\")\n",
    "    print(f\"   Max: {df['info_score_close_notes'].max():.3f}\")\n",
    "    print(f\"   Records with score ‚â• 0.8: {(df['info_score_close_notes'] >= 0.8).sum()} ({(df['info_score_close_notes'] >= 0.8).sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "if 'info_score_poor_close_notes' in df.columns:\n",
    "    print(f\"\\nüìä info_score_poor_close_notes:\")\n",
    "    print(f\"   Mean: {df['info_score_poor_close_notes'].mean():.3f}\")\n",
    "    print(f\"   Median: {df['info_score_poor_close_notes'].median():.3f}\")\n",
    "    print(f\"   Records with score ‚â§ 0.1: {(df['info_score_poor_close_notes'] <= 0.1).sum()} ({(df['info_score_poor_close_notes'] <= 0.1).sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Visualize score distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "if 'info_score_close_notes' in df.columns:\n",
    "    axes[0].hist(df['info_score_close_notes'].dropna(), bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[0].axvline(0.8, color='red', linestyle='--', label='Threshold (‚â•0.8)')\n",
    "    axes[0].set_xlabel('Info Score (close_notes)', fontsize=11)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[0].set_title('Close Notes Quality Score Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    axes[0].legend()\n",
    "\n",
    "if 'info_score_poor_close_notes' in df.columns:\n",
    "    axes[1].hist(df['info_score_poor_close_notes'].dropna(), bins=20, edgecolor='black', alpha=0.7, color='coral')\n",
    "    axes[1].axvline(0.1, color='red', linestyle='--', label='Threshold (‚â§0.1)')\n",
    "    axes[1].set_xlabel('Info Score (poor_close_notes)', fontsize=11)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[1].set_title('Poor Close Notes Score Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter High-Quality Close Notes\n",
    "\n",
    "Apply selection criteria to identify high-quality close notes:\n",
    "- `info_score_close_notes` ‚â• 0.8\n",
    "- `info_score_poor_close_notes` ‚â§ 0.1\n",
    "- Complete and informative text\n",
    "- Exclude generic phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generic phrases to exclude\n",
    "GENERIC_PHRASES = [\n",
    "    'no changes noted',\n",
    "    'issue resolved',\n",
    "    'resolved',\n",
    "    'closed',\n",
    "    'no further action',\n",
    "    'resolved per user',\n",
    "    'user confirmed resolved'\n",
    "]\n",
    "\n",
    "def is_generic_close_note(text):\n",
    "    \"\"\"Check if close note contains only generic phrases.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return True\n",
    "    \n",
    "    text_lower = text.lower().strip()\n",
    "    \n",
    "    # Check if text is too short (likely generic)\n",
    "    if len(text_lower) < 50:\n",
    "        return True\n",
    "    \n",
    "    # Check if text contains only generic phrases\n",
    "    words = set(text_lower.split())\n",
    "    generic_words = set()\n",
    "    for phrase in GENERIC_PHRASES:\n",
    "        generic_words.update(phrase.split())\n",
    "    \n",
    "    # If most words are generic, likely a generic note\n",
    "    if len(words) <= 5 and words.issubset(generic_words):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Apply filters\n",
    "print(\"Applying quality filters...\")\n",
    "print(f\"Initial records: {len(df)}\")\n",
    "\n",
    "# Filter 1: Must have close_notes\n",
    "df_filtered = df[df['close_notes'].notna()].copy()\n",
    "print(f\"After filtering for close_notes: {len(df_filtered)}\")\n",
    "\n",
    "# Filter 2: High quality score\n",
    "if 'info_score_close_notes' in df_filtered.columns:\n",
    "    df_filtered = df_filtered[df_filtered['info_score_close_notes'] >= 0.8].copy()\n",
    "    print(f\"After filtering for info_score_close_notes ‚â• 0.8: {len(df_filtered)}\")\n",
    "\n",
    "# Filter 3: Low poor quality score\n",
    "if 'info_score_poor_close_notes' in df_filtered.columns:\n",
    "    df_filtered = df_filtered[df_filtered['info_score_poor_close_notes'] <= 0.1].copy()\n",
    "    print(f\"After filtering for info_score_poor_close_notes ‚â§ 0.1: {len(df_filtered)}\")\n",
    "\n",
    "# Filter 4: Exclude generic notes\n",
    "df_filtered['is_generic'] = df_filtered['close_notes'].apply(is_generic_close_note)\n",
    "df_filtered = df_filtered[~df_filtered['is_generic']].copy()\n",
    "print(f\"After excluding generic notes: {len(df_filtered)}\")\n",
    "\n",
    "# Filter 5: Minimum text length (ensure informative)\n",
    "df_filtered['close_notes_length'] = df_filtered['close_notes'].astype(str).str.len()\n",
    "df_filtered = df_filtered[df_filtered['close_notes_length'] >= 100].copy()\n",
    "print(f\"After filtering for minimum length (‚â•100 chars): {len(df_filtered)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Final filtered dataset: {len(df_filtered)} high-quality records\")\n",
    "print(f\"   Reduction: {len(df) - len(df_filtered)} records filtered out ({((len(df) - len(df_filtered))/len(df)*100):.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze diversity\n",
    "print(\"=\"*80)\n",
    "print(\"DIVERSITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'category' in df_filtered.columns:\n",
    "    print(f\"\\nüìä Categories:\")\n",
    "    category_counts = df_filtered['category'].value_counts()\n",
    "    for cat, count in category_counts.items():\n",
    "        print(f\"   {cat}: {count} ({count/len(df_filtered)*100:.1f}%)\")\n",
    "\n",
    "if 'subcategory' in df_filtered.columns:\n",
    "    print(f\"\\nüìã Subcategories:\")\n",
    "    subcat_counts = df_filtered['subcategory'].value_counts()\n",
    "    print(f\"   Total unique: {df_filtered['subcategory'].nunique()}\")\n",
    "    for subcat, count in subcat_counts.head(10).items():\n",
    "        print(f\"   {subcat}: {count}\")\n",
    "\n",
    "if 'contact_type' in df_filtered.columns:\n",
    "    print(f\"\\nüìû Contact Types:\")\n",
    "    contact_counts = df_filtered['contact_type'].value_counts()\n",
    "    for contact, count in contact_counts.items():\n",
    "        print(f\"   {contact}: {count} ({count/len(df_filtered)*100:.1f}%)\")\n",
    "\n",
    "# Visualize diversity\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "if 'category' in df_filtered.columns:\n",
    "    category_counts = df_filtered['category'].value_counts()\n",
    "    axes[0].bar(range(len(category_counts)), category_counts.values, \n",
    "                color=sns.color_palette(\"husl\", len(category_counts)))\n",
    "    axes[0].set_xticks(range(len(category_counts)))\n",
    "    axes[0].set_xticklabels(category_counts.index, rotation=45, ha='right')\n",
    "    axes[0].set_ylabel('Count', fontsize=10)\n",
    "    axes[0].set_title('Category Distribution (Filtered)', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(category_counts.values):\n",
    "        axes[0].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "if 'subcategory' in df_filtered.columns:\n",
    "    subcat_counts = df_filtered['subcategory'].value_counts().head(10)\n",
    "    axes[1].barh(range(len(subcat_counts)), subcat_counts.values,\n",
    "                color=sns.color_palette(\"viridis\", len(subcat_counts)))\n",
    "    axes[1].set_yticks(range(len(subcat_counts)))\n",
    "    axes[1].set_yticklabels(subcat_counts.index)\n",
    "    axes[1].set_xlabel('Count', fontsize=10)\n",
    "    axes[1].set_title('Top 10 Subcategories (Filtered)', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "if 'contact_type' in df_filtered.columns:\n",
    "    contact_counts = df_filtered['contact_type'].value_counts()\n",
    "    axes[2].bar(contact_counts.index, contact_counts.values,\n",
    "               color=sns.color_palette(\"muted\", len(contact_counts)))\n",
    "    axes[2].set_ylabel('Count', fontsize=10)\n",
    "    axes[2].set_title('Contact Type Distribution (Filtered)', fontsize=12, fontweight='bold')\n",
    "    axes[2].grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(contact_counts.values):\n",
    "        axes[2].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensure Balanced Sampling\n",
    "\n",
    "If needed, sample balanced examples across different categories to ensure diversity in the ground truth dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy: Sample balanced examples if one category dominates\n",
    "TARGET_SAMPLES_PER_CATEGORY = 20  # Adjust based on dataset size\n",
    "MIN_SAMPLES_PER_CATEGORY = 5     # Minimum samples per category\n",
    "\n",
    "print(\"Applying balanced sampling strategy...\")\n",
    "\n",
    "if 'category' in df_filtered.columns:\n",
    "    sampled_records = []\n",
    "    \n",
    "    for category in df_filtered['category'].unique():\n",
    "        category_df = df_filtered[df_filtered['category'] == category].copy()\n",
    "        \n",
    "        if len(category_df) >= TARGET_SAMPLES_PER_CATEGORY:\n",
    "            # Sample TARGET_SAMPLES_PER_CATEGORY records\n",
    "            sampled = category_df.sample(\n",
    "                min(TARGET_SAMPLES_PER_CATEGORY, len(category_df)),\n",
    "                random_state=42\n",
    "            )\n",
    "        elif len(category_df) >= MIN_SAMPLES_PER_CATEGORY:\n",
    "            # Take all available if less than target but above minimum\n",
    "            sampled = category_df\n",
    "        else:\n",
    "            # Skip categories with too few samples\n",
    "            print(f\"   ‚ö†Ô∏è Skipping {category}: only {len(category_df)} samples\")\n",
    "            continue\n",
    "        \n",
    "        sampled_records.append(sampled)\n",
    "        print(f\"   ‚úÖ {category}: {len(sampled)} samples\")\n",
    "    \n",
    "    df_ground_truth = pd.concat(sampled_records, ignore_index=True)\n",
    "    print(f\"\\n‚úÖ Balanced ground truth dataset: {len(df_ground_truth)} records\")\n",
    "else:\n",
    "    # If no category column, use all filtered records\n",
    "    df_ground_truth = df_filtered.copy()\n",
    "    print(f\"\\n‚úÖ Using all filtered records: {len(df_ground_truth)} records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Ground Truth Dataset Structure\n",
    "\n",
    "Prepare the final dataset with the required structure: `number`, `content`, `close_notes_ref`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ground truth dataset with required structure\n",
    "gt_dataset = pd.DataFrame({\n",
    "    'number': df_ground_truth['number'].values,\n",
    "    'content': df_ground_truth['content'].values,\n",
    "    'close_notes_ref': df_ground_truth['close_notes'].values\n",
    "})\n",
    "\n",
    "# Add optional metadata columns for reference\n",
    "if 'category' in df_ground_truth.columns:\n",
    "    gt_dataset['category'] = df_ground_truth['category'].values\n",
    "if 'subcategory' in df_ground_truth.columns:\n",
    "    gt_dataset['subcategory'] = df_ground_truth['subcategory'].values\n",
    "if 'contact_type' in df_ground_truth.columns:\n",
    "    gt_dataset['contact_type'] = df_ground_truth['contact_type'].values\n",
    "if 'info_score_close_notes' in df_ground_truth.columns:\n",
    "    gt_dataset['info_score'] = df_ground_truth['info_score_close_notes'].values\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GROUND TRUTH DATASET STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal records: {len(gt_dataset)}\")\n",
    "print(f\"\\nColumns: {list(gt_dataset.columns)}\")\n",
    "print(f\"\\nFirst few records:\")\n",
    "print(gt_dataset.head())\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validate Ground Truth Quality\n",
    "\n",
    "Perform final validation checks on the ground truth dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation checks\n",
    "print(\"=\"*80)\n",
    "print(\"VALIDATION CHECKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check 1: No missing values in required columns\n",
    "missing_content = gt_dataset['content'].isna().sum()\n",
    "missing_close_notes = gt_dataset['close_notes_ref'].isna().sum()\n",
    "missing_number = gt_dataset['number'].isna().sum()\n",
    "\n",
    "print(f\"\\n‚úÖ Missing values check:\")\n",
    "print(f\"   Missing 'number': {missing_number}\")\n",
    "print(f\"   Missing 'content': {missing_content}\")\n",
    "print(f\"   Missing 'close_notes_ref': {missing_close_notes}\")\n",
    "\n",
    "# Check 2: Text length distribution\n",
    "gt_dataset['content_length'] = gt_dataset['content'].astype(str).str.len()\n",
    "gt_dataset['close_notes_length'] = gt_dataset['close_notes_ref'].astype(str).str.len()\n",
    "\n",
    "print(f\"\\n‚úÖ Text length statistics:\")\n",
    "print(f\"   Content - Mean: {gt_dataset['content_length'].mean():.0f}, Median: {gt_dataset['content_length'].median():.0f}\")\n",
    "print(f\"   Close Notes - Mean: {gt_dataset['close_notes_length'].mean():.0f}, Median: {gt_dataset['close_notes_length'].median():.0f}\")\n",
    "\n",
    "# Check 3: Ensure no duplicates\n",
    "duplicates = gt_dataset['number'].duplicated().sum()\n",
    "print(f\"\\n‚úÖ Duplicate check:\")\n",
    "print(f\"   Duplicate incident numbers: {duplicates}\")\n",
    "\n",
    "# Check 4: Quality score distribution (if available)\n",
    "if 'info_score' in gt_dataset.columns:\n",
    "    print(f\"\\n‚úÖ Quality score distribution:\")\n",
    "    print(f\"   Mean: {gt_dataset['info_score'].mean():.3f}\")\n",
    "    print(f\"   Min: {gt_dataset['info_score'].min():.3f}\")\n",
    "    print(f\"   Max: {gt_dataset['info_score'].max():.3f}\")\n",
    "    print(f\"   All scores ‚â• 0.8: {(gt_dataset['info_score'] >= 0.8).all()}\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Display Sample Examples\n",
    "\n",
    "Show a few examples of high-quality ground truth records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample examples\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE GROUND TRUTH EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in gt_dataset.head(3).iterrows():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {idx + 1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nüìã Incident Number: {row['number']}\")\n",
    "    if 'category' in row:\n",
    "        print(f\"üè∑Ô∏è  Category: {row['category']}\")\n",
    "    if 'subcategory' in row:\n",
    "        print(f\"üè∑Ô∏è  Subcategory: {row['subcategory']}\")\n",
    "    if 'info_score' in row:\n",
    "        print(f\"‚≠ê Quality Score: {row['info_score']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüìù Original Content (Input):\")\n",
    "    print(f\"   {row['content'][:300]}...\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Reference Close Notes (Ground Truth):\")\n",
    "    print(f\"   {row['close_notes_ref'][:400]}...\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Ground Truth Dataset\n",
    "\n",
    "Save the final ground truth dataset to CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final dataset (remove helper columns)\n",
    "gt_final = gt_dataset[[\n",
    "    'number',\n",
    "    'content',\n",
    "    'close_notes_ref'\n",
    "]].copy()\n",
    "\n",
    "# Add metadata columns if they exist\n",
    "for col in ['category', 'subcategory', 'contact_type', 'info_score']:\n",
    "    if col in gt_dataset.columns:\n",
    "        gt_final[col] = gt_dataset[col]\n",
    "\n",
    "# Save to CSV\n",
    "output_path = data_dir / \"gt_close_notes.csv\"\n",
    "gt_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GROUND TRUTH DATASET SAVED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úÖ Saved to: {output_path}\")\n",
    "print(f\"   Total records: {len(gt_final)}\")\n",
    "print(f\"   File size: {output_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"\\nColumns saved:\")\n",
    "for col in gt_final.columns:\n",
    "    print(f\"   - {col}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary Statistics\n",
    "\n",
    "Generate final summary statistics about the ground truth dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Ground Truth Dataset Statistics:\")\n",
    "print(f\"   Total records: {len(gt_final)}\")\n",
    "\n",
    "if 'category' in gt_final.columns:\n",
    "    print(f\"\\nüìã Category Distribution:\")\n",
    "    for cat, count in gt_final['category'].value_counts().items():\n",
    "        print(f\"   {cat}: {count} ({count/len(gt_final)*100:.1f}%)\")\n",
    "\n",
    "if 'subcategory' in gt_final.columns:\n",
    "    print(f\"\\nüìã Subcategory Distribution:\")\n",
    "    print(f\"   Unique subcategories: {gt_final['subcategory'].nunique()}\")\n",
    "    for subcat, count in gt_final['subcategory'].value_counts().head(5).items():\n",
    "        print(f\"   {subcat}: {count}\")\n",
    "\n",
    "if 'contact_type' in gt_final.columns:\n",
    "    print(f\"\\nüìû Contact Type Distribution:\")\n",
    "    for contact, count in gt_final['contact_type'].value_counts().items():\n",
    "        print(f\"   {contact}: {count} ({count/len(gt_final)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ground truth dataset ready for evaluation!\")\n",
    "print(f\"   Next step: Use this dataset in evaluation notebooks\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Embeddings for Semantic Analysis\n",
    "\n",
    "Generate embeddings for all close notes to enable semantic similarity analysis and visualization. This will be useful for:\n",
    "- Understanding semantic similarity between categories\n",
    "- Identifying clusters of similar incidents\n",
    "- Preparing for reference-based evaluation using semantic similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import embedding library\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sentence_transformers.util import cos_sim\n",
    "    EMBEDDINGS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è sentence-transformers not available. Install with: pip install sentence-transformers\")\n",
    "    EMBEDDINGS_AVAILABLE = False\n",
    "\n",
    "if EMBEDDINGS_AVAILABLE:\n",
    "    import os\n",
    "    print(\"Loading embedding model...\")\n",
    "    \n",
    "    # Model selection: Use BGE-M3 for multilingual, multi-granularity support\n",
    "    # Can be overridden via EMBEDDING_MODEL environment variable\n",
    "    DEFAULT_MODEL = 'BAAI/bge-m3'  # Multilingual, supports dense/sparse/multi-vector retrieval\n",
    "    # Alternative models:\n",
    "    # - 'BAAI/bge-small-en-v1.5' (faster, English-only)\n",
    "    # - 'BAAI/bge-base-en-v1.5' (slower, higher accuracy, English-only)\n",
    "    # - 'sentence-transformers/all-mpnet-base-v2' (proven alternative, English-only)\n",
    "    \n",
    "    embedding_model_name = os.getenv('EMBEDDING_MODEL', DEFAULT_MODEL)\n",
    "    print(f\"   Using model: {embedding_model_name}\")\n",
    "    \n",
    "    use_flag_embedding = False\n",
    "    try:\n",
    "        # Try sentence-transformers first\n",
    "        model = SentenceTransformer(embedding_model_name, trust_remote_code=True)\n",
    "        embedding_dim = model.get_sentence_embedding_dimension()\n",
    "        print(f\"‚úÖ Model loaded: {embedding_dim}-dimensional embeddings\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading with sentence-transformers: {e}\")\n",
    "        print(\"   Trying FlagEmbedding library...\")\n",
    "        try:\n",
    "            from FlagEmbedding import BGEM3FlagModel\n",
    "            model = BGEM3FlagModel(embedding_model_name)\n",
    "            use_flag_embedding = True\n",
    "            print(f\"‚úÖ Model loaded via FlagEmbedding (BGE-M3)\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è FlagEmbedding not installed. Install with: pip install FlagEmbedding\")\n",
    "            raise\n",
    "    \n",
    "    # Generate embeddings for all close notes\n",
    "    print(\"\\nGenerating embeddings for close notes...\")\n",
    "    close_notes_texts = gt_final['close_notes_ref'].astype(str).tolist()\n",
    "    \n",
    "    if use_flag_embedding:\n",
    "        # FlagEmbedding returns dict with 'dense_vecs', 'sparse', 'colbert_vecs'\n",
    "        output = model.encode(close_notes_texts, return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
    "        embeddings = output['dense_vecs']\n",
    "    else:\n",
    "        embeddings = model.encode(close_notes_texts, show_progress_bar=True, batch_size=32)\n",
    "    \n",
    "    print(f\"‚úÖ Generated embeddings for {len(embeddings)} close notes\")\n",
    "    print(f\"   Embedding dimensions: {embeddings.shape}\")\n",
    "    \n",
    "    # Store embeddings in the dataframe\n",
    "    gt_final['embedding'] = embeddings.tolist()\n",
    "    \n",
    "    print(\"\\n‚úÖ Embeddings generated and stored!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping embeddings generation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Analyze Semantic Similarity Between Categories\n",
    "\n",
    "Compare semantic similarity within and between different categories to understand how semantically distinct different incident types are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDINGS_AVAILABLE and 'embedding' in gt_final.columns and 'category' in gt_final.columns:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"SEMANTIC SIMILARITY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Convert embeddings to numpy array\n",
    "    embedding_array = np.array(gt_final['embedding'].tolist())\n",
    "    \n",
    "    # Calculate pairwise cosine similarities\n",
    "    similarity_matrix = cosine_similarity(embedding_array)\n",
    "    \n",
    "    # Analyze within-category similarity\n",
    "    categories = gt_final['category'].unique()\n",
    "    print(f\"\\nüìä Category-wise Semantic Similarity:\")\n",
    "    \n",
    "    for cat in categories:\n",
    "        cat_indices = gt_final[gt_final['category'] == cat].index\n",
    "        if len(cat_indices) > 1:\n",
    "            # Get similarity matrix for this category\n",
    "            cat_similarity = similarity_matrix[np.ix_(cat_indices, cat_indices)]\n",
    "            # Exclude diagonal (self-similarity = 1.0)\n",
    "            mask = np.ones_like(cat_similarity, dtype=bool)\n",
    "            np.fill_diagonal(mask, False)\n",
    "            within_similarity = cat_similarity[mask].mean()\n",
    "            print(f\"   {cat}: Mean within-category similarity: {within_similarity:.3f} (n={len(cat_indices)})\")\n",
    "    \n",
    "    # Analyze between-category similarity\n",
    "    if len(categories) > 1:\n",
    "        print(f\"\\nüìä Between-Category Semantic Similarity:\")\n",
    "        for i, cat1 in enumerate(categories):\n",
    "            for cat2 in categories[i+1:]:\n",
    "                cat1_indices = gt_final[gt_final['category'] == cat1].index\n",
    "                cat2_indices = gt_final[gt_final['category'] == cat2].index\n",
    "                \n",
    "                # Get similarity between categories\n",
    "                between_similarity = similarity_matrix[np.ix_(cat1_indices, cat2_indices)].mean()\n",
    "                print(f\"   {cat1} ‚Üî {cat2}: {between_similarity:.3f}\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nüìä Overall Statistics:\")\n",
    "    print(f\"   Mean similarity (all pairs): {similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)].mean():.3f}\")\n",
    "    print(f\"   Min similarity: {similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)].min():.3f}\")\n",
    "    print(f\"   Max similarity: {similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)].max():.3f}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Embeddings not available or category column missing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualize Embeddings with t-SNE\n",
    "\n",
    "Visualize the high-dimensional embeddings in 2D space using t-SNE to identify clusters and patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDINGS_AVAILABLE and 'embedding' in gt_final.columns:\n",
    "    try:\n",
    "        from sklearn.manifold import TSNE\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        print(\"Generating t-SNE visualization...\")\n",
    "        \n",
    "        # Prepare embeddings\n",
    "        embedding_array = np.array(gt_final['embedding'].tolist())\n",
    "        \n",
    "        # Apply t-SNE (reduce to 2D for visualization)\n",
    "        print(\"   Running t-SNE (this may take a moment)...\")\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(gt_final)-1), max_iter=1000)\n",
    "        embeddings_2d = tsne.fit_transform(embedding_array)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 2 if 'category' in gt_final.columns else 1, figsize=(16, 6))\n",
    "        if 'category' not in gt_final.columns:\n",
    "            axes = [axes]\n",
    "        \n",
    "        # Plot 1: Color by category (if available)\n",
    "        if 'category' in gt_final.columns:\n",
    "            categories = gt_final['category'].unique()\n",
    "            colors = sns.color_palette(\"husl\", len(categories))\n",
    "            category_color_map = {cat: colors[i] for i, cat in enumerate(categories)}\n",
    "            \n",
    "            for cat in categories:\n",
    "                cat_mask = gt_final['category'] == cat\n",
    "                axes[0].scatter(\n",
    "                    embeddings_2d[cat_mask, 0],\n",
    "                    embeddings_2d[cat_mask, 1],\n",
    "                    label=cat,\n",
    "                    color=category_color_map[cat],\n",
    "                    alpha=0.6,\n",
    "                    s=100\n",
    "                )\n",
    "            \n",
    "            axes[0].set_title('t-SNE Embeddings Colored by Category', fontsize=14, fontweight='bold')\n",
    "            axes[0].set_xlabel('t-SNE Dimension 1', fontsize=11)\n",
    "            axes[0].set_ylabel('t-SNE Dimension 2', fontsize=11)\n",
    "            axes[0].legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            axes[0].grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Color by quality score or all points\n",
    "        if 'info_score' in gt_final.columns:\n",
    "            scatter = axes[1].scatter(\n",
    "                embeddings_2d[:, 0],\n",
    "                embeddings_2d[:, 1],\n",
    "                c=gt_final['info_score'].values,\n",
    "                cmap='viridis',\n",
    "                alpha=0.6,\n",
    "                s=100\n",
    "            )\n",
    "            axes[1].set_title('t-SNE Embeddings Colored by Quality Score', fontsize=14, fontweight='bold')\n",
    "            axes[1].set_xlabel('t-SNE Dimension 1', fontsize=11)\n",
    "            axes[1].set_ylabel('t-SNE Dimension 2', fontsize=11)\n",
    "            plt.colorbar(scatter, ax=axes[1], label='Quality Score')\n",
    "            axes[1].grid(alpha=0.3)\n",
    "        else:\n",
    "            # If no quality score, just show all points\n",
    "            axes[1].scatter(\n",
    "                embeddings_2d[:, 0],\n",
    "                embeddings_2d[:, 1],\n",
    "                alpha=0.6,\n",
    "                s=100,\n",
    "                color='steelblue'\n",
    "            )\n",
    "            axes[1].set_title('t-SNE Embeddings - All Close Notes', fontsize=14, fontweight='bold')\n",
    "            axes[1].set_xlabel('t-SNE Dimension 1', fontsize=11)\n",
    "            axes[1].set_ylabel('t-SNE Dimension 2', fontsize=11)\n",
    "            axes[1].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Store 2D embeddings for potential future use\n",
    "        gt_final['tsne_x'] = embeddings_2d[:, 0]\n",
    "        gt_final['tsne_y'] = embeddings_2d[:, 1]\n",
    "        \n",
    "        print(\"‚úÖ t-SNE visualization complete!\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è scikit-learn not available. Install with: pip install scikit-learn\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error generating t-SNE visualization: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Embeddings not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Embeddings for Future Use\n",
    "\n",
    "Save the embeddings (or embeddings metadata) for use in evaluation notebooks. Since embeddings are large, we'll save them separately or include metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if EMBEDDINGS_AVAILABLE and 'embedding' in gt_final.columns:\n",
    "    print(\"=\"*80)\n",
    "    print(\"SAVING EMBEDDINGS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Save embeddings as numpy array (more efficient than storing in CSV)\n",
    "    embeddings_path = data_dir / \"gt_close_notes_embeddings.npy\"\n",
    "    embedding_array = np.array(gt_final['embedding'].tolist())\n",
    "    np.save(embeddings_path, embedding_array)\n",
    "    print(f\"‚úÖ Saved embeddings array to: {embeddings_path}\")\n",
    "    print(f\"   Shape: {embedding_array.shape}\")\n",
    "    print(f\"   Size: {embeddings_path.stat().st_size / 1024:.1f} KB\")\n",
    "    \n",
    "    # Save mapping between indices and incident numbers\n",
    "    import os\n",
    "    current_model = os.getenv('EMBEDDING_MODEL', 'BAAI/bge-m3')\n",
    "    embeddings_metadata = {\n",
    "        'indices': gt_final['number'].tolist(),\n",
    "        'model_name': current_model,\n",
    "        'embedding_dimension': embedding_array.shape[1],\n",
    "        'num_samples': len(embedding_array)\n",
    "    }\n",
    "    \n",
    "    metadata_path = data_dir / \"gt_close_notes_embeddings_metadata.pkl\"\n",
    "    with open(metadata_path, 'wb') as f:\n",
    "        pickle.dump(embeddings_metadata, f)\n",
    "    print(f\"‚úÖ Saved embeddings metadata to: {metadata_path}\")\n",
    "    \n",
    "    # Save updated CSV with t-SNE coordinates (if available)\n",
    "    if 'tsne_x' in gt_final.columns:\n",
    "        # Save a version without embeddings column (too large for CSV)\n",
    "        gt_final_export = gt_final.drop(columns=['embedding']).copy()\n",
    "        output_path_with_coords = data_dir / \"gt_close_notes_with_coords.csv\"\n",
    "        gt_final_export.to_csv(output_path_with_coords, index=False)\n",
    "        print(f\"‚úÖ Saved CSV with t-SNE coordinates to: {output_path_with_coords}\")\n",
    "    \n",
    "    print(\"\\nüí° To load embeddings later:\")\n",
    "    print(\"   embeddings = np.load('data/gt_close_notes_embeddings.npy')\")\n",
    "    print(\"   with open('data/gt_close_notes_embeddings_metadata.pkl', 'rb') as f:\")\n",
    "    print(\"       metadata = pickle.load(f)\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Embeddings not available - skipping save\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
