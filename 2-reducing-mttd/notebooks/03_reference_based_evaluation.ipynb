{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Reference-Based Evaluation with Unitxt\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. **Load** ground truth dataset using Unitxt\n",
    "2. **Perform N-gram comparisons** (BLEU, ROUGE) against reference close notes\n",
    "3. **Perform semantic comparisons** using embedding similarity\n",
    "4. **Implement LLM-as-a-Judge** for structured multi-dimensional evaluation\n",
    "5. **Compare** different models and prompts using comprehensive metrics\n",
    "6. **Visualize** evaluation results and metrics\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "**Phase 2: Reference-Based Evaluation**\n",
    "\n",
    "Using Unitxt to evaluate how well LLM-generated close notes match reference ground truth notes through multiple evaluation approaches:\n",
    "- **N-gram Metrics**: BLEU, ROUGE (ROUGE-1, ROUGE-2, ROUGE-L) for surface-level similarity\n",
    "- **Semantic Similarity**: Embedding-based comparison using BGE-M3 model\n",
    "- **LLM-as-a-Judge**: Structured evaluation across 6 ITSM-specific dimensions\n",
    "\n",
    "**Key Components:**\n",
    "- **Unitxt**: Standardized evaluation framework\n",
    "- **Ground Truth**: `data/gt_close_notes.csv` (26 high-quality examples)\n",
    "- **Embeddings**: Pre-computed BGE-M3 embeddings (`gt_close_notes_embeddings.npy`)\n",
    "- **Evaluation Metrics**: N-gram, semantic, and LLM-based scoring\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Prerequisites\n",
    "\n",
    "**‚ö†Ô∏è REQUIRED:**\n",
    "- **Unitxt installed** (`pip install unitxt` or `uv pip install unitxt`) - **The notebook will not run without it**\n",
    "- Ground truth dataset: `data/gt_close_notes.csv`\n",
    "- Pre-computed embeddings: `data/gt_close_notes_embeddings.npy`\n",
    "\n",
    "**Optional:**\n",
    "- LLM for generating close notes (Ollama or other provider)\n",
    "- LLM for LLM-as-a-Judge evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unitxt if not available\n",
    "try:\n",
    "    import unitxt\n",
    "    print(\"‚úÖ Unitxt is already installed\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Unitxt not found. Installing...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unitxt>=1.0.0\"], \n",
    "                            stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n",
    "        print(\"‚úÖ Unitxt installed successfully\")\n",
    "        # Reload the module after installation\n",
    "        import importlib\n",
    "        importlib.invalidate_caches()\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise ImportError(\n",
    "            \"‚ùå Failed to install Unitxt automatically.\\n\"\n",
    "            \"   Please install it manually with: pip install unitxt\\n\"\n",
    "            \"   Or using uv: uv pip install unitxt\\n\"\n",
    "            \"   The notebook cannot proceed without Unitxt.\"\n",
    "        ) from e\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(str(Path(\"../src\").resolve()))\n",
    "\n",
    "from utils import (\n",
    "    load_ground_truth_embeddings, \n",
    "    compute_semantic_similarity,\n",
    "    find_most_similar_close_note\n",
    ")\n",
    "from prompts import get_prompt_variants\n",
    "\n",
    "# Unitxt for evaluation - REQUIRED\n",
    "try:\n",
    "    import unitxt\n",
    "    from unitxt import load_dataset, Metric\n",
    "    UNITXT_AVAILABLE = True\n",
    "    print(\"‚úÖ Unitxt imported successfully\")\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"‚ùå Unitxt import failed after installation attempt.\\n\"\n",
    "        \"   Please restart the kernel and try again, or install manually:\\n\"\n",
    "        \"   pip install unitxt\\n\"\n",
    "        \"   The notebook cannot proceed without Unitxt.\"\n",
    "    )\n",
    "\n",
    "# Evaluation metrics\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "    ROUGE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ROUGE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è rouge-score not available. Install with: pip install rouge-score\")\n",
    "\n",
    "try:\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import nltk\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    BLEU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    BLEU_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è nltk not available. Install with: pip install nltk\")\n",
    "\n",
    "# Set up plotting style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn')\n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set up paths and configuration for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(\"../data\")\n",
    "GT_DATASET_PATH = DATA_DIR / \"gt_close_notes.csv\"\n",
    "EMBEDDINGS_PATH = DATA_DIR / \"gt_close_notes_embeddings.npy\"\n",
    "EMBEDDINGS_METADATA_PATH = DATA_DIR / \"gt_close_notes_embeddings_metadata.pkl\"\n",
    "\n",
    "# Evaluation settings\n",
    "EVAL_RANDOM_STATE = 42\n",
    "SMOOTHING_FUNCTION = SmoothingFunction().method1 if BLEU_AVAILABLE else None\n",
    "\n",
    "print(\"üìÅ Configuration:\")\n",
    "print(f\"   Data directory: {DATA_DIR}\")\n",
    "print(f\"   Ground truth dataset: {GT_DATASET_PATH}\")\n",
    "print(f\"   Embeddings: {EMBEDDINGS_PATH}\")\n",
    "print(f\"\\n‚úÖ Configuration loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Ground Truth Dataset\n",
    "\n",
    "Load the ground truth close notes dataset that we'll use for evaluation. We'll prepare it in a format suitable for Unitxt evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth dataset\n",
    "if not GT_DATASET_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Ground truth dataset not found at {GT_DATASET_PATH}\")\n",
    "\n",
    "gt_df = pd.read_csv(GT_DATASET_PATH)\n",
    "print(f\"‚úÖ Loaded ground truth dataset: {len(gt_df)} examples\")\n",
    "print(f\"\\nDataset columns: {list(gt_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(gt_df.head())\n",
    "\n",
    "# Show basic statistics\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   Total examples: {len(gt_df)}\")\n",
    "print(f\"   Categories: {gt_df['category'].value_counts().to_dict()}\")\n",
    "print(f\"   Average info_score: {gt_df['info_score'].mean():.2f}\")\n",
    "print(f\"   Info score range: {gt_df['info_score'].min():.2f} - {gt_df['info_score'].max():.2f}\")\n",
    "\n",
    "# Prepare dataset for evaluation\n",
    "# Format: source (incident content) -> target (reference close notes)\n",
    "eval_data = []\n",
    "for _, row in gt_df.iterrows():\n",
    "    eval_data.append({\n",
    "        \"source\": row[\"content\"],\n",
    "        \"target\": row[\"close_notes_ref\"],\n",
    "        \"incident_number\": row[\"number\"],\n",
    "        \"category\": row[\"category\"],\n",
    "        \"subcategory\": row[\"subcategory\"],\n",
    "        \"contact_type\": row[\"contact_type\"],\n",
    "        \"info_score\": row[\"info_score\"]\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úÖ Prepared {len(eval_data)} examples for evaluation\")\n",
    "print(f\"\\nExample entry:\")\n",
    "print(json.dumps(eval_data[0], indent=2, ensure_ascii=False)[:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: N-gram Comparisons\n",
    "\n",
    "Perform n-gram based comparisons between predicted and reference close notes using BLEU and ROUGE metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram evaluation functions\n",
    "\n",
    "def compute_bleu_score(reference: str, candidate: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute BLEU scores (1-4 grams) between reference and candidate texts.\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference text\n",
    "        candidate: Candidate/predicted text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with BLEU scores for different n-grams\n",
    "    \"\"\"\n",
    "    if not BLEU_AVAILABLE:\n",
    "        return {\"bleu_1\": 0.0, \"bleu_2\": 0.0, \"bleu_3\": 0.0, \"bleu_4\": 0.0}\n",
    "    \n",
    "    try:\n",
    "        ref_tokens = word_tokenize(reference.lower())\n",
    "        cand_tokens = word_tokenize(candidate.lower())\n",
    "        \n",
    "        smoothing = SmoothingFunction().method1\n",
    "        \n",
    "        scores = {}\n",
    "        for n in range(1, 5):\n",
    "            weights = tuple([1.0/n] * n + [0.0] * (4-n))\n",
    "            score = sentence_bleu([ref_tokens], cand_tokens, weights=weights, smoothing_function=smoothing)\n",
    "            scores[f\"bleu_{n}\"] = float(score)\n",
    "        \n",
    "        return scores\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error computing BLEU: {e}\")\n",
    "        return {\"bleu_1\": 0.0, \"bleu_2\": 0.0, \"bleu_3\": 0.0, \"bleu_4\": 0.0}\n",
    "\n",
    "\n",
    "def compute_rouge_scores(reference: str, candidate: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L) between reference and candidate texts.\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference text\n",
    "        candidate: Candidate/predicted text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with ROUGE scores\n",
    "    \"\"\"\n",
    "    if not ROUGE_AVAILABLE:\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "    \n",
    "    try:\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(reference, candidate)\n",
    "        \n",
    "        return {\n",
    "            \"rouge1\": scores['rouge1'].fmeasure,\n",
    "            \"rouge2\": scores['rouge2'].fmeasure,\n",
    "            \"rougeL\": scores['rougeL'].fmeasure,\n",
    "            \"rouge1_precision\": scores['rouge1'].precision,\n",
    "            \"rouge1_recall\": scores['rouge1'].recall,\n",
    "            \"rouge2_precision\": scores['rouge2'].precision,\n",
    "            \"rouge2_recall\": scores['rouge2'].recall,\n",
    "            \"rougeL_precision\": scores['rougeL'].precision,\n",
    "            \"rougeL_recall\": scores['rougeL'].recall,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error computing ROUGE: {e}\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "\n",
    "def evaluate_ngram_metrics(reference: str, candidate: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute all n-gram based metrics (BLEU + ROUGE).\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference text\n",
    "        candidate: Candidate/predicted text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all n-gram metrics\n",
    "    \"\"\"\n",
    "    bleu_scores = compute_bleu_score(reference, candidate)\n",
    "    rouge_scores = compute_rouge_scores(reference, candidate)\n",
    "    \n",
    "    return {**bleu_scores, **rouge_scores}\n",
    "\n",
    "\n",
    "# Test on a sample\n",
    "if len(eval_data) > 0:\n",
    "    sample = eval_data[0]\n",
    "    print(\"üß™ Testing n-gram metrics on sample:\")\n",
    "    print(f\"\\nReference (first 200 chars):\\n{sample['target'][:200]}...\")\n",
    "    print(f\"\\nCandidate (using reference as candidate for demo):\\n{sample['target'][:200]}...\")\n",
    "    \n",
    "    metrics = evaluate_ngram_metrics(sample['target'], sample['target'])\n",
    "    print(f\"\\nüìä N-gram Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No evaluation data available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-computed embeddings\n",
    "try:\n",
    "    embeddings, embeddings_metadata = load_ground_truth_embeddings(data_dir=str(DATA_DIR))\n",
    "    print(f\"‚úÖ Loaded embeddings: shape {embeddings.shape}\")\n",
    "    print(f\"   Metadata keys: {list(embeddings_metadata.keys())}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ö†Ô∏è Embeddings not found: {e}\")\n",
    "    print(\"   Will compute embeddings on-the-fly if needed\")\n",
    "    embeddings = None\n",
    "    embeddings_metadata = None\n",
    "\n",
    "\n",
    "def evaluate_semantic_similarity(reference: str, candidate: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute semantic similarity between reference and candidate texts.\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference text\n",
    "        candidate: Candidate/predicted text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with semantic similarity scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        similarity = compute_semantic_similarity(reference, candidate)\n",
    "        return {\n",
    "            \"semantic_similarity\": similarity,\n",
    "            \"cosine_similarity\": similarity  # Alias for clarity\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error computing semantic similarity: {e}\")\n",
    "        return {\"semantic_similarity\": 0.0, \"cosine_similarity\": 0.0}\n",
    "\n",
    "\n",
    "# Test semantic similarity on a sample\n",
    "if len(eval_data) > 0:\n",
    "    sample = eval_data[0]\n",
    "    print(\"üß™ Testing semantic similarity on sample:\")\n",
    "    print(f\"\\nReference (first 200 chars):\\n{sample['target'][:200]}...\")\n",
    "    \n",
    "    # Test with the same text (should be 1.0)\n",
    "    semantic_scores = evaluate_semantic_similarity(sample['target'], sample['target'])\n",
    "    print(f\"\\nüìä Semantic Metrics (self-comparison):\")\n",
    "    for metric, value in semantic_scores.items():\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Test with a modified version (shorter, should be < 1.0)\n",
    "    if len(sample['target']) > 100:\n",
    "        modified = sample['target'][:len(sample['target'])//2] + \" (truncated)\"\n",
    "        semantic_scores_modified = evaluate_semantic_similarity(sample['target'], modified)\n",
    "        print(f\"\\nüìä Semantic Metrics (truncated comparison):\")\n",
    "        for metric, value in semantic_scores_modified.items():\n",
    "            print(f\"   {metric}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No evaluation data available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-computed embeddings\n",
    "try:\n",
    "    embeddings, embeddings_metadata = load_ground_truth_embeddings(data_dir=str(DATA_DIR))\n",
    "    print(f\"‚úÖ Loaded embeddings: shape {embeddings.shape}\")\n",
    "    print(f\"   Metadata keys: {list(embeddings_metadata.keys())}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ö†Ô∏è Embeddings not found: {e}\")\n",
    "    print(\"   Will compute embeddings on-the-fly if needed\")\n",
    "    embeddings = None\n",
    "    embeddings_metadata = None\n",
    "\n",
    "\n",
    "def evaluate_semantic_similarity(reference: str, candidate: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute semantic similarity between reference and candidate texts.\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference text\n",
    "        candidate: Candidate/predicted text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with semantic similarity scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        similarity = compute_semantic_similarity(reference, candidate)\n",
    "        return {\n",
    "            \"semantic_similarity\": similarity,\n",
    "            \"cosine_similarity\": similarity  # Alias for clarity\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error computing semantic similarity: {e}\")\n",
    "        return {\"semantic_similarity\": 0.0, \"cosine_similarity\": 0.0}\n",
    "\n",
    "\n",
    "# Test semantic similarity on a sample\n",
    "if len(eval_data) > 0:\n",
    "    sample = eval_data[0]\n",
    "    print(\"üß™ Testing semantic similarity on sample:\")\n",
    "    print(f\"\\nReference (first 200 chars):\\n{sample['target'][:200]}...\")\n",
    "    \n",
    "    # Test with the same text (should be 1.0)\n",
    "    semantic_scores = evaluate_semantic_similarity(sample['target'], sample['target'])\n",
    "    print(f\"\\nüìä Semantic Metrics (self-comparison):\")\n",
    "    for metric, value in semantic_scores.items():\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Test with a modified version (shorter, should be < 1.0)\n",
    "    if len(sample['target']) > 100:\n",
    "        modified = sample['target'][:len(sample['target'])//2] + \" (truncated)\"\n",
    "        semantic_scores_modified = evaluate_semantic_similarity(sample['target'], modified)\n",
    "        print(f\"\\nüìä Semantic Metrics (truncated comparison):\")\n",
    "        for metric, value in semantic_scores_modified.items():\n",
    "            print(f\"   {metric}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No evaluation data available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: LLM-as-a-Judge\n",
    "\n",
    "Implement structured evaluation using an LLM as a judge to assess quality across 6 ITSM-specific dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-as-a-Judge prompt template\n",
    "\n",
    "LLM_JUDGE_PROMPT_TEMPLATE = \"\"\"You are an expert in IT Service Management and incident documentation.\n",
    "Your task is to evaluate how accurately and completely a *generated close note* describes the resolution of an incident, compared to a *reference note*.\n",
    "\n",
    "Compare the following texts:\n",
    "\n",
    "* **Reference (ground truth) close note:**\n",
    "{close_notes_ref}\n",
    "\n",
    "* **Generated close note:**\n",
    "{close_notes_pred}\n",
    "\n",
    "Evaluate the generated note according to the following criteria.\n",
    "For each, assign a **score from 0 to 5** and include a one-sentence explanation.\n",
    "\n",
    "1. **Incident coverage (0‚Äì5)** ‚Äî Does it address the same issue and context?\n",
    "2. **Technical steps & resolution actions (0‚Äì5)** ‚Äî Are the main diagnostic and corrective actions consistent and complete?\n",
    "3. **Accuracy of facts (0‚Äì5)** ‚Äî Does it avoid inventing systems, errors, or results?\n",
    "4. **Customer/system context (0‚Äì5)** ‚Äî Does it correctly reference the affected service, device, or user?\n",
    "5. **Clarity & structure (0‚Äì5)** ‚Äî Is it readable, logically ordered, and professionally written?\n",
    "6. **Resolution summary (0‚Äì5)** ‚Äî Does it clearly describe the outcome or confirmation of resolution?\n",
    "\n",
    "Then compute:\n",
    "\n",
    "* `\"general_score\"` ‚Äî the average of the six scores\n",
    "* `\"general_score_explanation\"` ‚Äî a brief summary of your overall judgment\n",
    "\n",
    "Return the evaluation as valid JSON only:\n",
    "\n",
    "{{\n",
    "  \"check_incident_coverage\": 5,\n",
    "  \"check_incident_coverage_explanation\": \"...\",\n",
    "  \"check_technical_steps\": 5,\n",
    "  \"check_technical_steps_explanation\": \"...\",\n",
    "  \"check_accuracy_of_facts\": 5,\n",
    "  \"check_accuracy_of_facts_explanation\": \"...\",\n",
    "  \"check_customer_context\": 5,\n",
    "  \"check_customer_context_explanation\": \"...\",\n",
    "  \"check_clarity_structure\": 4,\n",
    "  \"check_clarity_structure_explanation\": \"...\",\n",
    "  \"check_resolution_summary\": 5,\n",
    "  \"check_resolution_summary_explanation\": \"...\",\n",
    "  \"general_score\": 4.83,\n",
    "  \"general_score_explanation\": \"The generated close note accurately covers the same incident, includes consistent troubleshooting steps, and provides a clear resolution summary with no invented facts.\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_llm_judge_prompt(reference: str, candidate: str) -> str:\n",
    "    \"\"\"\n",
    "    Create LLM-as-a-Judge prompt with reference and candidate texts.\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference close note\n",
    "        candidate: Generated/predicted close note\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    return LLM_JUDGE_PROMPT_TEMPLATE.format(\n",
    "        close_notes_ref=reference,\n",
    "        close_notes_pred=candidate\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_llm_judge_response(response: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse LLM judge response (JSON) into structured format.\n",
    "    \n",
    "    Args:\n",
    "        response: LLM response string (should contain JSON)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with parsed scores and explanations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to extract JSON from response\n",
    "        if \"{\" in response and \"}\" in response:\n",
    "            start = response.find(\"{\")\n",
    "            end = response.rfind(\"}\") + 1\n",
    "            json_str = response[start:end]\n",
    "            result = json.loads(json_str)\n",
    "            return result\n",
    "        else:\n",
    "            # Fallback: try parsing entire response\n",
    "            return json.loads(response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ö†Ô∏è Error parsing LLM judge response: {e}\")\n",
    "        print(f\"   Response: {response[:200]}...\")\n",
    "        return {\n",
    "            \"check_incident_coverage\": 0,\n",
    "            \"check_technical_steps\": 0,\n",
    "            \"check_accuracy_of_facts\": 0,\n",
    "            \"check_customer_context\": 0,\n",
    "            \"check_clarity_structure\": 0,\n",
    "            \"check_resolution_summary\": 0,\n",
    "            \"general_score\": 0,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "# Example prompt generation\n",
    "if len(eval_data) > 0:\n",
    "    sample = eval_data[0]\n",
    "    example_prompt = create_llm_judge_prompt(\n",
    "        reference=sample['target'],\n",
    "        candidate=sample['target']  # Using same as candidate for demo\n",
    "    )\n",
    "    print(\"üìù Example LLM-as-a-Judge Prompt (first 500 chars):\")\n",
    "    print(example_prompt[:500] + \"...\")\n",
    "    print(\"\\n‚úÖ LLM-as-a-Judge prompt template ready\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No evaluation data available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Comprehensive Evaluation Pipeline\n",
    "\n",
    "Combine all evaluation methods (N-gram, Semantic, LLM-as-a-Judge) into a comprehensive evaluation pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation(\n",
    "    reference: str,\n",
    "    candidate: str,\n",
    "    compute_llm_judge: bool = False,\n",
    "    llm_provider=None\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Perform comprehensive evaluation combining all metrics.\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference close note\n",
    "        candidate: Generated/predicted close note\n",
    "        compute_llm_judge: Whether to compute LLM-as-a-Judge scores (requires LLM)\n",
    "        llm_provider: Optional LLM provider function (e.g., Ollama, OpenAI)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all evaluation metrics\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"reference\": reference[:200] + \"...\" if len(reference) > 200 else reference,\n",
    "        \"candidate\": candidate[:200] + \"...\" if len(candidate) > 200 else candidate,\n",
    "    }\n",
    "    \n",
    "    # N-gram metrics\n",
    "    ngram_metrics = evaluate_ngram_metrics(reference, candidate)\n",
    "    results.update({f\"ngram_{k}\": v for k, v in ngram_metrics.items()})\n",
    "    \n",
    "    # Semantic similarity\n",
    "    semantic_metrics = evaluate_semantic_similarity(reference, candidate)\n",
    "    results.update({f\"semantic_{k}\": v for k, v in semantic_metrics.items()})\n",
    "    \n",
    "    # LLM-as-a-Judge (optional)\n",
    "    if compute_llm_judge and llm_provider:\n",
    "        try:\n",
    "            prompt = create_llm_judge_prompt(reference, candidate)\n",
    "            llm_response = llm_provider(prompt)\n",
    "            llm_scores = parse_llm_judge_response(llm_response)\n",
    "            results.update({f\"llm_judge_{k}\": v for k, v in llm_scores.items()})\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è LLM-as-a-Judge evaluation failed: {e}\")\n",
    "            results[\"llm_judge_error\"] = str(e)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example: Evaluate a sample with all metrics\n",
    "if len(eval_data) > 0:\n",
    "    sample = eval_data[0]\n",
    "    print(\"üîç Comprehensive Evaluation Example:\")\n",
    "    print(f\"\\nIncident: {sample['incident_number']}\")\n",
    "    print(f\"Category: {sample['category']}\")\n",
    "    \n",
    "    # For demo, use reference as candidate (perfect match scenario)\n",
    "    # In real usage, candidate would be LLM-generated\n",
    "    eval_results = comprehensive_evaluation(\n",
    "        reference=sample['target'],\n",
    "        candidate=sample['target'],  # Replace with actual LLM output\n",
    "        compute_llm_judge=False  # Set to True if LLM provider is available\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìä Evaluation Results:\")\n",
    "    print(json.dumps(eval_results, indent=2, ensure_ascii=False)[:800] + \"...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No evaluation data available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
