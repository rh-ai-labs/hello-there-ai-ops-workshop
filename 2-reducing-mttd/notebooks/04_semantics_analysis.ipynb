{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: Embeddings and Semantics Analysis\n",
    "\n",
    "## üéØ What is This Notebook About?\n",
    "\n",
    "This notebook analyzes **semantic similarity** between close notes using **embeddings** to understand how meaning relates to quality.\n",
    "\n",
    "**Context:**\n",
    "1. We have **two datasets:**\n",
    "   - **Reference Dataset** (good close notes) - High-quality examples\n",
    "   - **Other Incidents Dataset** (bad/regular close notes) - Standard examples\n",
    "   \n",
    "2. We want to understand: **Do good close notes cluster together semantically?**\n",
    "   - If yes ‚Üí Semantic similarity can help evaluate quality\n",
    "   - If no ‚Üí We need more sophisticated evaluation (LLM-as-a-Judge)\n",
    "\n",
    "**This notebook's purpose:**\n",
    "- **Generate embeddings** - Create semantic representations for all close notes\n",
    "- **Compare semantic similarity** - Measure how similar good vs bad close notes are\n",
    "- **Visualize relationships** - Show semantic space and clustering\n",
    "- **Validate quality scores** - Check if semantic similarity correlates with quality\n",
    "\n",
    "**What we'll learn:**\n",
    "- Good close notes should be semantically closer to each other\n",
    "- Bad close notes should be further from good references\n",
    "- This validates that semantic evaluation is useful for assessing quality\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Key Concepts Explained\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "**Embeddings** are mathematical representations of text that capture **meaning**, not just words.\n",
    "\n",
    "**Think of it like this:**\n",
    "- **Words:** \"User cannot login\" vs \"Login failure\"\n",
    "- **Similar meaning** ‚Üí Similar embeddings (close in semantic space)\n",
    "- **Different meaning** ‚Üí Different embeddings (far in semantic space)\n",
    "\n",
    "**How embeddings work:**\n",
    "- Each close note becomes a **vector** (list of numbers)\n",
    "- Similar meanings ‚Üí Similar vectors ‚Üí Close together in space\n",
    "- Different meanings ‚Üí Different vectors ‚Üí Far apart in space\n",
    "\n",
    "**Example:**\n",
    "- \"Reset password and verified access\" ‚Üí Embedding: [0.2, -0.5, 0.8, ...]\n",
    "- \"Password reset successful\" ‚Üí Embedding: [0.25, -0.48, 0.82, ...]\n",
    "- These are **close** because they mean similar things!\n",
    "\n",
    "### What is Semantic Similarity?\n",
    "\n",
    "**Semantic similarity** measures how similar two texts are in **meaning**, not just words.\n",
    "\n",
    "**How we measure it:**\n",
    "- Calculate **cosine similarity** between embeddings\n",
    "- Score ranges from **-1.0** to **1.0**:\n",
    "  - **1.0** = Identical meaning\n",
    "  - **0.8-0.9** = Very similar meaning\n",
    "  - **0.5-0.7** = Somewhat similar\n",
    "  - **0.0-0.4** = Different meaning\n",
    "  - **-1.0** = Opposite meaning\n",
    "\n",
    "**Why this matters:**\n",
    "- \"Issue resolved\" and \"Problem fixed\" have **different words** but **similar meaning**\n",
    "- Semantic similarity captures this, word overlap doesn't!\n",
    "\n",
    "### Why This Analysis Matters\n",
    "\n",
    "**Hypothesis:** Good close notes should be semantically similar to each other and to reference examples.\n",
    "\n",
    "**What we're testing:**\n",
    "- ‚úÖ **If confirmed:** Semantic similarity can help evaluate quality\n",
    "- ‚úÖ **Validation:** Quality scores make sense (similar scores = similar semantics)\n",
    "- ‚úÖ **Foundation:** Prepare for LLM-as-a-Judge evaluation (Notebook 05)\n",
    "\n",
    "**Expected outcome:**\n",
    "- Good close notes cluster together semantically\n",
    "- Bad close notes are further from good references\n",
    "- Semantic similarity correlates with quality scores\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "This notebook will:\n",
    "1. **Load** reference and other incidents datasets\n",
    "2. **Generate embeddings** for all close notes\n",
    "3. **Calculate semantic similarity** between good and bad close notes\n",
    "4. **Visualize** semantic relationships (t-SNE, heatmaps)\n",
    "5. **Validate** quality scores using semantic similarity\n",
    "6. **Prepare** for LLM-as-a-Judge evaluation (Notebook 05)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã What We're Analyzing\n",
    "\n",
    "**Datasets:**\n",
    "- **Reference Dataset** (`reference_close_notes.csv`): High-quality close notes\n",
    "  - Contains: `close_notes_ref` - well-written resolution notes\n",
    "  \n",
    "- **Other Incidents Dataset** (`other_incidents.csv`): Remaining incidents\n",
    "  - Contains: `close_notes` - standard close notes (for comparison)\n",
    "\n",
    "**What we'll compare:**\n",
    "- Semantic similarity **within** reference dataset (good vs good)\n",
    "- Semantic similarity **within** other incidents dataset (bad vs bad)\n",
    "- Semantic similarity **between** reference and other incidents (good vs bad)\n",
    "- Correlation between semantic similarity and quality scores\n",
    "\n",
    "**Output:**\n",
    "- Embeddings for all close notes\n",
    "- Similarity analysis results\n",
    "- Visualizations showing semantic relationships\n",
    "- Validation that quality scores align with semantic similarity\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Using Embedding Models\n",
    "\n",
    "**Model:** BAAI/bge-m3 (BAAI General Embedding)\n",
    "- **Multilingual** - Works with multiple languages\n",
    "- **High quality** - State-of-the-art semantic understanding\n",
    "- **1024 dimensions** - Rich representation of meaning\n",
    "\n",
    "**Why this model?**\n",
    "- Proven performance on semantic similarity tasks\n",
    "- Handles technical language well\n",
    "- Standard choice for professional NLP applications\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# These are the tools we need to work with data, embeddings, and visualizations\n",
    "\n",
    "import pandas as pd  # For working with tables (like Excel spreadsheets)\n",
    "import numpy as np   # For mathematical operations\n",
    "import matplotlib.pyplot as plt  # For creating charts and graphs\n",
    "import seaborn as sns  # For prettier charts\n",
    "from pathlib import Path  # For handling file paths\n",
    "import sys\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Hide warning messages to keep output clean\n",
    "\n",
    "# Add src directory to path so we can use utility functions\n",
    "sys.path.append(str(Path(\"../src\").resolve()))\n",
    "\n",
    "# Embedding libraries\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sentence_transformers.util import cos_sim\n",
    "    EMBEDDINGS_AVAILABLE = True\n",
    "    print(\"‚úÖ Sentence-transformers imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è sentence-transformers not available. Install with: pip install sentence-transformers\")\n",
    "    EMBEDDINGS_AVAILABLE = False\n",
    "\n",
    "# Visualization libraries\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    VISUALIZATION_AVAILABLE = True\n",
    "    print(\"‚úÖ Visualization libraries imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è sklearn not available. Install with: pip install scikit-learn\")\n",
    "    VISUALIZATION_AVAILABLE = False\n",
    "\n",
    "# Set up plotting style (makes charts look nicer)\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except OSError:\n",
    "    plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display charts in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"\\n‚úÖ All libraries imported successfully!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets\n",
    "\n",
    "**What we're doing:** Loading both datasets we want to compare semantically.\n",
    "\n",
    "**Why:** We need both datasets in memory before we can generate embeddings and compare them.\n",
    "\n",
    "**Datasets we'll load:**\n",
    "- **Reference Dataset** (`reference_close_notes.csv`): High-quality close notes (created in Notebook 02)\n",
    "  - Contains: `close_notes_ref` - well-written resolution notes\n",
    "  \n",
    "- **Other Incidents Dataset** (`other_incidents.csv`): Remaining incidents (created in Notebook 02)\n",
    "  - Contains: `close_notes` - standard close notes (for comparison)\n",
    "\n",
    "**What we'll analyze:**\n",
    "- Semantic similarity between good and bad close notes\n",
    "- Whether good close notes cluster together semantically\n",
    "- If semantic similarity correlates with quality scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "# We'll load both datasets so we can compare them semantically\n",
    "\n",
    "data_dir = Path(\"../data\")  # Where our data files are stored\n",
    "\n",
    "# Load reference dataset (good close notes)\n",
    "reference_path = data_dir / \"reference_close_notes.csv\"\n",
    "reference_df = pd.read_csv(reference_path)\n",
    "print(f\"‚úÖ Loaded reference dataset: {len(reference_df)} records\")\n",
    "print(f\"   Columns: {list(reference_df.columns)}\")\n",
    "\n",
    "# Load other incidents dataset (bad/regular close notes)\n",
    "other_incidents_path = data_dir / \"other_incidents.csv\"\n",
    "other_incidents_df = pd.read_csv(other_incidents_path)\n",
    "print(f\"‚úÖ Loaded other incidents dataset: {len(other_incidents_df)} records\")\n",
    "print(f\"   Columns: {list(other_incidents_df.columns)}\")\n",
    "\n",
    "# Display basic info about both datasets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Reference Dataset Info:\")\n",
    "print(\"=\"*60)\n",
    "print(reference_df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Other Incidents Dataset Info:\")\n",
    "print(\"=\"*60)\n",
    "print(other_incidents_df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìä Reference (good) close notes: {len(reference_df)}\")\n",
    "print(f\"üìä Other incidents (bad/regular) close notes: {len(other_incidents_df)}\")\n",
    "print(f\"üìä Total close notes to analyze: {len(reference_df) + len(other_incidents_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Embedding Generation\n",
    "\n",
    "**What we're doing:** Preparing close notes text from both datasets for embedding generation.\n",
    "\n",
    "**Why:** We need clean, consistent text before generating embeddings. We'll extract:\n",
    "- `close_notes_ref` from reference dataset (good examples)\n",
    "- `close_notes` from other incidents dataset (bad/regular examples)\n",
    "\n",
    "**What to check:**\n",
    "- Make sure we have text content (not empty)\n",
    "- Handle missing values\n",
    "- Prepare a combined dataset for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for embedding generation\n",
    "# We'll combine both datasets and prepare the close notes text\n",
    "\n",
    "# Prepare reference dataset\n",
    "reference_df_prep = reference_df.copy()\n",
    "reference_df_prep['close_notes_text'] = reference_df_prep['close_notes_ref'].astype(str)\n",
    "reference_df_prep['dataset_type'] = 'reference'\n",
    "reference_df_prep['quality_label'] = 'good'\n",
    "\n",
    "# Prepare other incidents dataset\n",
    "other_incidents_df_prep = other_incidents_df.copy()\n",
    "other_incidents_df_prep['close_notes_text'] = other_incidents_df_prep['close_notes'].astype(str)\n",
    "other_incidents_df_prep['dataset_type'] = 'other'\n",
    "other_incidents_df_prep['quality_label'] = 'bad/regular'\n",
    "\n",
    "# Combine both datasets\n",
    "# Select common columns for comparison\n",
    "common_cols = ['number', 'category', 'subcategory', 'close_notes_text', 'dataset_type', 'quality_label']\n",
    "all_close_notes = pd.concat([\n",
    "    reference_df_prep[common_cols],\n",
    "    other_incidents_df_prep[common_cols]\n",
    "], ignore_index=True)\n",
    "\n",
    "# Filter out empty or very short close notes\n",
    "all_close_notes = all_close_notes[\n",
    "    (all_close_notes['close_notes_text'].str.strip() != '') &\n",
    "    (all_close_notes['close_notes_text'].str.strip() != 'nan') &\n",
    "    (all_close_notes['close_notes_text'].str.len() > 10)\n",
    "].copy()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA PREPARATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìä Reference (good) close notes: {len(reference_df_prep)}\")\n",
    "print(f\"üìä Other incidents (bad/regular) close notes: {len(other_incidents_df_prep)}\")\n",
    "print(f\"üìä Total after filtering: {len(all_close_notes)}\")\n",
    "print(f\"\\nüìã Dataset breakdown:\")\n",
    "print(f\"   - Reference (good): {len(all_close_notes[all_close_notes['dataset_type'] == 'reference'])}\")\n",
    "print(f\"   - Other incidents (bad/regular): {len(all_close_notes[all_close_notes['dataset_type'] == 'other'])}\")\n",
    "\n",
    "# Show sample close notes\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE CLOSE NOTES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìù Reference (Good) Example:\")\n",
    "ref_sample = all_close_notes[all_close_notes['dataset_type'] == 'reference']['close_notes_text'].iloc[0]\n",
    "print(f\"   {ref_sample[:200]}...\")\n",
    "\n",
    "print(\"\\nüìù Other Incidents (Bad/Regular) Example:\")\n",
    "other_sample = all_close_notes[all_close_notes['dataset_type'] == 'other']['close_notes_text'].iloc[0]\n",
    "print(f\"   {other_sample[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings\n",
    "\n",
    "**What we're doing:** Creating semantic embeddings for all close notes using an embedding model.\n",
    "\n",
    "**How it works:**\n",
    "1. Load the embedding model (BAAI/bge-m3)\n",
    "2. Convert each close note text into a vector (embedding)\n",
    "3. Store embeddings for similarity calculations\n",
    "\n",
    "**What embeddings capture:**\n",
    "- **Meaning** - Similar meanings ‚Üí similar embeddings\n",
    "- **Context** - Understands technical language\n",
    "- **Relationships** - Can measure semantic similarity\n",
    "\n",
    "**Why this matters:**\n",
    "- Embeddings allow us to measure meaning, not just word overlap\n",
    "- We can compare good vs bad close notes semantically\n",
    "- This validates whether quality correlates with meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all close notes\n",
    "# This will take a few minutes depending on the number of close notes\n",
    "\n",
    "if not EMBEDDINGS_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è Cannot generate embeddings: sentence-transformers not available\")\n",
    "    print(\"   Install with: pip install sentence-transformers\")\n",
    "else:\n",
    "    import os\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"LOADING EMBEDDING MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Model selection: Use BGE-M3 for multilingual, multi-granularity support\n",
    "    DEFAULT_MODEL = 'BAAI/bge-m3'  # Multilingual, supports dense/sparse/multi-vector retrieval\n",
    "    embedding_model_name = os.getenv('EMBEDDING_MODEL', DEFAULT_MODEL)\n",
    "    print(f\"üì¶ Using model: {embedding_model_name}\")\n",
    "    print(\"   This model creates 1024-dimensional embeddings that capture meaning\")\n",
    "    \n",
    "    use_flag_embedding = False\n",
    "    try:\n",
    "        # Try sentence-transformers first\n",
    "        model = SentenceTransformer(embedding_model_name, trust_remote_code=True)\n",
    "        embedding_dim = model.get_sentence_embedding_dimension()\n",
    "        print(f\"‚úÖ Model loaded: {embedding_dim}-dimensional embeddings\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading with sentence-transformers: {e}\")\n",
    "        print(\"   Trying FlagEmbedding library...\")\n",
    "        try:\n",
    "            from FlagEmbedding import BGEM3FlagModel\n",
    "            model = BGEM3FlagModel(embedding_model_name)\n",
    "            use_flag_embedding = True\n",
    "            embedding_dim = 1024\n",
    "            print(f\"‚úÖ Model loaded via FlagEmbedding: {embedding_dim}-dimensional embeddings\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è FlagEmbedding not installed. Install with: pip install FlagEmbedding\")\n",
    "            raise\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ö†Ô∏è Error loading with FlagEmbedding: {e2}\")\n",
    "            raise\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATING EMBEDDINGS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìä Generating embeddings for {len(all_close_notes)} close notes...\")\n",
    "    print(\"   This may take a few minutes...\")\n",
    "    \n",
    "    # Extract close notes text\n",
    "    close_notes_texts = all_close_notes['close_notes_text'].astype(str).tolist()\n",
    "    \n",
    "    # Generate embeddings\n",
    "    if use_flag_embedding:\n",
    "        # FlagEmbedding returns dict with 'dense_vecs', 'sparse', 'colbert_vecs'\n",
    "        output = model.encode(close_notes_texts, return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
    "        embeddings = output['dense_vecs']\n",
    "    else:\n",
    "        embeddings = model.encode(close_notes_texts, show_progress_bar=True, batch_size=32)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Generated embeddings for {len(embeddings)} close notes\")\n",
    "    print(f\"   Embedding dimensions: {embeddings.shape}\")\n",
    "    print(f\"   Each close note is now represented as a {embeddings.shape[1]}-dimensional vector\")\n",
    "    \n",
    "    # Store embeddings in dataframe\n",
    "    all_close_notes['embedding'] = embeddings.tolist()\n",
    "    \n",
    "    print(\"\\n‚úÖ Embeddings generated and stored!\")\n",
    "    print(\"   Ready for semantic similarity analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Semantic Similarity\n",
    "\n",
    "**What we're doing:** Calculating how similar close notes are to each other semantically.\n",
    "\n",
    "**What we'll calculate:**\n",
    "1. **Within-group similarity:**\n",
    "   - How similar are good close notes to each other?\n",
    "   - How similar are bad close notes to each other?\n",
    "\n",
    "2. **Between-group similarity:**\n",
    "   - How similar are bad close notes to good references?\n",
    "   - This tells us if good and bad are semantically different\n",
    "\n",
    "3. **Average similarity scores:**\n",
    "   - Average similarity within reference dataset\n",
    "   - Average similarity within other incidents dataset\n",
    "   - Average similarity between datasets\n",
    "\n",
    "**Expected results:**\n",
    "- Good close notes should be more similar to each other (higher within-group similarity)\n",
    "- Bad close notes should be less similar to good references (lower between-group similarity)\n",
    "- This validates that semantic similarity can distinguish quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate semantic similarity between close notes\n",
    "# We'll compare good vs good, bad vs bad, and good vs bad\n",
    "\n",
    "if 'embedding' not in all_close_notes.columns:\n",
    "    print(\"‚ö†Ô∏è Embeddings not available. Please run the embedding generation cell first.\")\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"CALCULATING SEMANTIC SIMILARITY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Convert embeddings to numpy array for faster computation\n",
    "    embeddings_array = np.array(all_close_notes['embedding'].tolist())\n",
    "    \n",
    "    # Separate reference and other incidents\n",
    "    reference_mask = all_close_notes['dataset_type'] == 'reference'\n",
    "    reference_embeddings = embeddings_array[reference_mask]\n",
    "    other_embeddings = embeddings_array[~reference_mask]\n",
    "    \n",
    "    print(f\"\\nüìä Reference (good) close notes: {len(reference_embeddings)}\")\n",
    "    print(f\"üìä Other incidents (bad/regular) close notes: {len(other_embeddings)}\")\n",
    "    \n",
    "    # Calculate similarity matrices\n",
    "    print(\"\\nüîÑ Calculating similarity matrices...\")\n",
    "    \n",
    "    # Within reference (good vs good)\n",
    "    if len(reference_embeddings) > 1:\n",
    "        ref_ref_similarity = cosine_similarity(reference_embeddings, reference_embeddings)\n",
    "        # Remove diagonal (self-similarity = 1.0)\n",
    "        ref_ref_similarity_nodiag = ref_ref_similarity[np.triu_indices(len(reference_embeddings), k=1)]\n",
    "        ref_ref_mean = ref_ref_similarity_nodiag.mean()\n",
    "        ref_ref_std = ref_ref_similarity_nodiag.std()\n",
    "    else:\n",
    "        ref_ref_mean = 0.0\n",
    "        ref_ref_std = 0.0\n",
    "    \n",
    "    # Within other incidents (bad vs bad)\n",
    "    if len(other_embeddings) > 1:\n",
    "        other_other_similarity = cosine_similarity(other_embeddings, other_embeddings)\n",
    "        # Remove diagonal\n",
    "        other_other_similarity_nodiag = other_other_similarity[np.triu_indices(len(other_embeddings), k=1)]\n",
    "        other_other_mean = other_other_similarity_nodiag.mean()\n",
    "        other_other_std = other_other_similarity_nodiag.std()\n",
    "    else:\n",
    "        other_other_mean = 0.0\n",
    "        other_other_std = 0.0\n",
    "    \n",
    "    # Between reference and other incidents (good vs bad)\n",
    "    ref_other_similarity = cosine_similarity(reference_embeddings, other_embeddings)\n",
    "    ref_other_mean = ref_other_similarity.mean()\n",
    "    ref_other_std = ref_other_similarity.std()\n",
    "    \n",
    "    # Store results\n",
    "    similarity_results = {\n",
    "        'within_reference_mean': ref_ref_mean,\n",
    "        'within_reference_std': ref_ref_std,\n",
    "        'within_other_mean': other_other_mean,\n",
    "        'within_other_std': other_other_std,\n",
    "        'between_ref_other_mean': ref_other_mean,\n",
    "        'between_ref_other_std': ref_other_std\n",
    "    }\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SEMANTIC SIMILARITY RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nüìä Within Reference (Good vs Good):\")\n",
    "    print(f\"   Mean similarity: {ref_ref_mean:.4f}\")\n",
    "    print(f\"   Std deviation: {ref_ref_std:.4f}\")\n",
    "    print(f\"   Interpretation: {'High' if ref_ref_mean > 0.7 else 'Moderate' if ref_ref_mean > 0.5 else 'Low'} similarity\")\n",
    "    \n",
    "    print(f\"\\nüìä Within Other Incidents (Bad vs Bad):\")\n",
    "    print(f\"   Mean similarity: {other_other_mean:.4f}\")\n",
    "    print(f\"   Std deviation: {other_other_std:.4f}\")\n",
    "    print(f\"   Interpretation: {'High' if other_other_mean > 0.7 else 'Moderate' if other_other_mean > 0.5 else 'Low'} similarity\")\n",
    "    \n",
    "    print(f\"\\nüìä Between Reference and Other (Good vs Bad):\")\n",
    "    print(f\"   Mean similarity: {ref_other_mean:.4f}\")\n",
    "    print(f\"   Std deviation: {ref_other_std:.4f}\")\n",
    "    print(f\"   Interpretation: {'High' if ref_other_mean > 0.7 else 'Moderate' if ref_other_mean > 0.5 else 'Low'} similarity\")\n",
    "    \n",
    "    # Key insight\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"KEY INSIGHT\")\n",
    "    print(\"=\"*60)\n",
    "    if ref_ref_mean > ref_other_mean:\n",
    "        print(\"‚úÖ Good close notes are MORE similar to each other than to bad close notes\")\n",
    "        print(\"   ‚Üí Semantic similarity CAN distinguish good from bad\")\n",
    "        print(\"   ‚Üí This validates semantic evaluation is useful!\")\n",
    "    elif ref_ref_mean < ref_other_mean:\n",
    "        print(\"‚ö†Ô∏è Good close notes are LESS similar to each other than to bad close notes\")\n",
    "        print(\"   ‚Üí This is unexpected - semantic similarity may not distinguish quality well\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Good and bad close notes show similar semantic similarity\")\n",
    "        print(\"   ‚Üí Semantic similarity alone may not be sufficient for evaluation\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Similarity analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Semantic Relationships\n",
    "\n",
    "**What we're doing:** Creating visualizations to see how close notes cluster in semantic space.\n",
    "\n",
    "**Visualizations we'll create:**\n",
    "1. **t-SNE Plot** - Shows 2D representation of semantic space\n",
    "   - Colors: Good (green) vs Bad (blue)\n",
    "   - Clusters: Good close notes should cluster together\n",
    "   \n",
    "2. **Similarity Heatmap** - Shows similarity matrix\n",
    "   - Dark colors = High similarity\n",
    "   - Light colors = Low similarity\n",
    "   - Blocks should show clustering\n",
    "\n",
    "**What to look for:**\n",
    "- **Good clustering:** Good close notes grouped together (green dots close)\n",
    "- **Separation:** Good and bad close notes in different areas\n",
    "- **Validation:** Confirms semantic similarity can distinguish quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize semantic relationships using t-SNE\n",
    "# This reduces high-dimensional embeddings to 2D for visualization\n",
    "# Colors represent categories, marker shapes represent quality (good vs bad)\n",
    "\n",
    "if 'embedding' not in all_close_notes.columns:\n",
    "    print(\"‚ö†Ô∏è Embeddings not available. Please run the embedding generation cell first.\")\n",
    "elif not VISUALIZATION_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è Visualization libraries not available. Install with: pip install scikit-learn\")\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"CREATING t-SNE VISUALIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"üîÑ Reducing embeddings to 2D using t-SNE...\")\n",
    "    print(\"   This may take a minute...\")\n",
    "    \n",
    "    # Convert embeddings to numpy array\n",
    "    embeddings_array = np.array(all_close_notes['embedding'].tolist())\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings_array)-1))\n",
    "    embeddings_2d = tsne.fit_transform(embeddings_array)\n",
    "    \n",
    "    # Store 2D coordinates\n",
    "    all_close_notes['tsne_x'] = embeddings_2d[:, 0]\n",
    "    all_close_notes['tsne_y'] = embeddings_2d[:, 1]\n",
    "    \n",
    "    # Get unique categories\n",
    "    categories = sorted(all_close_notes['category'].dropna().unique())\n",
    "    \n",
    "    # Create color palette for categories\n",
    "    # Using a distinct color palette that works well with colorblind users\n",
    "    category_colors = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
    "    category_color_map = {cat: category_colors[i] for i, cat in enumerate(categories)}\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    # Plot each category separately, using different markers for good vs bad\n",
    "    legend_elements = []\n",
    "    \n",
    "    for category in categories:\n",
    "        category_data = all_close_notes[all_close_notes['category'] == category]\n",
    "        color = category_color_map[category]\n",
    "        \n",
    "        # Plot reference (good) close notes for this category - use circles\n",
    "        ref_mask = (all_close_notes['category'] == category) & (all_close_notes['dataset_type'] == 'reference')\n",
    "        if ref_mask.sum() > 0:\n",
    "            ref_data = all_close_notes[ref_mask]\n",
    "            scatter = ax.scatter(\n",
    "                ref_data['tsne_x'],\n",
    "                ref_data['tsne_y'],\n",
    "                c=color,  # Single color for all points in this group\n",
    "                marker='o',  # Circle for good\n",
    "                label=f'{category} (Good)',\n",
    "                alpha=0.8,\n",
    "                s=120,\n",
    "                edgecolors='black',\n",
    "                linewidths=1.5\n",
    "            )\n",
    "            legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                             markerfacecolor=color, markersize=10, \n",
    "                                             markeredgecolor='black', markeredgewidth=1.5,\n",
    "                                             label=f'{category} (Good)'))\n",
    "        \n",
    "        # Plot other incidents (bad/regular) close notes for this category - use squares\n",
    "        other_mask = (all_close_notes['category'] == category) & (all_close_notes['dataset_type'] == 'other')\n",
    "        if other_mask.sum() > 0:\n",
    "            other_data = all_close_notes[other_mask]\n",
    "            scatter = ax.scatter(\n",
    "                other_data['tsne_x'],\n",
    "                other_data['tsne_y'],\n",
    "                c=color,  # Single color for all points in this group\n",
    "                marker='s',  # Square for bad/regular\n",
    "                label=f'{category} (Bad/Regular)',\n",
    "                alpha=0.5,\n",
    "                s=80,\n",
    "                edgecolors='black',\n",
    "                linewidths=0.8\n",
    "            )\n",
    "            legend_elements.append(plt.Line2D([0], [0], marker='s', color='w', \n",
    "                                             markerfacecolor=color, markersize=8, \n",
    "                                             markeredgecolor='black', markeredgewidth=0.8,\n",
    "                                             label=f'{category} (Bad/Regular)', alpha=0.5))\n",
    "    \n",
    "    ax.set_xlabel('t-SNE Dimension 1', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('t-SNE Dimension 2', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Semantic Space Visualization (t-SNE)\\nCategories: Colors | Quality: Shapes (‚óã Good, ‚ñ° Bad/Regular)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Create legend with categories\n",
    "    ax.legend(handles=legend_elements, fontsize=9, loc='center left', bbox_to_anchor=(1, 0.5), \n",
    "              framealpha=0.9, ncol=1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Reading the t-SNE plot:\")\n",
    "    print(\"   - Colors = Different categories\")\n",
    "    print(\"   - ‚óã Circles = Good close notes (reference)\")\n",
    "    print(\"   - ‚ñ° Squares = Bad/regular close notes (other incidents)\")\n",
    "    print(\"   - Close dots = Semantically similar\")\n",
    "    print(\"   - Far dots = Semantically different\")\n",
    "    print(\"   - If same-colored circles cluster together ‚Üí Good close notes in same category are similar!\")\n",
    "    print(\"   - If circles and squares of same color are separated ‚Üí Quality distinction within category!\")\n",
    "    \n",
    "    # Print category summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CATEGORY BREAKDOWN IN VISUALIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    for category in categories:\n",
    "        ref_count = len(all_close_notes[(all_close_notes['category'] == category) & \n",
    "                                       (all_close_notes['dataset_type'] == 'reference')])\n",
    "        other_count = len(all_close_notes[(all_close_notes['category'] == category) & \n",
    "                                         (all_close_notes['dataset_type'] == 'other')])\n",
    "        print(f\"   {category}: {ref_count} good (‚óã), {other_count} bad/regular (‚ñ°)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ t-SNE visualization complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category-aware similarity analysis\n",
    "# Compare good vs bad within the same category\n",
    "\n",
    "if 'embedding' not in all_close_notes.columns:\n",
    "    print(\"‚ö†Ô∏è Embeddings not available. Please run the embedding generation cell first.\")\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"CATEGORY-AWARE SIMILARITY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüîÑ Analyzing similarity within same categories...\")\n",
    "    print(\"   This might show better separation than comparing across all categories\")\n",
    "    \n",
    "    # Get unique categories\n",
    "    categories = all_close_notes['category'].unique()\n",
    "    category_results = []\n",
    "    \n",
    "    for category in categories:\n",
    "        category_data = all_close_notes[all_close_notes['category'] == category]\n",
    "        \n",
    "        if len(category_data) < 2:\n",
    "            continue\n",
    "        \n",
    "        ref_in_category = category_data[category_data['dataset_type'] == 'reference']\n",
    "        other_in_category = category_data[category_data['dataset_type'] == 'other']\n",
    "        \n",
    "        if len(ref_in_category) == 0 or len(other_in_category) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get embeddings\n",
    "        ref_embeddings = np.array(ref_in_category['embedding'].tolist())\n",
    "        other_embeddings = np.array(other_in_category['embedding'].tolist())\n",
    "        \n",
    "        # Within reference (good vs good)\n",
    "        if len(ref_embeddings) > 1:\n",
    "            ref_ref_sim = cosine_similarity(ref_embeddings, ref_embeddings)\n",
    "            ref_ref_mean = ref_ref_sim[np.triu_indices(len(ref_embeddings), k=1)].mean()\n",
    "        else:\n",
    "            ref_ref_mean = 0.0\n",
    "        \n",
    "        # Between good and bad (within same category)\n",
    "        ref_other_sim = cosine_similarity(ref_embeddings, other_embeddings)\n",
    "        ref_other_mean = ref_other_sim.mean()\n",
    "        \n",
    "        # Separation score\n",
    "        separation = ref_ref_mean - ref_other_mean\n",
    "        \n",
    "        category_results.append({\n",
    "            'category': category,\n",
    "            'reference_count': len(ref_in_category),\n",
    "            'other_count': len(other_in_category),\n",
    "            'within_ref_mean': ref_ref_mean,\n",
    "            'between_mean': ref_other_mean,\n",
    "            'separation_score': separation\n",
    "        })\n",
    "    \n",
    "    if category_results:\n",
    "        category_df = pd.DataFrame(category_results)\n",
    "        category_df = category_df.sort_values('separation_score', ascending=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CATEGORY-AWARE SIMILARITY RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(category_df.to_string(index=False))\n",
    "        \n",
    "        print(\"\\nüí° Interpretation:\")\n",
    "        print(\"   - Positive 'separation_score' = Good notes more similar to each other than to bad\")\n",
    "        print(\"   - Negative 'separation_score' = Good notes less similar to each other than to bad\")\n",
    "        print(\"   - Categories with better separation might be easier to evaluate\")\n",
    "        \n",
    "        # Overall insight\n",
    "        avg_separation = category_df['separation_score'].mean()\n",
    "        print(f\"\\nüìä Average separation score across categories: {avg_separation:.4f}\")\n",
    "        \n",
    "        if avg_separation > 0:\n",
    "            print(\"‚úÖ Category-aware comparison shows better separation!\")\n",
    "            print(\"   ‚Üí Consider using category filtering when finding similar references\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Even within categories, separation is limited\")\n",
    "            print(\"   ‚Üí This confirms that semantic similarity alone may not distinguish quality\")\n",
    "            print(\"   ‚Üí LLM-as-a-Judge (Notebook 05) will be essential for quality evaluation\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Not enough data for category analysis\")\n",
    "        print(\"   Some categories may not have both good and bad examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps\n",
    "\n",
    "**What we learned:**\n",
    "- Semantic similarity can measure meaning, not just word overlap\n",
    "- Good close notes should cluster together semantically\n",
    "- This validates that semantic evaluation is useful for assessing quality\n",
    "\n",
    "**Key findings:**\n",
    "- Embeddings capture meaning and context\n",
    "- Semantic similarity can distinguish good from bad close notes\n",
    "- This prepares us for LLM-as-a-Judge evaluation (Notebook 05)\n",
    "\n",
    "**Next steps:**\n",
    "- **Notebook 05:** Use semantic similarity to find similar references for LLM-as-a-Judge evaluation\n",
    "- **Notebook 06:** Use semantic similarity to evaluate generated close notes\n",
    "\n",
    "**What this enables:**\n",
    "- Finding similar reference close notes for comparison\n",
    "- Evaluating generated close notes semantically\n",
    "- Understanding relationships between close notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and save results for next notebooks\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'embedding' in all_close_notes.columns:\n",
    "    print(\"\\n‚úÖ Embeddings generated successfully\")\n",
    "    print(f\"   - Total close notes with embeddings: {len(all_close_notes)}\")\n",
    "    print(f\"   - Reference (good): {len(all_close_notes[all_close_notes['dataset_type'] == 'reference'])}\")\n",
    "    print(f\"   - Other incidents (bad/regular): {len(all_close_notes[all_close_notes['dataset_type'] == 'other'])}\")\n",
    "    \n",
    "    if 'similarity_results' in locals():\n",
    "        print(\"\\n‚úÖ Semantic similarity analysis complete\")\n",
    "        print(f\"   - Good vs Good similarity: {similarity_results['within_reference_mean']:.4f}\")\n",
    "        print(f\"   - Bad vs Bad similarity: {similarity_results['within_other_mean']:.4f}\")\n",
    "        print(f\"   - Good vs Bad similarity: {similarity_results['between_ref_other_mean']:.4f}\")\n",
    "    \n",
    "    print(\"\\nüìù Data available for next notebooks:\")\n",
    "    print(\"   - 'all_close_notes' dataframe with embeddings\")\n",
    "    print(\"   - 'reference_df' and 'other_incidents_df' (original datasets)\")\n",
    "    print(\"   - Semantic similarity results\")\n",
    "    \n",
    "    print(\"\\nüéØ Next steps:\")\n",
    "    print(\"   - Notebook 05: Use embeddings to find similar references for LLM-as-a-Judge\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Notebook 04 complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Please run all cells above to generate embeddings and complete the analysis\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
