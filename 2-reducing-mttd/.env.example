# Llama Stack Configuration
# Note: API key is OPTIONAL - only needed for remote/cloud deployments
# For local development, you can leave LLAMA_STACK_API_KEY empty
LLAMA_STACK_BASE_URL=http://localhost:8000
LLAMA_STACK_API_KEY=
LLAMA_STACK_PROVIDER_ID=default-provider

# Ollama Configuration (for local LLM serving)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL_SCENARIO_A=llama2:13b
OLLAMA_MODEL_SCENARIO_B=llama2:7b

# LLM Generation Parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1024
LLM_TOP_P=0.9

# MLflow Configuration
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_EXPERIMENT_NAME=incident_enrichment

# Langfuse Configuration (optional)
LANGFUSE_SECRET_KEY=
LANGFUSE_PUBLIC_KEY=
LANGFUSE_HOST=https://cloud.langfuse.com

# Embedding Model
EMBEDDING_MODEL=BAAI/bge-m3

# Evaluation Configuration
EVAL_BATCH_SIZE=10
EVAL_MAX_SAMPLES=
