# Unified requirements.txt for hello-there-ai-ops-workshop
# This file combines requirements from all modules

# Core dependencies
pandas>=2.0.0
numpy>=1.24.0,<2.0.0  # Limit to numpy 1.x for compatibility with most packages
jupyter>=1.0.0
notebook>=7.0.0
matplotlib>=3.7.0
seaborn>=0.12.0

# Hugging Face and datasets
datasets>=2.14.0
transformers>=4.30.0
sentence-transformers>=2.2.0

# LangChain and LLM integration
langchain>=0.1.0
langchain-community>=0.0.20

# LlamaStack for agent framework
# IMPORTANT: Versions 0.3.1+ require Python 3.12+
# If your server is 0.3.1+ but you're using Python 3.11, you have two options:
#   1. Upgrade to Python 3.12+ (recommended)
#   2. Downgrade the server to 0.2.12: pip install llama-stack==0.2.12
# The client version MUST match the server version!
# For Python 3.11: llama-stack==0.2.12 llama-stack-client==0.2.12
# For Python 3.12+: llama-stack>=0.3.1 llama-stack-client>=0.3.1
llama-stack>=0.3.1 # Use 0.2.12 for Python 3.11, or >=0.3.1 for Python 3.12+
llama-stack-client>=0.3.1 # Must match server version - use 0.2.12 for Python 3.11
fire>=0.5.0  # Required by llama-stack-client for MCP OAuth functionality
opentelemetry-api>=1.26.0
opentelemetry-sdk>=1.26.0
opentelemetry-exporter-otlp-proto-http>=1.26.0

# Ollama client for local LLM serving
ollama>=0.1.0

# HTTP client for API requests
requests>=2.31.0

# MLflow for experiment tracking
mlflow>=2.8.0

# Evaluation metrics
rouge-score>=0.1.2
nltk>=3.8.0
scikit-learn>=1.3.0
unitxt>=1.0.0
sacrebleu>=2.0.0

# Llama Stack optional provider dependencies
# These are required because llama-stack starter distribution initializes all providers
# LLM Provider SDKs
together>=1.5.30
botocore>=1.40.72
boto3>=1.40.65
anthropic>=0.72.0
openai>=2.7.1
# google-generativeai>=0.8.5  # Commented out - may cause conflicts
fireworks-ai>=0.19.20
groq>=0.33.0
sambanova>=1.1.5

# Vector Store Providers
faiss-cpu>=1.12.0
sqlite-vec>=0.1.6

# Tool Runtime Providers
mcp>=1.20.0

# Evaluation Providers
autoevals>=0.0.130

# Utilities
python-dotenv>=1.0.0
tqdm>=4.65.0
nbstripout>=0.6.0
termcolor>=2.0.0

# DuckDuckGo search (no API key required) - used in Module 5
duckduckgo-search>=6.0.0

# Optional: Langfuse for LLM observability
# langfuse>=2.0.0
