{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 01: Build Your First RAG System for IT Ticket Search\n",
    "\n",
    "## üéØ Your Mission\n",
    "\n",
    "You're an IT support engineer building your first intelligent ticket search system. Your job today: index IT tickets into a vector database and enable semantic search that finds similar incidents, even when they use different words.\n",
    "\n",
    "**Why this matters:** This same RAG (Retrieval-Augmented Generation) approach is how you could build search systems that understand meaning, not just keywords - enabling faster incident resolution by finding similar past problems and their solutions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## ‚ö° Quick Win: See Semantic Search in Action\n",
    "\n",
    "**The Problem:** Traditional keyword search only finds exact matches. If you search for \"application crashes\", it won't find tickets about \"software failures\" or \"system errors\" - even though they're the same problem!\n",
    "\n",
    "**The Solution:** Semantic search understands meaning. It knows that \"application crashes\", \"software failures\", and \"system errors\" are similar concepts.\n",
    "\n",
    "**Try this:** After we build the system, search for \"application crashes\" and watch it find tickets with different wording but the same meaning!\n",
    "\n",
    "**üí° This is the power of RAG - matching by meaning, not just keywords.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Build a simple RAG system that indexes and searches IT tickets\n",
    "- ‚úÖ Understand how semantic search works (matching meaning, not keywords)\n",
    "- ‚úÖ See the difference between keyword search and semantic search\n",
    "- ‚úÖ Use RAG to answer questions using retrieved ticket context\n",
    "\n",
    "**Time:** ~15-20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üìã The Journey\n",
    "\n",
    "We'll build this step by step:\n",
    "\n",
    "1. **Explore the Data** - See what tickets look like\n",
    "2. **Understand RAG** - Learn how semantic search works\n",
    "3. **Set Up LlamaStack** - Connect to our RAG platform\n",
    "4. **Index Tickets** - Convert tickets into searchable vectors\n",
    "5. **Query & Search** - Find tickets by meaning, not keywords\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Explore the Dataset\n",
    "\n",
    "**What we're doing:** Loading IT call center tickets and examining their structure.\n",
    "\n",
    "**Why:** We need to understand what data we're working with before we can index it for semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "from llama_stack_client import RAGDocument\n",
    "\n",
    "# Load the CSV file from the data directory\n",
    "data_dir = Path(\"../data\")\n",
    "file_path = data_dir / \"synthetic-it-call-center-tickets-sample.csv\"\n",
    "\n",
    "print(\"üîÑ Loading IT call center tickets dataset...\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df)} tickets\")\n",
    "print(f\"üìã Dataset shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"\\nüîç Let's examine the dataset:\")\n",
    "print(\"=\" * 60)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "**What we see:** Each ticket has a `short_description` field that describes the problem. This is what we'll index for semantic search.\n",
    "\n",
    "**üí° Key insight:** Traditional keyword search would only find exact matches. RAG uses semantic similarity - it understands that \"application crashes\" and \"software failures\" mean similar things!\n",
    "\n",
    "Let's examine some example tickets to understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataset structure and example tickets\n",
    "print(\"üìä Dataset Structure:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nüìù Key Field for Simple RAG:\")\n",
    "print(f\"   - short_description: Problem summary (this is what we'll index)\")\n",
    "\n",
    "# Show example tickets with more detail\n",
    "print(\"\\nüìã Example Tickets:\")\n",
    "print(\"=\" * 60)\n",
    "if len(df) > 0:\n",
    "    for i in range(min(5, len(df))):\n",
    "        example = df.iloc[i]\n",
    "        desc = str(example.get('short_description', 'N/A'))\n",
    "        print(f\"\\nüé´ Ticket #{example.get('number', 'N/A')}\")\n",
    "        print(f\"   Description: {desc}\")\n",
    "        if len(desc) > 100:\n",
    "            print(f\"   (Full description: {desc[:150]}...)\")\n",
    "    \n",
    "    print(f\"\\nüí° Notice how each ticket describes a problem differently!\")\n",
    "    print(f\"   Traditional search: Would only find exact word matches\")\n",
    "    print(f\"   Semantic search: Finds tickets with similar meaning, even different words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: Understand How RAG Works\n",
    "\n",
    "**What we're learning:** How semantic search differs from traditional keyword search.\n",
    "\n",
    "**Why:** Understanding the concept helps you see why RAG is powerful for IT operations.\n",
    "\n",
    "#### üîç Keyword Search (Traditional)\n",
    "\n",
    "```\n",
    "Search: \"application crashes\"\n",
    "Results: Only tickets with exact words \"application\" AND \"crashes\"\n",
    "‚ùå Misses: \"software failures\", \"system errors\", \"app stops working\"\n",
    "```\n",
    "\n",
    "#### üß† Semantic Search (RAG)\n",
    "\n",
    "```\n",
    "Search: \"application crashes\"  \n",
    "Results: Tickets with similar meaning:\n",
    "  ‚úÖ \"application crashes\"\n",
    "  ‚úÖ \"software failures\"  \n",
    "  ‚úÖ \"system errors\"\n",
    "  ‚úÖ \"app stops working\"\n",
    "  ‚úÖ \"program terminates unexpectedly\"\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "1. **Indexing:** Convert ticket descriptions into \"embeddings\" (vectors that capture meaning)\n",
    "2. **Querying:** Convert your search query into the same type of embedding\n",
    "3. **Matching:** Find tickets whose embeddings are similar to your query embedding\n",
    "4. **Retrieval:** Return the most semantically similar tickets\n",
    "\n",
    "**üí° Think of it like:** Instead of matching words, we're matching meanings. The system understands that \"crash\" and \"failure\" mean similar things in context.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Step 3: Set Up LlamaStack Client\n",
    "\n",
    "**What we're doing:** Connecting to LlamaStack - our RAG platform that handles:\n",
    "- Vector database (stores ticket embeddings)\n",
    "- Embedding generation (converts text to vectors)\n",
    "- Semantic search (finds similar tickets)\n",
    "\n",
    "**Why:** LlamaStack provides all the RAG infrastructure we need, so we can focus on building the search system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**What happened:** We explored the dataset and understand how semantic search works. Now let's connect to LlamaStack to start building our RAG system.\n",
    "\n",
    "**What's next:** We'll initialize the LlamaStack client and verify the connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "**Ready to connect?** Let's initialize the LlamaStack client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for LlamaStack\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "# Add root src directory to path to import shared config\n",
    "root_dir = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(root_dir / \"src\"))\n",
    "\n",
    "# Import centralized configuration\n",
    "from config import LLAMA_STACK_URL, MODEL, CONFIG\n",
    "\n",
    "# Configuration values (automatically detected based on environment)\n",
    "llamastack_url = LLAMA_STACK_URL\n",
    "model = MODEL\n",
    "\n",
    "if not llamastack_url:\n",
    "    raise ValueError(\n",
    "        \"LLAMA_STACK_URL is not configured!\\n\"\n",
    "        \"Please run: ./scripts/setup-env.sh\\n\"\n",
    "        \"Or set LLAMA_STACK_URL environment variable:\\n\"\n",
    "        \"  export LLAMA_STACK_URL='https://llamastack-route-my-first-model.apps.ocp.example.com'\"\n",
    "    )\n",
    "\n",
    "print(\"üîÑ Connecting to LlamaStack...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üì° LlamaStack URL: {llamastack_url}\")\n",
    "print(f\"ü§ñ Model: {model}\")\n",
    "print(f\"üìç Environment: {'Inside OpenShift cluster' if CONFIG['inside_cluster'] else 'Outside OpenShift cluster'}\")\n",
    "print(f\"üì¶ Namespace: {CONFIG['namespace']}\")\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "client = LlamaStackClient(base_url=llamastack_url)\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    model_count = len(models.data) if hasattr(models, 'data') else len(models)\n",
    "    print(f\"\\n‚úÖ Connected to LlamaStack\")\n",
    "    print(f\"   Available models: {model_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Cannot connect to LlamaStack: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   1. Check if route exists: oc get route llamastack-route -n my-first-model\")\n",
    "    print(\"   2. Run setup script: ./scripts/setup-env.sh\")\n",
    "    print(\"   3. Or set LLAMA_STACK_URL manually in .env file\")\n",
    "    raise\n",
    "\n",
    "# Configure inference parameters\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Inference Parameters:\")\n",
    "print(f\"   Model: {model}\")\n",
    "print(f\"   Temperature: {temperature}\")\n",
    "print(f\"   Max Tokens: {max_tokens}\")\n",
    "print(f\"   Stream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "**What happened:** We connected to LlamaStack successfully! ‚úÖ\n",
    "\n",
    "**What's next:** Now we'll create a vector store (where ticket embeddings will be stored) and then index our tickets.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Create Vector Store and Index Documents\n",
    "\n",
    "**What we're doing:** Creating a ChromaDB vector store and indexing ticket descriptions for semantic search.\n",
    "\n",
    "**Why:** The vector store enables semantic search - finding tickets by meaning, not just exact keyword matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ChromaDB vector store\n",
    "print(\"\\nüîÑ Creating ChromaDB vector store...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"   - Provider: ChromaDB (embedded in LlamaStack)\")\n",
    "print(\"   - Embedding model: sentence-transformers/nomic-ai/nomic-embed-text-v1.5\")\n",
    "print(\"   - Embedding dimension: 768\")\n",
    "\n",
    "vs_chroma = client.vector_stores.create(\n",
    "    extra_body={\n",
    "        \"provider_id\": \"chromadb\",  # ChromaDB is managed by LlamaStack\n",
    "        \"embedding_model\": \"sentence-transformers/nomic-ai/nomic-embed-text-v1.5\",\n",
    "        \"embedding_dimension\": 768\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Vector store created!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üì¶ Vector Store Details:\")\n",
    "print(f\"   ID: {vs_chroma.id}\")\n",
    "print(f\"   Status: {vs_chroma.status}\")\n",
    "if vs_chroma.name:\n",
    "    print(f\"   Name: {vs_chroma.name}\")\n",
    "if vs_chroma.metadata:\n",
    "    provider = vs_chroma.metadata.get('provider_id', 'N/A')\n",
    "    print(f\"   Provider: {provider}\")\n",
    "if hasattr(vs_chroma, 'file_counts') and vs_chroma.file_counts:\n",
    "    print(f\"\\nüìä File Statistics:\")\n",
    "    print(f\"   Total files: {vs_chroma.file_counts.total}\")\n",
    "    print(f\"   Completed: {vs_chroma.file_counts.completed}\")\n",
    "    print(f\"   In progress: {vs_chroma.file_counts.in_progress}\")\n",
    "    print(f\"   Failed: {vs_chroma.file_counts.failed}\")\n",
    "    print(f\"   Cancelled: {vs_chroma.file_counts.cancelled}\")\n",
    "if hasattr(vs_chroma, 'usage_bytes') and vs_chroma.usage_bytes:\n",
    "    usage_mb = vs_chroma.usage_bytes / (1024 * 1024)\n",
    "    print(f\"\\nüíæ Storage:\")\n",
    "    print(f\"   Usage: {usage_mb:.2f} MB\")\n",
    "if hasattr(vs_chroma, 'created_at') and vs_chroma.created_at:\n",
    "    from datetime import datetime\n",
    "    created_time = datetime.fromtimestamp(vs_chroma.created_at)\n",
    "    print(f\"\\nüïí Timestamps:\")\n",
    "    print(f\"   Created: {created_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    if hasattr(vs_chroma, 'last_active_at') and vs_chroma.last_active_at:\n",
    "        last_active = datetime.fromtimestamp(vs_chroma.last_active_at)\n",
    "        print(f\"   Last active: {last_active.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "**What happened:** We created a ChromaDB vector store! ‚úÖ\n",
    "\n",
    "**üí° What is ChromaDB?** It's a vector database that stores embeddings. Think of it as a specialized database optimized for finding similar vectors (similar meanings).\n",
    "\n",
    "**Key point:** ChromaDB is embedded in LlamaStack - no separate deployment needed! This makes setup simple.\n",
    "\n",
    "**What's next:** Now we'll prepare our ticket data and convert it into a format that can be indexed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "print(\"\\nüîÑ Preparing data for indexing...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# Use all tickets (sample file already has 1000 rows)\n",
    "df_1000 = df  # Sample file already has 1000 rows\n",
    "print(f\"   Processing {len(df_1000)} tickets (out of {len(df)} total)\")\n",
    "\n",
    "# Create RAG documents using only short_description\n",
    "print(\"\\nüîÑ Creating RAG documents...\")\n",
    "print(\"   Using field: short_description (problem summary)\")\n",
    "print(\"   Storing other fields as metadata\")\n",
    "\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"ticket-{i}\",\n",
    "        content=df_1000.iloc[i][\"short_description\"],\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata=df_1000.iloc[i].drop(\"short_description\").to_dict(),\n",
    "    )\n",
    "    for i in range(len(df_1000))\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(documents)} RAG documents\")\n",
    "print(f\"\\nüí° Each document contains:\")\n",
    "print(f\"   - Content: short_description (what we'll search)\")\n",
    "print(f\"   - Metadata: All other fields (for filtering)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "**What happened:** We created RAG documents! ‚úÖ\n",
    "\n",
    "**üí° What is a RAG Document?**\n",
    "- **Content:** The `short_description` field (what we'll search)\n",
    "- **Metadata:** All other ticket fields (for filtering later)\n",
    "\n",
    "**Why \"Simple RAG\"?** We're using only one field (`short_description`) for search. This is the simplest approach.\n",
    "\n",
    "**üí° In notebook 02:** We'll see how combining multiple fields (`short_description` + `content` + `close_notes`) creates richer documents that improve search quality!\n",
    "\n",
    "**What's next:** Now we'll index these documents - LlamaStack will automatically:\n",
    "1. Chunk the text (split into manageable pieces)\n",
    "2. Generate embeddings (convert to vectors)\n",
    "3. Store in ChromaDB (make them searchable)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index documents into the vector store (in batches to avoid timeout)\n",
    "print(\"\\nüîÑ Indexing documents into vector store...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Chunk size: 1024 tokens\")\n",
    "print(f\"   Total documents: {len(documents)}\")\n",
    "print(f\"   Processing in batches of 100 to avoid timeout...\")\n",
    "\n",
    "# Process in batches to avoid gateway timeout\n",
    "BATCH_SIZE = 100\n",
    "total_batches = (len(documents) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "inserted_count = 0\n",
    "\n",
    "for batch_num in range(total_batches):\n",
    "    start_idx = batch_num * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(documents))\n",
    "    batch = documents[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"\\n   Batch {batch_num + 1}/{total_batches}: Processing documents {start_idx} to {end_idx-1}...\")\n",
    "    \n",
    "    try:\n",
    "        insert_result = client.tool_runtime.rag_tool.insert( \n",
    "            chunk_size_in_tokens=1024,\n",
    "            documents=batch,\n",
    "            vector_db_id=str(vs_chroma.id),\n",
    "            extra_body={\"vector_store_id\": str(vs_chroma.id)},\n",
    "            extra_headers=None,\n",
    "            extra_query=None,\n",
    "            timeout=300  # 5 minute timeout per batch\n",
    "        )\n",
    "        inserted_count += len(batch)\n",
    "        print(f\"   ‚úÖ Batch {batch_num + 1} indexed successfully ({inserted_count}/{len(documents)} documents)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error indexing batch {batch_num + 1}: {e}\")\n",
    "        print(f\"   üí° Tip: You can continue with the documents already indexed, or reduce BATCH_SIZE\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Indexing complete!\")\n",
    "print(f\"   Successfully indexed: {inserted_count}/{len(documents)} documents\")\n",
    "print(f\"   Vector store ID: {vs_chroma.id}\")\n",
    "print(f\"\\nüí° LlamaStack automatically:\")\n",
    "print(f\"   - Chunked the documents\")\n",
    "print(f\"   - Generated embeddings for each chunk\")\n",
    "print(f\"   - Stored them in ChromaDB for semantic search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display vector store with documents after indexing\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä Vector Store Status After Indexing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Retrieve updated vector store information\n",
    "vs_updated = client.vector_stores.retrieve(vs_chroma.id)\n",
    "\n",
    "print(f\"\\nüì¶ Vector Store Details:\")\n",
    "print(f\"   ID: {vs_updated.id}\")\n",
    "print(f\"   Status: {vs_updated.status}\")\n",
    "if vs_updated.name:\n",
    "    print(f\"   Name: {vs_updated.name}\")\n",
    "if vs_updated.metadata:\n",
    "    provider = vs_updated.metadata.get(\"provider_id\", \"N/A\")\n",
    "    print(f\"   Provider: {provider}\")\n",
    "\n",
    "# Display file/document statistics\n",
    "if hasattr(vs_updated, \"file_counts\") and vs_updated.file_counts:\n",
    "    print(f\"\\nüìä Document Statistics:\")\n",
    "    print(f\"   Total files: {vs_updated.file_counts.total}\")\n",
    "    print(f\"   Completed: {vs_updated.file_counts.completed}\")\n",
    "    print(f\"   In progress: {vs_updated.file_counts.in_progress}\")\n",
    "    print(f\"   Failed: {vs_updated.file_counts.failed}\")\n",
    "    print(f\"   Cancelled: {vs_updated.file_counts.cancelled}\")\n",
    "\n",
    "# Display storage usage\n",
    "if hasattr(vs_updated, \"usage_bytes\") and vs_updated.usage_bytes:\n",
    "    usage_mb = vs_updated.usage_bytes / (1024 * 1024)\n",
    "    print(f\"\\nüíæ Storage:\")\n",
    "    print(f\"   Usage: {usage_mb:.2f} MB\")\n",
    "\n",
    "# Display timestamps\n",
    "if hasattr(vs_updated, \"created_at\") and vs_updated.created_at:\n",
    "    from datetime import datetime\n",
    "\n",
    "    created_time = datetime.fromtimestamp(vs_updated.created_at)\n",
    "    print(f\"\\nüïí Timestamps:\")\n",
    "    print(f\"   Created: {created_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    if hasattr(vs_updated, \"last_active_at\") and vs_updated.last_active_at:\n",
    "        last_active = datetime.fromtimestamp(vs_updated.last_active_at)\n",
    "        print(f\"   Last active: {last_active.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Query the vector store to show sample documents\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"üîç Sample Documents in Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Querying vector store to retrieve sample documents...\")\n",
    "\n",
    "try:\n",
    "    # Query with a general query to get some sample results\n",
    "    sample_query = \"IT support ticket\"\n",
    "    query_result = client.tool_runtime.rag_tool.query(\n",
    "        content=sample_query,\n",
    "        vector_db_ids=[str(vs_chroma.id)],\n",
    "        extra_body={\"vector_store_ids\": [str(vs_chroma.id)]},\n",
    "    )\n",
    "\n",
    "    print(f\"\\n‚úÖ Vector store is queryable and contains indexed documents!\")\n",
    "    print(f\"\\nüìÑ Sample Query Results:\")\n",
    "    print(f\"   Query: '{sample_query}'\")\n",
    "\n",
    "    # Extract document information if available\n",
    "    if hasattr(query_result, \"content\") and query_result.content:\n",
    "        content_preview = (\n",
    "            query_result.content[:300]\n",
    "            if len(query_result.content) > 300\n",
    "            else query_result.content\n",
    "        )\n",
    "        print(f\"\\n   Retrieved content preview:\")\n",
    "        print(f\"   {content_preview}...\")\n",
    "\n",
    "    print(f\"\\nüí° The vector store is ready for semantic search!\")\n",
    "    print(f\"   You can now query it to find tickets by meaning, not just keywords.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Could not query vector store: {e}\")\n",
    "    print(f\"   Vector store may still be processing documents.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "**What happened:** We indexed all documents into ChromaDB! ‚úÖ\n",
    "\n",
    "**üéâ Success!** The tickets are now searchable using semantic similarity. Each ticket description has been:\n",
    "- ‚úÖ Converted into embeddings (vectors representing meaning)\n",
    "- ‚úÖ Stored in ChromaDB (ready for semantic search)\n",
    "\n",
    "**üí° What happened behind the scenes:**\n",
    "- LlamaStack automatically chunked long descriptions\n",
    "- Generated embeddings using the embedding model\n",
    "- Stored them in the vector database\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Query with RAG - See Semantic Search in Action!\n",
    "\n",
    "**What we're doing:** Testing our RAG system with a query to see how semantic search works.\n",
    "\n",
    "**Why:** This is where you'll see the power of RAG - finding relevant tickets even when they use different words. It matches meaning, not just keywords!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "**Ready to see semantic search in action?** Let's query our indexed tickets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "**What happened:** We indexed the tickets successfully! ‚úÖ\n",
    "\n",
    "**What's next:** Now let's query the system to see semantic search in action. Watch how it finds tickets with similar meaning, even if they use different words!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test query\n",
    "query = \"What was the root cause and resolution for application crashes related to memory issues?\"\n",
    "\n",
    "print(\"üîç Querying RAG System\")\n",
    "print(\"=\" * 60)\n",
    "cprint(f\"\\nüìù User Query: {query}\", \"blue\")\n",
    "print(\"\\nüí° This query will use semantic search to find:\")\n",
    "print(\"   - Tickets about 'application crashes'\")\n",
    "print(\"   - Tickets about 'memory issues'\")\n",
    "print(\"   - Even if they use different words!\")\n",
    "print(\"\\nüîÑ Searching vector store...\")\n",
    "\n",
    "# Step 1: RAG retrieval - find relevant document chunks\n",
    "rag_response = client.tool_runtime.rag_tool.query(\n",
    "    content=query,\n",
    "    vector_db_ids=[str(vs_chroma.id)],\n",
    "    extra_body={\"vector_store_ids\": [str(vs_chroma.id)]},\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Retrieved relevant context from vector store\")\n",
    "print(f\"\\nüìÑ Retrieved Context (first 500 chars):\")\n",
    "print(\"=\" * 60)\n",
    "print(rag_response.content[:500] + \"...\" if len(rag_response.content) > 500 else rag_response.content)\n",
    "print(\"\\nüí° Notice: The system found relevant tickets using semantic similarity!\")\n",
    "\n",
    "# Step 2: Construct extended prompt with retrieved context\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful IT support assistant.\"}]\n",
    "extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{rag_response.content}\\n\\nQUERY:\\n{query}\"\n",
    "messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "# Step 3: Generate answer using LLM\n",
    "print(\"\\nüîÑ Generating answer with LLM...\")\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=model,\n",
    "    stream=stream,\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Answer:\")\n",
    "print(\"=\" * 60)\n",
    "if stream:\n",
    "    for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "    print()\n",
    "else:\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "**What happened:** We used RAG to find relevant tickets and generate an answer! The system matched meaning, not just keywords.\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaway\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) enables semantic search - finding documents by meaning, not just exact keyword matches. By indexing ticket descriptions into a vector database, we can search for similar incidents even when they use different words. This is the foundation for intelligent IT operations search systems.\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Connection\n",
    "\n",
    "**How this applies to IT Operations:**\n",
    "\n",
    "The same RAG approach can be used for:\n",
    "\n",
    "- **Incident Resolution:** \"Find similar incidents to this one\" ‚Üí Get past solutions\n",
    "- **Knowledge Base Search:** Search through documentation and runbooks using natural language\n",
    "- **Pattern Recognition:** Identify recurring problems across incidents\n",
    "- **Root Cause Analysis:** Find incidents with similar symptoms to learn from past diagnostics\n",
    "\n",
    "**The pattern is the same:** Index historical data ‚Üí Query semantically ‚Üí Retrieve relevant context ‚Üí Use context to answer questions or suggest solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® Your Turn\n",
    "\n",
    "**Try this:** Modify the query to search for different types of problems. For example:\n",
    "- \"How do I fix database connection errors?\"\n",
    "- \"What causes slow application performance?\"\n",
    "- \"Find tickets about network issues\"\n",
    "\n",
    "Notice how the semantic search finds relevant tickets even with different wording!\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ You Did It!\n",
    "\n",
    "You've built your first RAG system! You learned how to index IT tickets and enable semantic search that understands meaning, not just keywords. \n",
    "\n",
    "**Next:** `02_multifield_RAG_llama_stack_chromadb.ipynb` - Learn how combining multiple fields (problem + diagnosis + solution) creates even better search results!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Want to Go Deeper?\n",
    "\n",
    "<details>\n",
    "<summary>üìñ Additional Resources (Click to expand)</summary>\n",
    "\n",
    "- [LlamaStack Documentation](https://github.com/llamastack/llamastack) - RAG and vector store capabilities\n",
    "- [ChromaDB Documentation](https://www.trychroma.com/) - Vector database used by LlamaStack\n",
    "- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/) - RAG techniques and patterns\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
