{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 01: Build Your First RAG System for IT Ticket Search\n",
    "\n",
    "## üéØ Your Mission\n",
    "\n",
    "You're an IT support engineer building your first intelligent ticket search system. Your job today: index IT tickets into a vector database and enable semantic search that finds similar incidents, even when they use different words.\n",
    "\n",
    "**Why this matters:** This same RAG (Retrieval-Augmented Generation) approach is how you could build search systems that understand meaning, not just keywords - enabling faster incident resolution by finding similar past problems and their solutions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## ‚ö° Quick Win (First 2 Minutes)\n",
    "\n",
    "Let's see RAG in action! Run the cell below to see how semantic search finds relevant tickets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "tags": []
   },
   "source": [
    "**What you'll see:** A RAG system that can find relevant IT tickets using semantic similarity - matching meaning, not just exact keywords. For example, searching for \"application crashes\" will find tickets about \"software failures\" and \"system errors\" too!\n",
    "\n",
    "Now let's build it step by step to understand how it works.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Build a simple RAG system that indexes and searches IT tickets\n",
    "- ‚úÖ Understand how semantic search works (matching meaning, not keywords)\n",
    "- ‚úÖ Use RAG to answer questions using retrieved ticket context\n",
    "\n",
    "**Time:** ~15-20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üìã The Journey\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Load and Explore the Dataset\n",
    "\n",
    "**What we're doing:** Loading IT call center tickets and examining their structure.\n",
    "\n",
    "**Why:** We need to understand the data before indexing it for semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "from llama_stack_client import RAGDocument\n",
    "\n",
    "# Load the CSV file from the data directory\n",
    "data_dir = Path(\"../data\")\n",
    "file_path = data_dir / \"synthetic-it-call-center-tickets.csv\"\n",
    "\n",
    "print(\"üîÑ Loading IT call center tickets dataset...\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df)} tickets\")\n",
    "print(f\"üìã Dataset shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"\\nüîç Let's examine the dataset:\")\n",
    "print(\"=\" * 60)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "**What we see:** Each ticket has a `short_description` field that describes the problem. This is what we'll index for semantic search.\n",
    "\n",
    "**üí° Key insight:** Traditional keyword search would only find exact matches. RAG uses semantic similarity - it understands that \"application crashes\" and \"software failures\" mean similar things!\n",
    "\n",
    "Let's see the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataset structure and example tickets\n",
    "print(\"üìä Dataset Structure:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nüìù Key Field for Simple RAG:\")\n",
    "print(f\"   - short_description: Problem summary (this is what we'll index)\")\n",
    "\n",
    "# Show example tickets\n",
    "print(\"\\nüìã Example Tickets:\")\n",
    "print(\"=\" * 60)\n",
    "if len(df) > 0:\n",
    "    for i in range(min(3, len(df))):\n",
    "        example = df.iloc[i]\n",
    "        print(f\"\\nüé´ Ticket #{example.get('number', 'N/A')}\")\n",
    "        print(f\"   Description: {str(example.get('short_description', 'N/A'))[:100]}...\")\n",
    "    print(f\"\\nüí° We'll index these descriptions for semantic search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: Set Up LlamaStack Client\n",
    "\n",
    "**What we're doing:** Connecting to LlamaStack and configuring our environment.\n",
    "\n",
    "**Why:** We need LlamaStack to handle vector database operations, embeddings, and RAG queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "**What happened:** We explored the dataset. Now let's connect to LlamaStack.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**What happened:** We connected to LlamaStack. Now we're ready to create the vector store and index documents.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Create Vector Store and Index Documents\n",
    "\n",
    "**What we're doing:** Creating a ChromaDB vector store and indexing ticket descriptions for semantic search.\n",
    "\n",
    "**Why:** The vector store enables semantic search - finding tickets by meaning, not just exact keyword matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for LlamaStack\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "# Add root src directory to path to import shared config\n",
    "root_dir = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(root_dir / \"src\"))\n",
    "\n",
    "# Import centralized configuration\n",
    "from config import LLAMA_STACK_URL, MODEL, CONFIG\n",
    "\n",
    "# Configuration values (automatically detected based on environment)\n",
    "llamastack_url = LLAMA_STACK_URL\n",
    "model = MODEL\n",
    "\n",
    "if not llamastack_url:\n",
    "    raise ValueError(\n",
    "        \"LLAMA_STACK_URL is not configured!\\n\"\n",
    "        \"Please run: ./scripts/setup-env.sh\\n\"\n",
    "        \"Or set LLAMA_STACK_URL environment variable:\\n\"\n",
    "        \"  export LLAMA_STACK_URL='https://llamastack-route-my-first-model.apps.ocp.example.com'\"\n",
    "    )\n",
    "\n",
    "print(\"üîÑ Step 1: Connecting to LlamaStack...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üì° LlamaStack URL: {llamastack_url}\")\n",
    "print(f\"ü§ñ Model: {model}\")\n",
    "print(f\"üìç Environment: {'Inside OpenShift cluster' if CONFIG['inside_cluster'] else 'Outside OpenShift cluster'}\")\n",
    "print(f\"üì¶ Namespace: {CONFIG['namespace']}\")\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "client = LlamaStackClient(base_url=llamastack_url)\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    model_count = len(models.data) if hasattr(models, 'data') else len(models)\n",
    "    print(f\"\\n‚úÖ Connected to LlamaStack\")\n",
    "    print(f\"   Available models: {model_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Cannot connect to LlamaStack: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   1. Check if route exists: oc get route llamastack-route -n my-first-model\")\n",
    "    print(\"   2. Run setup script: ./scripts/setup-env.sh\")\n",
    "    print(\"   3. Or set LLAMA_STACK_URL manually in .env file\")\n",
    "    raise\n",
    "\n",
    "# Configure inference parameters\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Inference Parameters:\")\n",
    "print(f\"   Model: {model}\")\n",
    "print(f\"   Temperature: {temperature}\")\n",
    "print(f\"   Max Tokens: {max_tokens}\")\n",
    "print(f\"   Stream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "**What happened:** We connected to LlamaStack and configured our inference parameters. Now let's create the vector store.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create ChromaDB vector store\n",
    "print(\"\\nüîÑ Step 1: Creating ChromaDB vector store...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"   - Provider: ChromaDB (embedded in LlamaStack)\")\n",
    "print(\"   - Embedding model: sentence-transformers/nomic-ai/nomic-embed-text-v1.5\")\n",
    "print(\"   - Embedding dimension: 768\")\n",
    "\n",
    "vs_chroma = client.vector_stores.create(\n",
    "    extra_body={\n",
    "        \"provider_id\": \"chromadb\",  # ChromaDB is managed by LlamaStack\n",
    "        \"embedding_model\": \"sentence-transformers/nomic-ai/nomic-embed-text-v1.5\",\n",
    "        \"embedding_dimension\": 768\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Vector store created!\")\n",
    "print(f\"   Vector Store ID: {vs_chroma.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "**What happened:** We created a ChromaDB vector store. ChromaDB is embedded in LlamaStack (no separate deployment needed).\n",
    "\n",
    "Now let's prepare the ticket data for indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the data\n",
    "print(\"\\nüîÑ Step 2: Preparing data for indexing...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# Limit to first 1000 records for faster processing (you can use more for production)\n",
    "df_1000 = df.head(1000)\n",
    "print(f\"   Processing {len(df_1000)} tickets (out of {len(df)} total)\")\n",
    "\n",
    "# Step 3: Create RAG documents using only short_description\n",
    "print(\"\\nüîÑ Step 3: Creating RAG documents...\")\n",
    "print(\"   Using field: short_description (problem summary)\")\n",
    "print(\"   Storing other fields as metadata\")\n",
    "\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"ticket-{i}\",\n",
    "        content=df_1000.iloc[i][\"short_description\"],\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata=df_1000.iloc[i].drop(\"short_description\").to_dict(),\n",
    "    )\n",
    "    for i in range(len(df_1000))\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(documents)} RAG documents\")\n",
    "print(f\"\\nüí° Each document contains:\")\n",
    "print(f\"   - Content: short_description (what we'll search)\")\n",
    "print(f\"   - Metadata: All other fields (for filtering)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "**What happened:** We created RAG documents using only the `short_description` field. This is \"simple RAG\" - using one field for search.\n",
    "\n",
    "**üí° Note:** In the next notebook (02), we'll see how combining multiple fields (`short_description` + `content` + `close_notes`) creates richer documents that improve search quality!\n",
    "\n",
    "Now let's index these documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Index documents into the vector store (in batches to avoid timeout)\n",
    "print(\"\\nüîÑ Step 4: Indexing documents into vector store...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Chunk size: 1024 tokens\")\n",
    "print(f\"   Total documents: {len(documents)}\")\n",
    "print(f\"   Processing in batches of 100 to avoid timeout...\")\n",
    "\n",
    "# Process in batches to avoid gateway timeout\n",
    "BATCH_SIZE = 100\n",
    "total_batches = (len(documents) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "inserted_count = 0\n",
    "\n",
    "for batch_num in range(total_batches):\n",
    "    start_idx = batch_num * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(documents))\n",
    "    batch = documents[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"\\n   Batch {batch_num + 1}/{total_batches}: Processing documents {start_idx} to {end_idx-1}...\")\n",
    "    \n",
    "    try:\n",
    "        insert_result = client.tool_runtime.rag_tool.insert( \n",
    "            chunk_size_in_tokens=1024,\n",
    "            documents=batch,\n",
    "            vector_db_id=str(vs_chroma.id),\n",
    "            extra_body={\"vector_store_id\": str(vs_chroma.id)},\n",
    "            extra_headers=None,\n",
    "            extra_query=None,\n",
    "            timeout=300  # 5 minute timeout per batch\n",
    "        )\n",
    "        inserted_count += len(batch)\n",
    "        print(f\"   ‚úÖ Batch {batch_num + 1} indexed successfully ({inserted_count}/{len(documents)} documents)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error indexing batch {batch_num + 1}: {e}\")\n",
    "        print(f\"   üí° Tip: You can continue with the documents already indexed, or reduce BATCH_SIZE\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Indexing complete!\")\n",
    "print(f\"   Successfully indexed: {inserted_count}/{len(documents)} documents\")\n",
    "print(f\"   Vector store ID: {vs_chroma.id}\")\n",
    "print(f\"\\nüí° LlamaStack automatically:\")\n",
    "print(f\"   - Chunked the documents\")\n",
    "print(f\"   - Generated embeddings for each chunk\")\n",
    "print(f\"   - Stored them in ChromaDB for semantic search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "**What happened:** We indexed all documents into ChromaDB! The tickets are now searchable using semantic similarity.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Query with RAG\n",
    "\n",
    "**What we're doing:** Testing our RAG system with a query to see how semantic search works.\n",
    "\n",
    "**Why:** RAG enables finding relevant tickets even when they use different words - it matches meaning, not just keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Let's test semantic search with a query:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "**What happened:** We indexed the tickets. Now let's query the system to see semantic search in action!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test query\n",
    "query = \"What was the root cause and resolution for application crashes related to memory issues?\"\n",
    "\n",
    "print(\"üîç Querying RAG System\")\n",
    "print(\"=\" * 60)\n",
    "cprint(f\"\\nüìù User Query: {query}\", \"blue\")\n",
    "print(\"\\nüí° This query will use semantic search to find:\")\n",
    "print(\"   - Tickets about 'application crashes'\")\n",
    "print(\"   - Tickets about 'memory issues'\")\n",
    "print(\"   - Even if they use different words!\")\n",
    "print(\"\\nüîÑ Searching vector store...\")\n",
    "\n",
    "# Step 1: RAG retrieval - find relevant document chunks\n",
    "rag_response = client.tool_runtime.rag_tool.query(\n",
    "    content=query,\n",
    "    vector_db_ids=[str(vs_chroma.id)],\n",
    "    extra_body={\"vector_store_ids\": [str(vs_chroma.id)]},\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Retrieved relevant context from vector store\")\n",
    "print(f\"\\nüìÑ Retrieved Context (first 500 chars):\")\n",
    "print(\"=\" * 60)\n",
    "print(rag_response.content[:500] + \"...\" if len(rag_response.content) > 500 else rag_response.content)\n",
    "print(\"\\nüí° Notice: The system found relevant tickets using semantic similarity!\")\n",
    "\n",
    "# Step 2: Construct extended prompt with retrieved context\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful IT support assistant.\"}]\n",
    "extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{rag_response.content}\\n\\nQUERY:\\n{query}\"\n",
    "messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "# Step 3: Generate answer using LLM\n",
    "print(\"\\nüîÑ Generating answer with LLM...\")\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=model,\n",
    "    stream=stream,\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Answer:\")\n",
    "print(\"=\" * 60)\n",
    "if stream:\n",
    "    for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "    print()\n",
    "else:\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "**What happened:** We used RAG to find relevant tickets and generate an answer! The system matched meaning, not just keywords.\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaway\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) enables semantic search - finding documents by meaning, not just exact keyword matches. By indexing ticket descriptions into a vector database, we can search for similar incidents even when they use different words. This is the foundation for intelligent IT operations search systems.\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Connection\n",
    "\n",
    "**How this applies to IT Operations:**\n",
    "\n",
    "The same RAG approach can be used for:\n",
    "\n",
    "- **Incident Resolution:** \"Find similar incidents to this one\" ‚Üí Get past solutions\n",
    "- **Knowledge Base Search:** Search through documentation and runbooks using natural language\n",
    "- **Pattern Recognition:** Identify recurring problems across incidents\n",
    "- **Root Cause Analysis:** Find incidents with similar symptoms to learn from past diagnostics\n",
    "\n",
    "**The pattern is the same:** Index historical data ‚Üí Query semantically ‚Üí Retrieve relevant context ‚Üí Use context to answer questions or suggest solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® Your Turn\n",
    "\n",
    "**Try this:** Modify the query to search for different types of problems. For example:\n",
    "- \"How do I fix database connection errors?\"\n",
    "- \"What causes slow application performance?\"\n",
    "- \"Find tickets about network issues\"\n",
    "\n",
    "Notice how the semantic search finds relevant tickets even with different wording!\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ You Did It!\n",
    "\n",
    "You've built your first RAG system! You learned how to index IT tickets and enable semantic search that understands meaning, not just keywords. \n",
    "\n",
    "**Next:** `02_multifield_RAG_llama_stack_chromadb.ipynb` - Learn how combining multiple fields (problem + diagnosis + solution) creates even better search results!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Want to Go Deeper?\n",
    "\n",
    "<details>\n",
    "<summary>üìñ Additional Resources (Click to expand)</summary>\n",
    "\n",
    "- [LlamaStack Documentation](https://github.com/llamastack/llamastack) - RAG and vector store capabilities\n",
    "- [ChromaDB Documentation](https://www.trychroma.com/) - Vector database used by LlamaStack\n",
    "- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/) - RAG techniques and patterns\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
