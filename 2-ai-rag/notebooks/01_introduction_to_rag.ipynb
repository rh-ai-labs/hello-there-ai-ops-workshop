{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 01: Introduction to RAG - Semantic Search Basics\n",
    "\n",
    "## üéØ Your Mission\n",
    "\n",
    "You're an IT support engineer learning to use semantic search with RAG. Your job today: query a pre-indexed vector database to find IT tickets using semantic similarity - matching meaning, not just keywords.\n",
    "\n",
    "**Why this matters:** This same RAG (Retrieval-Augmented Generation) approach is how you could build search systems that understand meaning, not just keywords - enabling faster incident resolution by finding similar past problems and their solutions.\n",
    "\n",
    "**üìã Prerequisites:** \n",
    "- Complete `00_data_ingestion.ipynb` first (it creates both vector stores)\n",
    "- The single-field vector store will be automatically found by name\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## ‚ö° Quick Win: See Semantic Search in Action\n",
    "\n",
    "**The Problem:** Traditional keyword search only finds exact matches. If you search for \"application crashes\", it won't find tickets about \"software failures\" or \"system errors\" - even though they're the same problem!\n",
    "\n",
    "**The Solution:** Semantic search understands meaning. It knows that \"application crashes\", \"software failures\", and \"system errors\" are similar concepts.\n",
    "\n",
    "**Try this:** After we build the system, search for \"application crashes\" and watch it find tickets with different wording but the same meaning!\n",
    "\n",
    "**üí° This is the power of RAG - matching by meaning, not just keywords.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Build a simple RAG system that indexes and searches IT tickets\n",
    "- ‚úÖ Understand how semantic search works (matching meaning, not keywords)\n",
    "- ‚úÖ See the difference between keyword search and semantic search\n",
    "- ‚úÖ Use RAG to answer questions using retrieved ticket context\n",
    "\n",
    "**Time:** ~15-20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üìã The Journey\n",
    "\n",
    "We'll query the RAG system step by step:\n",
    "\n",
    "1. **Explore the Data** - See what tickets look like\n",
    "2. **Understand RAG** - Learn how semantic search works\n",
    "3. **Set Up LlamaStack** - Connect to our RAG platform\n",
    "4. **Load Vector Store** - Connect to the pre-indexed vector database\n",
    "5. **Query & Search** - Find tickets by meaning, not keywords\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Explore the Dataset\n",
    "\n",
    "**What we're doing:** Loading IT call center tickets and examining their structure.\n",
    "\n",
    "**Why:** We need to understand what data we're working with before we can index it for semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "from llama_stack_client import RAGDocument\n",
    "\n",
    "# Load the CSV file from the data directory\n",
    "data_dir = Path(\"../data\")\n",
    "file_path = data_dir / \"synthetic-it-call-center-tickets-sample.csv\"\n",
    "\n",
    "print(\"üîÑ Loading IT call center tickets dataset...\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df)} tickets\")\n",
    "print(f\"üìã Dataset shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"\\nüîç Let's examine the dataset:\")\n",
    "print(\"=\" * 60)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "**What we see:** Each ticket has a `short_description` field that describes the problem. This is what we'll index for semantic search.\n",
    "\n",
    "**üí° Key insight:** Traditional keyword search would only find exact matches. RAG uses semantic similarity - it understands that \"application crashes\" and \"software failures\" mean similar things!\n",
    "\n",
    "Let's examine some example tickets to understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataset structure and example tickets\n",
    "print(\"üìä Dataset Structure:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nüìù Key Field for Simple RAG:\")\n",
    "print(f\"   - short_description: Problem summary (this is what we'll index)\")\n",
    "\n",
    "# Show example tickets with more detail\n",
    "print(\"\\nüìã Example Tickets:\")\n",
    "print(\"=\" * 60)\n",
    "if len(df) > 0:\n",
    "    for i in range(min(5, len(df))):\n",
    "        example = df.iloc[i]\n",
    "        desc = str(example.get('short_description', 'N/A'))\n",
    "        print(f\"\\nüé´ Ticket #{example.get('number', 'N/A')}\")\n",
    "        print(f\"   Description: {desc}\")\n",
    "        if len(desc) > 100:\n",
    "            print(f\"   (Full description: {desc[:150]}...)\")\n",
    "    \n",
    "    print(f\"\\nüí° Notice how each ticket describes a problem differently!\")\n",
    "    print(f\"   Traditional search: Would only find exact word matches\")\n",
    "    print(f\"   Semantic search: Finds tickets with similar meaning, even different words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: Understand How RAG Works\n",
    "\n",
    "**What we're learning:** How semantic search differs from traditional keyword search.\n",
    "\n",
    "**Why:** Understanding the concept helps you see why RAG is powerful for IT operations.\n",
    "\n",
    "#### üîç Keyword Search (Traditional)\n",
    "\n",
    "```\n",
    "Search: \"application crashes\"\n",
    "Results: Only tickets with exact words \"application\" AND \"crashes\"\n",
    "‚ùå Misses: \"software failures\", \"system errors\", \"app stops working\"\n",
    "```\n",
    "\n",
    "#### üß† Semantic Search (RAG)\n",
    "\n",
    "```\n",
    "Search: \"application crashes\"  \n",
    "Results: Tickets with similar meaning:\n",
    "  ‚úÖ \"application crashes\"\n",
    "  ‚úÖ \"software failures\"  \n",
    "  ‚úÖ \"system errors\"\n",
    "  ‚úÖ \"app stops working\"\n",
    "  ‚úÖ \"program terminates unexpectedly\"\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "1. **Indexing:** Convert ticket descriptions into \"embeddings\" (vectors that capture meaning)\n",
    "2. **Querying:** Convert your search query into the same type of embedding\n",
    "3. **Matching:** Find tickets whose embeddings are similar to your query embedding\n",
    "4. **Retrieval:** Return the most semantically similar tickets\n",
    "\n",
    "**üí° Think of it like:** Instead of matching words, we're matching meanings. The system understands that \"crash\" and \"failure\" mean similar things in context.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Step 3: Set Up LlamaStack Client\n",
    "\n",
    "**What we're doing:** Connecting to LlamaStack - our RAG platform that handles:\n",
    "- Vector database (stores ticket embeddings)\n",
    "- Embedding generation (converts text to vectors)\n",
    "- Semantic search (finds similar tickets)\n",
    "\n",
    "**Why:** LlamaStack provides all the RAG infrastructure we need, so we can focus on building the search system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**What happened:** We explored the dataset and understand how semantic search works. Now let's connect to LlamaStack to start building our RAG system.\n",
    "\n",
    "**What's next:** We'll initialize the LlamaStack client and verify the connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "**Ready to connect?** Let's initialize the LlamaStack client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for LlamaStack\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "# Add root src directory to path to import shared config\n",
    "root_dir = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(root_dir / \"src\"))\n",
    "\n",
    "# Import centralized configuration\n",
    "from config import LLAMA_STACK_URL, MODEL, CONFIG\n",
    "\n",
    "# Configuration values (automatically detected based on environment)\n",
    "llamastack_url = LLAMA_STACK_URL\n",
    "model = MODEL\n",
    "\n",
    "if not llamastack_url:\n",
    "    raise ValueError(\n",
    "        \"LLAMA_STACK_URL is not configured!\\n\"\n",
    "        \"Please run: ./scripts/setup-env.sh\\n\"\n",
    "        \"Or set LLAMA_STACK_URL environment variable:\\n\"\n",
    "        \"  export LLAMA_STACK_URL='https://llamastack-route-my-first-model.apps.ocp.example.com'\"\n",
    "    )\n",
    "\n",
    "print(\"üîÑ Connecting to LlamaStack...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üì° LlamaStack URL: {llamastack_url}\")\n",
    "print(f\"ü§ñ Model: {model}\")\n",
    "print(f\"üìç Environment: {'Inside OpenShift cluster' if CONFIG['inside_cluster'] else 'Outside OpenShift cluster'}\")\n",
    "print(f\"üì¶ Namespace: {CONFIG['namespace']}\")\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "client = LlamaStackClient(base_url=llamastack_url)\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    model_count = len(models.data) if hasattr(models, 'data') else len(models)\n",
    "    print(f\"\\n‚úÖ Connected to LlamaStack\")\n",
    "    print(f\"   Available models: {model_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Cannot connect to LlamaStack: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   1. Check if route exists: oc get route llamastack-route -n my-first-model\")\n",
    "    print(\"   2. Run setup script: ./scripts/setup-env.sh\")\n",
    "    print(\"   3. Or set LLAMA_STACK_URL manually in .env file\")\n",
    "    raise\n",
    "\n",
    "# Configure inference parameters\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Inference Parameters:\")\n",
    "print(f\"   Model: {model}\")\n",
    "print(f\"   Temperature: {temperature}\")\n",
    "print(f\"   Max Tokens: {max_tokens}\")\n",
    "print(f\"   Stream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "**What happened:** We connected to LlamaStack successfully! ‚úÖ\n",
    "\n",
    "**What's next:** Now we'll create a vector store (where ticket embeddings will be stored) and then index our tickets.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Load Vector Store\n",
    "\n",
    "**What we're doing:** Connecting to the pre-indexed vector store created in `00_data_ingestion.ipynb`.\n",
    "\n",
    "**Why:** The vector store already contains indexed tickets. We just need to connect to it to start querying.\n",
    "\n",
    "**üìã Prerequisites:** \n",
    "- You must have completed `00_data_ingestion.ipynb` (it creates both vector stores)\n",
    "- The single-field vector store will be automatically found by name\n",
    "\n",
    "**üí° How it works:**\n",
    "- The ingestion notebook creates a vector store named `single-field-rag-tickets`\n",
    "- This notebook will automatically find it by name\n",
    "- Or you can manually set `VECTOR_STORE_ID_SINGLE_FIELD` if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector store ID\n",
    "# Option 1: Use the vector store ID from the ingestion notebook (if you ran it in the same session)\n",
    "# Option 2: Manually set it here if you have a saved vector store ID\n",
    "\n",
    "print(\"\\nüîÑ Step 4: Loading vector store...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Try to use VECTOR_STORE_ID_SINGLE_FIELD from ingestion notebook, or find by name\n",
    "try:\n",
    "    # Check if VECTOR_STORE_ID_SINGLE_FIELD exists from ingestion notebook\n",
    "    if 'VECTOR_STORE_ID_SINGLE_FIELD' in globals():\n",
    "        vector_store_id = VECTOR_STORE_ID_SINGLE_FIELD\n",
    "        VECTOR_STORE_ID = VECTOR_STORE_ID_SINGLE_FIELD  # For backward compatibility\n",
    "        print(f\"‚úÖ Using vector store ID from ingestion notebook\")\n",
    "    else:\n",
    "        raise NameError(\"VECTOR_STORE_ID_SINGLE_FIELD not found\")\n",
    "except NameError:\n",
    "    # Try to find vector store by name\n",
    "    print(\"üîÑ Attempting to find vector store by name 'single-field-rag-tickets'...\")\n",
    "    try:\n",
    "        vector_stores = client.vector_stores.list()\n",
    "        found = False\n",
    "        for vs in vector_stores.data if hasattr(vector_stores, 'data') else []:\n",
    "            if hasattr(vs, 'name') and vs.name == 'single-field-rag-tickets':\n",
    "                VECTOR_STORE_ID_SINGLE_FIELD = vs.id\n",
    "                VECTOR_STORE_ID = vs.id  # For backward compatibility\n",
    "                vector_store_id = vs.id\n",
    "                found = True\n",
    "                print(f\"‚úÖ Found vector store: {vs.id}\")\n",
    "                break\n",
    "        if not found:\n",
    "            raise ValueError(\"Vector store not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not find vector store: {e}\")\n",
    "        print(f\"\\nüí° Please run 00_data_ingestion.ipynb first!\")\n",
    "        print(f\"   It will create both vector stores (single-field and multi-field)\")\n",
    "        raise\n",
    "\n",
    "# Retrieve vector store information\n",
    "vs_chroma = client.vector_stores.retrieve(VECTOR_STORE_ID_SINGLE_FIELD)\n",
    "\n",
    "print(f\"\\n‚úÖ Vector store loaded!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üì¶ Vector Store Details:\")\n",
    "print(f\"   ID: {vs_chroma.id}\")\n",
    "print(f\"   Status: {vs_chroma.status}\")\n",
    "if vs_chroma.name:\n",
    "    print(f\"   Name: {vs_chroma.name}\")\n",
    "if vs_chroma.metadata:\n",
    "    provider = vs_chroma.metadata.get('provider_id', 'N/A')\n",
    "    print(f\"   Provider: {provider}\")\n",
    "\n",
    "# Display document statistics\n",
    "if hasattr(vs_chroma, 'file_counts') and vs_chroma.file_counts:\n",
    "    print(f\"\\nüìä Document Statistics:\")\n",
    "    print(f\"   Total files: {vs_chroma.file_counts.total}\")\n",
    "    print(f\"   Completed: {vs_chroma.file_counts.completed}\")\n",
    "    print(f\"   In progress: {vs_chroma.file_counts.in_progress}\")\n",
    "    print(f\"   Failed: {vs_chroma.file_counts.failed}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüí° Vector store is ready for semantic search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "**What happened:** We loaded the vector store! ‚úÖ\n",
    "\n",
    "**üí° What is a Vector Store?** It's a database that stores document embeddings. Think of it as a specialized database optimized for finding similar vectors (similar meanings).\n",
    "\n",
    "**Key point:** The vector store was created and populated in `00_data_ingestion.ipynb`. It already contains indexed tickets ready for semantic search.\n",
    "\n",
    "**What's next:** Now we can query the vector store to find tickets by meaning, not just keywords!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector store is ready! No need to prepare or index documents here.\n",
    "# All documents were already indexed in 00_data_ingestion.ipynb\n",
    "\n",
    "print(\"‚úÖ Vector store is ready for querying!\")\n",
    "print(\"üí° Documents were indexed in 00_data_ingestion.ipynb\")\n",
    "print(\"   - Content field: short_description (problem summary)\")\n",
    "print(\"   - Metadata: All other ticket fields (for filtering)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "**What happened:** The vector store is ready! ‚úÖ\n",
    "\n",
    "**üí° What's in the Vector Store?**\n",
    "- **Content:** The `short_description` field (what we'll search)\n",
    "- **Metadata:** All other ticket fields (for filtering later)\n",
    "\n",
    "**Why \"Simple RAG\"?** We're using only one field (`short_description`) for search. This is the simplest approach.\n",
    "\n",
    "**üí° In notebook 02:** We'll see how combining multiple fields (`short_description` + `content` + `close_notes`) creates richer documents that improve search quality!\n",
    "\n",
    "**What's next:** Now we can query the vector store to see semantic search in action!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display vector store with documents after indexing\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä Vector Store Status After Indexing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Retrieve updated vector store information\n",
    "vs_updated = client.vector_stores.retrieve(vs_chroma.id)\n",
    "\n",
    "print(f\"\\nüì¶ Vector Store Details:\")\n",
    "print(f\"   ID: {vs_updated.id}\")\n",
    "print(f\"   Status: {vs_updated.status}\")\n",
    "if vs_updated.name:\n",
    "    print(f\"   Name: {vs_updated.name}\")\n",
    "if vs_updated.metadata:\n",
    "    provider = vs_updated.metadata.get(\"provider_id\", \"N/A\")\n",
    "    print(f\"   Provider: {provider}\")\n",
    "\n",
    "# Display file/document statistics\n",
    "if hasattr(vs_updated, \"file_counts\") and vs_updated.file_counts:\n",
    "    print(f\"\\nüìä Document Statistics:\")\n",
    "    print(f\"   Total files: {vs_updated.file_counts.total}\")\n",
    "    print(f\"   Completed: {vs_updated.file_counts.completed}\")\n",
    "    print(f\"   In progress: {vs_updated.file_counts.in_progress}\")\n",
    "    print(f\"   Failed: {vs_updated.file_counts.failed}\")\n",
    "    print(f\"   Cancelled: {vs_updated.file_counts.cancelled}\")\n",
    "\n",
    "# Display storage usage\n",
    "if hasattr(vs_updated, \"usage_bytes\") and vs_updated.usage_bytes:\n",
    "    usage_mb = vs_updated.usage_bytes / (1024 * 1024)\n",
    "    print(f\"\\nüíæ Storage:\")\n",
    "    print(f\"   Usage: {usage_mb:.2f} MB\")\n",
    "\n",
    "# Display timestamps\n",
    "if hasattr(vs_updated, \"created_at\") and vs_updated.created_at:\n",
    "    from datetime import datetime\n",
    "\n",
    "    created_time = datetime.fromtimestamp(vs_updated.created_at)\n",
    "    print(f\"\\nüïí Timestamps:\")\n",
    "    print(f\"   Created: {created_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    if hasattr(vs_updated, \"last_active_at\") and vs_updated.last_active_at:\n",
    "        last_active = datetime.fromtimestamp(vs_updated.last_active_at)\n",
    "        print(f\"   Last active: {last_active.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Query the vector store to show sample documents\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"üîç Sample Documents in Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Querying vector store to retrieve sample documents...\")\n",
    "\n",
    "try:\n",
    "    # Query with a general query to get some sample results\n",
    "    sample_query = \"IT support ticket\"\n",
    "    query_result = client.tool_runtime.rag_tool.query(\n",
    "        content=sample_query,\n",
    "        vector_db_ids=[str(vs_chroma.id)],\n",
    "        extra_body={\"vector_store_ids\": [str(vs_chroma.id)]},\n",
    "    )\n",
    "\n",
    "    print(f\"\\n‚úÖ Vector store is queryable and contains indexed documents!\")\n",
    "    print(f\"\\nüìÑ Sample Query Results:\")\n",
    "    print(f\"   Query: '{sample_query}'\")\n",
    "\n",
    "    # Extract document information if available\n",
    "    if hasattr(query_result, \"content\") and query_result.content:\n",
    "        content_preview = (\n",
    "            query_result.content[:300]\n",
    "            if len(query_result.content) > 300\n",
    "            else query_result.content\n",
    "        )\n",
    "        print(f\"\\n   Retrieved content preview:\")\n",
    "        print(f\"   {content_preview}...\")\n",
    "\n",
    "    print(f\"\\nüí° The vector store is ready for semantic search!\")\n",
    "    print(f\"   You can now query it to find tickets by meaning, not just keywords.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Could not query vector store: {e}\")\n",
    "    print(f\"   Vector store may still be processing documents.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "**What happened:** We indexed all documents into ChromaDB! ‚úÖ\n",
    "\n",
    "**üéâ Success!** The tickets are now searchable using semantic similarity. Each ticket description has been:\n",
    "- ‚úÖ Converted into embeddings (vectors representing meaning)\n",
    "- ‚úÖ Stored in ChromaDB (ready for semantic search)\n",
    "\n",
    "**üí° What happened behind the scenes:**\n",
    "- LlamaStack automatically chunked long descriptions\n",
    "- Generated embeddings using the embedding model\n",
    "- Stored them in the vector database\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Query with RAG - See Semantic Search in Action!\n",
    "\n",
    "**What we're doing:** Testing our RAG system with a query to see how semantic search works.\n",
    "\n",
    "**Why:** This is where you'll see the power of RAG - finding relevant tickets even when they use different words. It matches meaning, not just keywords!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "**Ready to see semantic search in action?** Let's query our indexed tickets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "**What happened:** We indexed the tickets successfully! ‚úÖ\n",
    "\n",
    "**What's next:** Now let's query the system to see semantic search in action. Watch how it finds tickets with similar meaning, even if they use different words!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test query\n",
    "query = \"What was the root cause and resolution for application crashes related to memory issues?\"\n",
    "\n",
    "print(\"üîç Querying RAG System\")\n",
    "print(\"=\" * 60)\n",
    "cprint(f\"\\nüìù User Query: {query}\", \"blue\")\n",
    "print(\"\\nüí° This query will use semantic search to find:\")\n",
    "print(\"   - Tickets about 'application crashes'\")\n",
    "print(\"   - Tickets about 'memory issues'\")\n",
    "print(\"   - Even if they use different words!\")\n",
    "print(\"\\nüîÑ Searching vector store...\")\n",
    "\n",
    "# Step 1: RAG retrieval - find relevant document chunks\n",
    "rag_response = client.tool_runtime.rag_tool.query(\n",
    "    content=query,\n",
    "    vector_db_ids=[str(vs_chroma.id)],\n",
    "    extra_body={\"vector_store_ids\": [str(vs_chroma.id)]},\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Retrieved relevant context from vector store\")\n",
    "print(f\"\\nüìÑ Retrieved Context (first 500 chars):\")\n",
    "print(\"=\" * 60)\n",
    "print(rag_response.content[:500] + \"...\" if len(rag_response.content) > 500 else rag_response.content)\n",
    "print(\"\\nüí° Notice: The system found relevant tickets using semantic similarity!\")\n",
    "\n",
    "# Step 2: Construct extended prompt with retrieved context\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful IT support assistant.\"}]\n",
    "extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{rag_response.content}\\n\\nQUERY:\\n{query}\"\n",
    "messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "# Step 3: Generate answer using LLM\n",
    "print(\"\\nüîÑ Generating answer with LLM...\")\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=model,\n",
    "    stream=stream,\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Answer:\")\n",
    "print(\"=\" * 60)\n",
    "if stream:\n",
    "    for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "    print()\n",
    "else:\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "**What happened:** We used RAG to find relevant tickets and generate an answer! The system matched meaning, not just keywords.\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaway\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) enables semantic search - finding documents by meaning, not just exact keyword matches. The data was indexed in `00_data_ingestion.ipynb`, and now we can search for similar incidents even when they use different words. This is the foundation for intelligent IT operations search systems.\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Connection\n",
    "\n",
    "**How this applies to IT Operations:**\n",
    "\n",
    "The same RAG approach can be used for:\n",
    "\n",
    "- **Incident Resolution:** \"Find similar incidents to this one\" ‚Üí Get past solutions\n",
    "- **Knowledge Base Search:** Search through documentation and runbooks using natural language\n",
    "- **Pattern Recognition:** Identify recurring problems across incidents\n",
    "- **Root Cause Analysis:** Find incidents with similar symptoms to learn from past diagnostics\n",
    "\n",
    "**The pattern is the same:** Index historical data (in `00_data_ingestion.ipynb`) ‚Üí Query semantically ‚Üí Retrieve relevant context ‚Üí Use context to answer questions or suggest solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® Your Turn\n",
    "\n",
    "**Try this:** Modify the query to search for different types of problems. For example:\n",
    "- \"How do I fix database connection errors?\"\n",
    "- \"What causes slow application performance?\"\n",
    "- \"Find tickets about network issues\"\n",
    "\n",
    "Notice how the semantic search finds relevant tickets even with different wording!\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ You Did It!\n",
    "\n",
    "You've queried your first RAG system! You learned how to use semantic search to find IT tickets by meaning, not just keywords. \n",
    "\n",
    "**What you learned:**\n",
    "- ‚úÖ How to connect to a pre-indexed vector store\n",
    "- ‚úÖ How to query using semantic similarity\n",
    "- ‚úÖ How RAG retrieves relevant context and generates answers\n",
    "\n",
    "**Next Steps:**\n",
    "1. **For multi-field RAG**: Run `00_data_ingestion.ipynb` with `INGESTION_MODE = \"multi-field\"`, then use `02_advanced_rag_with_multiple_fields.ipynb`\n",
    "2. **To re-index data**: Use `00_data_ingestion.ipynb` anytime you need to update the vector store\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Want to Go Deeper?\n",
    "\n",
    "<details>\n",
    "<summary>üìñ Additional Resources (Click to expand)</summary>\n",
    "\n",
    "- [LlamaStack Documentation](https://github.com/llamastack/llamastack) - RAG and vector store capabilities\n",
    "- [ChromaDB Documentation](https://www.trychroma.com/) - Vector database used by LlamaStack\n",
    "- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/) - RAG techniques and patterns\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
