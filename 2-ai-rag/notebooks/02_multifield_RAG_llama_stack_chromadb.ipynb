{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 02: Build Multi-Field RAG for IT Ticket Search\n",
    "\n",
    "## ðŸŽ¯ Your Mission\n",
    "\n",
    "You're an IT support engineer building a smarter ticket search system. Your job today: combine multiple ticket fields (`short_description`, `content`, and `close_notes`) to create richer document representations that help find both problems AND solutions.\n",
    "\n",
    "**Why this matters:** This same multi-field RAG approach is how you could build intelligent search systems that understand the full context of incidents - from initial problem reports to diagnostic findings to final resolutions - enabling better pattern recognition and faster problem resolution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## âš¡ Quick Win (First 2 Minutes)\n",
    "\n",
    "Let's see multi-field RAG in action! Run the cell below to see how combining multiple fields improves search results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "tags": []
   },
   "source": [
    "**What you'll see:** By combining multiple fields (problem description + diagnostic details + resolution steps), the RAG system can find tickets that match both the problem AND the solution, not just the problem description alone.\n",
    "\n",
    "Now let's build it step by step to understand how it works.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- âœ… Combine multiple ticket fields to create richer RAG documents\n",
    "- âœ… Understand why multi-field RAG outperforms single-field RAG\n",
    "- âœ… Build a search system that finds both problems and solutions\n",
    "\n",
    "**Time:** ~15-20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ The Journey\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Step 1: Load and Explore the Dataset\n",
    "\n",
    "**What we're doing:** Loading IT call center tickets and examining their structure.\n",
    "\n",
    "**Why:** We need to understand what fields are available so we can combine them effectively for better search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "from llama_stack_client import RAGDocument\n",
    "\n",
    "# Load the CSV file from the data directory\n",
    "data_dir = Path(\"../data\")\n",
    "file_path = data_dir / \"synthetic-it-call-center-tickets.csv\"\n",
    "\n",
    "print(\"ðŸ”„ Loading IT call center tickets dataset...\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"âœ… Loaded {len(df)} tickets\")\n",
    "print(f\"ðŸ“‹ Dataset shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"\\nðŸ” Let's examine the dataset structure:\")\n",
    "print(\"=\" * 60)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "**What we see:** Each ticket has multiple fields:\n",
    "- **`short_description`** - Brief problem summary\n",
    "- **`content`** - Detailed problem description\n",
    "- **`close_notes`** - Diagnostic findings and resolution steps\n",
    "- **Other fields** - Metadata like ticket number, priority, etc.\n",
    "\n",
    "**ðŸ’¡ Key insight:** By combining `short_description`, `content`, and `close_notes`, we create documents that contain the full ticket lifecycle - problem â†’ diagnosis â†’ solution. This enables better search!\n",
    "\n",
    "Let's see the field structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataset structure and key fields\n",
    "print(\"ðŸ“Š Dataset Structure:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nðŸ“ Key Fields for Multi-Field RAG:\")\n",
    "print(f\"   - short_description: Brief problem summary\")\n",
    "print(f\"   - content: Detailed problem description\")  \n",
    "print(f\"   - close_notes: Diagnostic findings and resolution steps\")\n",
    "print(f\"\\nðŸ’¡ Other fields will be stored as metadata for filtering\")\n",
    "\n",
    "# Show an example ticket to illustrate the multi-field concept\n",
    "print(\"\\nðŸ“‹ Example Ticket (showing multi-field structure):\")\n",
    "print(\"=\" * 60)\n",
    "if len(df) > 0:\n",
    "    example = df.iloc[0]\n",
    "    print(f\"\\nðŸŽ« Ticket #{example.get('number', 'N/A')}\")\n",
    "    print(f\"\\nðŸ“Œ Short Description:\")\n",
    "    print(f\"   {example.get('short_description', 'N/A')[:100]}...\")\n",
    "    print(f\"\\nðŸ“„ Content:\")\n",
    "    print(f\"   {str(example.get('content', 'N/A'))[:150]}...\")\n",
    "    print(f\"\\nâœ… Close Notes:\")\n",
    "    print(f\"   {str(example.get('close_notes', 'N/A'))[:150]}...\")\n",
    "    print(f\"\\nðŸ’¡ Notice: Combining all three fields gives us the complete ticket story!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: Set Up LlamaStack Client\n",
    "\n",
    "**What we're doing:** Connecting to LlamaStack and configuring our environment.\n",
    "\n",
    "**Why:** We need LlamaStack to handle vector database operations, embeddings, and RAG queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**What happened:** We explored the dataset and understand its structure. Now let's connect to LlamaStack.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for LlamaStack\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "# Add root src directory to path to import shared config\n",
    "root_dir = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(root_dir / \"src\"))\n",
    "\n",
    "# Import centralized configuration\n",
    "from config import LLAMA_STACK_URL, MODEL, CONFIG\n",
    "\n",
    "# Configuration values (automatically detected based on environment)\n",
    "llamastack_url = LLAMA_STACK_URL\n",
    "model = MODEL\n",
    "\n",
    "if not llamastack_url:\n",
    "    raise ValueError(\n",
    "        \"LLAMA_STACK_URL is not configured!\\n\"\n",
    "        \"Please run: ./scripts/setup-env.sh\\n\"\n",
    "        \"Or set LLAMA_STACK_URL environment variable:\\n\"\n",
    "        \"  export LLAMA_STACK_URL='https://llamastack-route-my-first-model.apps.ocp.example.com'\"\n",
    "    )\n",
    "\n",
    "print(\"ðŸ”„ Step 1: Connecting to LlamaStack...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ðŸ“¡ LlamaStack URL: {llamastack_url}\")\n",
    "print(f\"ðŸ¤– Model: {model}\")\n",
    "print(f\"ðŸ“ Environment: {'Inside OpenShift cluster' if CONFIG['inside_cluster'] else 'Outside OpenShift cluster'}\")\n",
    "print(f\"ðŸ“¦ Namespace: {CONFIG['namespace']}\")\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "client = LlamaStackClient(base_url=llamastack_url)\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    model_count = len(models.data) if hasattr(models, 'data') else len(models)\n",
    "    print(f\"\\nâœ… Connected to LlamaStack\")\n",
    "    print(f\"   Available models: {model_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Cannot connect to LlamaStack: {e}\")\n",
    "    print(\"\\nðŸ’¡ Troubleshooting:\")\n",
    "    print(\"   1. Check if route exists: oc get route llamastack-route -n my-first-model\")\n",
    "    print(\"   2. Run setup script: ./scripts/setup-env.sh\")\n",
    "    print(\"   3. Or set LLAMA_STACK_URL manually in .env file\")\n",
    "    raise\n",
    "\n",
    "# Configure inference parameters\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"\\nâš™ï¸  Inference Parameters:\")\n",
    "print(f\"   Model: {model}\")\n",
    "print(f\"   Temperature: {temperature}\")\n",
    "print(f\"   Max Tokens: {max_tokens}\")\n",
    "print(f\"   Stream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "**What happened:** We connected to LlamaStack and configured our inference parameters. Now we're ready to create the vector store and index documents.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Create Vector Store and Index Documents\n",
    "\n",
    "**What we're doing:** Creating a ChromaDB vector store and indexing our multi-field documents.\n",
    "\n",
    "**Why:** The vector store will enable semantic search over our ticket data. By combining multiple fields, we create richer document representations that improve retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit - specify embedding model and/or provider when you need specific ones\n",
    "vs_chroma = client.vector_stores.create(\n",
    "    extra_body={\n",
    "        \"provider_id\": \"chromadb\",  # Optional: specify vector store provider\n",
    "        \"embedding_model\": \"sentence-transformers/nomic-ai/nomic-embed-text-v1.5\",\n",
    "        \"embedding_dimension\": 768  # Optional: will be auto-detected if not provided\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "**What happened:** We created a ChromaDB vector store. ChromaDB is embedded in LlamaStack (no separate deployment needed), unlike MongoDB which requires a separate MCP server.\n",
    "\n",
    "Now let's prepare and combine the ticket fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the data\n",
    "print(\"\\nðŸ”„ Step 2: Preparing data for indexing...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# Limit to first 1000 records for faster processing (you can use more for production)\n",
    "df_1000 = df.head(1000)\n",
    "print(f\"   Processing {len(df_1000)} tickets (out of {len(df)} total)\")\n",
    "\n",
    "# Step 3: Create multi-field RAG documents\n",
    "print(\"\\nðŸ”„ Step 3: Creating multi-field RAG documents...\")\n",
    "print(\"   Combining fields: short_description + content + close_notes\")\n",
    "print(\"   Storing other fields as metadata\")\n",
    "\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"ticket-{i}\",\n",
    "        content=f\"{df_1000.iloc[i]['short_description']}\\n\\n{df_1000.iloc[i]['content']}\\n\\n{df_1000.iloc[i]['close_notes']}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata=df_1000.iloc[i].drop([\"short_description\", \"content\", \"close_notes\"]).to_dict(),\n",
    "    )\n",
    "    for i in range(len(df_1000))\n",
    "]\n",
    "\n",
    "print(f\"âœ… Created {len(documents)} RAG documents\")\n",
    "print(f\"\\nðŸ’¡ Each document contains:\")\n",
    "print(f\"   - Content: short_description + content + close_notes (full ticket story)\")\n",
    "print(f\"   - Metadata: All other fields (for filtering)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "**What happened:** We created RAG documents that combine multiple fields. Each document now contains the complete ticket story - from problem description to diagnostic findings to resolution steps.\n",
    "\n",
    "**ðŸ’¡ Key insight:** This multi-field approach enables the RAG system to match queries based on:\n",
    "- Problem descriptions (from `short_description` and `content`)\n",
    "- Diagnostic details (from `content` and `close_notes`)\n",
    "- Solution steps (from `close_notes`)\n",
    "\n",
    "This is much more powerful than single-field RAG!\n",
    "\n",
    "Now let's index these documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Index documents into the vector store (in batches to avoid timeout)\n",
    "print(\"\\nðŸ”„ Step 4: Indexing documents into vector store...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Chunk size: 1024 tokens\")\n",
    "print(f\"   Total documents: {len(documents)}\")\n",
    "print(f\"   Processing in batches of 100 to avoid timeout...\")\n",
    "\n",
    "# Process in batches to avoid gateway timeout\n",
    "BATCH_SIZE = 10\n",
    "total_batches = (len(documents) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "inserted_count = 0\n",
    "\n",
    "for batch_num in range(total_batches):\n",
    "    start_idx = batch_num * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(documents))\n",
    "    batch = documents[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"\\n   Batch {batch_num + 1}/{total_batches}: Processing documents {start_idx} to {end_idx-1}...\")\n",
    "    \n",
    "    try:\n",
    "        insert_result = client.tool_runtime.rag_tool.insert( \n",
    "            chunk_size_in_tokens=1024,\n",
    "            documents=batch,\n",
    "            vector_db_id=str(vs_chroma.id),\n",
    "            extra_body={\"vector_store_id\": str(vs_chroma.id)},\n",
    "            extra_headers=None,\n",
    "            extra_query=None,\n",
    "            timeout=300  # 5 minute timeout per batch\n",
    "        )\n",
    "        inserted_count += len(batch)\n",
    "        print(f\"   âœ… Batch {batch_num + 1} indexed successfully ({inserted_count}/{len(documents)} documents)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Error indexing batch {batch_num + 1}: {e}\")\n",
    "        print(f\"   ðŸ’¡ Tip: You can continue with the documents already indexed, or reduce BATCH_SIZE\")\n",
    "        # Continue with next batch instead of failing completely\n",
    "        continue\n",
    "\n",
    "print(f\"\\nâœ… Indexing complete!\")\n",
    "print(f\"   Successfully indexed: {inserted_count}/{len(documents)} documents\")\n",
    "print(f\"   Vector store ID: {vs_chroma.id}\")\n",
    "print(f\"\\nðŸ’¡ LlamaStack automatically:\")\n",
    "print(f\"   - Chunked the documents\")\n",
    "print(f\"   - Generated embeddings for each chunk\")\n",
    "print(f\"   - Stored them in ChromaDB for semantic search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "**What happened:** We indexed all documents into ChromaDB! The documents are now searchable using semantic similarity.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Query with Multi-Field RAG\n",
    "\n",
    "**What we're doing:** Testing our multi-field RAG system with queries that benefit from combined fields.\n",
    "\n",
    "**Why:** Multi-field RAG excels at queries that need both problem AND solution context, not just problem descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.vector_io.query(vector_db_id=vs_chroma.id,query=\"ZTrend crashes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 3. Executing Queries via the Built-in RAG Tool\n",
    "- Directly invoke the RAG tool to query the vector database we ingested into at the previous stage.\n",
    "- Construct an extended prompt using the retrieved chunks.\n",
    "- Query the model with the extended prompt.\n",
    "- Output the reply received from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What was the root cause and resolution for application crashes related to memory issues?\",\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # RAG retrieval call\n",
    "    rag_response = client.tool_runtime.rag_tool.query(\n",
    "        content=prompt,\n",
    "        vector_db_ids=[str(vs_chroma.id)],   # o SDK exige isso\n",
    "        extra_body={\"vector_store_ids\": [str(vs_chroma.id)]},  # o backend exige isso\n",
    "    )\n",
    "\n",
    "    print(rag_response.content)\n",
    "    # the list of messages to be sent to the model must start with the system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "    # construct the actual prompt to be executed, incorporating the original query and the retrieved content\n",
    "    prompt_context = rag_response.content\n",
    "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{prompt_context}\\n\\nQUERY:\\n{prompt}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "    # use Llama Stack inference API to directly communicate with the desired model\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        stream=stream,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "if stream:\n",
    "    for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "    print()  # nova linha apÃ³s streaming\n",
    "else:\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 4. Why Multi-Field RAG is Better: Example Queries\n",
    "\n",
    "Using multiple fields (`short_description`, `content`, and `close_notes`) instead of just `short_description` significantly improves retrieval quality for certain types of queries. Here are examples where multi-field RAG outperforms single-field RAG:\n",
    "\n",
    "### Example 1: Troubleshooting Steps and Solutions\n",
    "**Query**: \"How do I fix ZTrend crashes when saving files?\"\n",
    "\n",
    "- **Single-field (short_description only)**: May retrieve tickets about crashes, but won't have the solution steps\n",
    "- **Multi-field**: Retrieves tickets with both the problem description AND the detailed troubleshooting steps from `close_notes`, providing complete answers\n",
    "\n",
    "### Example 2: Historical Context and Resolution\n",
    "**Query**: \"What was the root cause and resolution for application crashes related to memory issues?\"\n",
    "\n",
    "- **Single-field**: Only finds tickets mentioning \"crashes\" but misses the diagnostic details and resolution steps\n",
    "- **Multi-field**: Retrieves tickets with full context from `content` (initial problem description) and `close_notes` (diagnostic findings and resolution), enabling comprehensive answers\n",
    "\n",
    "### Example 3: Pattern Recognition Across Problem-Solution Pairs\n",
    "**Query**: \"What are common solutions for software crashes that involve configuration files?\"\n",
    "\n",
    "- **Single-field**: Can identify crash-related tickets but can't see the solutions\n",
    "- **Multi-field**: Can match both problem patterns (from `short_description`/`content`) and solution patterns (from `close_notes`), enabling identification of recurring problem-solution patterns\n",
    "\n",
    "### Example 4: Detailed Technical Information\n",
    "**Query**: \"Show me tickets where log file analysis revealed the issue\"\n",
    "\n",
    "- **Single-field**: May miss tickets where log analysis is only mentioned in `content` or `close_notes`\n",
    "- **Multi-field**: Captures technical details from all fields, ensuring comprehensive retrieval of relevant tickets\n",
    "\n",
    "### Example 5: End-to-End Ticket Understanding\n",
    "**Query**: \"Find tickets where the customer reported a problem, diagnostics were performed, and the issue was resolved by reinstalling software\"\n",
    "\n",
    "- **Single-field**: Can't capture the full narrative flow from problem â†’ diagnosis â†’ solution\n",
    "- **Multi-field**: Preserves the complete ticket lifecycle, enabling retrieval based on complex multi-stage scenarios\n",
    "\n",
    "**Key Insight**: Multi-field RAG is especially powerful for queries that require understanding both the problem AND the solution, or queries that need to match patterns across different stages of the ticket lifecycle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "This notebook demonstrated how to set up and use the built-in RAG tool for ingesting user-provided documents in a vector database and utilizing them during inference via direct retrieval. \n",
    "\n",
    "Key points:\n",
    "- **Multi-field content**: We combined `short_description`, `content`, and `close_notes` fields to create richer document representations, improving the quality of retrieval and context understanding.\n",
    "- **Metadata preservation**: Other fields from the dataset are stored as metadata, allowing for filtering and additional context during retrieval.\n",
    "- **Vector database integration**: The documents are chunked and indexed into ChromaDB using Llama Stack's RAG tool, enabling semantic search over the ticket data.\n",
    "- **Query advantages**: As shown in Section 4, multi-field RAG excels at queries requiring both problem and solution context, pattern recognition across ticket lifecycle stages, and comprehensive technical information retrieval.\n",
    "\n",
    "Now that we've seen how easy it is to implement RAG with Llama Stack, We'll move on to building a simple agent with Llama Stack next in our [Simple Agents](./Level2_simple_agent_with_websearch.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
