{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 00: Data Ingestion for RAG Systems\n",
    "\n",
    "## üéØ Your Mission\n",
    "\n",
    "You're preparing data for RAG (Retrieval-Augmented Generation) systems. Your job: ingest IT tickets into vector databases so they can be searched semantically.\n",
    "\n",
    "**Why this matters:** Before you can search documents semantically, you need to index them into a vector database. This notebook handles the ingestion process for both single-field and multi-field RAG approaches.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Quick Overview\n",
    "\n",
    "**What this notebook does:**\n",
    "- ‚úÖ Loads IT ticket data from CSV files\n",
    "- ‚úÖ Creates vector stores in LlamaStack\n",
    "- ‚úÖ Prepares documents for indexing (single-field or multi-field)\n",
    "- ‚úÖ Indexes documents into vector stores in batches\n",
    "- ‚úÖ Verifies ingestion success\n",
    "\n",
    "**This notebook will:**\n",
    "- Create **both** vector stores (single-field and multi-field) in one run\n",
    "- Index documents into both stores\n",
    "- Prepare everything needed for notebooks 01 and 02\n",
    "\n",
    "**Time:** ~10-15 minutes (depending on data size)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand how to prepare data for RAG ingestion\n",
    "- ‚úÖ Create both vector stores in LlamaStack (single-field and multi-field)\n",
    "- ‚úÖ Index documents using both approaches\n",
    "- ‚úÖ Verify that both ingestions were successful\n",
    "\n",
    "---\n",
    "\n",
    "## üìã The Journey\n",
    "\n",
    "1. **Load Data** - Load IT tickets from CSV\n",
    "2. **Set Up LlamaStack** - Connect to LlamaStack\n",
    "3. **Choose Ingestion Mode** - Single-field or multi-field\n",
    "4. **Create Vector Store** - Set up the vector database\n",
    "5. **Prepare Documents** - Format data for ingestion\n",
    "6. **Index Documents** - Ingest into vector store\n",
    "7. **Verify Ingestion** - Confirm documents are searchable\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Explore the Dataset\n",
    "\n",
    "**What we're doing:** Loading IT call center tickets from CSV files.\n",
    "\n",
    "**Why:** We need to understand the data structure before we can ingest it into the vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from llama_stack_client import RAGDocument\n",
    "\n",
    "# Load the CSV file from the data directory\n",
    "data_dir = Path(\"../data\")\n",
    "file_path = data_dir / \"synthetic-it-call-center-tickets-sample.csv\"\n",
    "\n",
    "print(\"üîÑ Loading IT call center tickets dataset...\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df)} tickets\")\n",
    "print(f\"üìã Dataset shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"\\nüîç Let's examine the dataset structure:\")\n",
    "print(\"=\" * 60)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we see:** Each ticket has multiple fields:\n",
    "- **`short_description`** - Brief problem summary\n",
    "- **`content`** - Detailed problem description  \n",
    "- **`close_notes`** - Diagnostic findings and resolution steps\n",
    "- **Other fields** - Metadata like ticket number, priority, etc.\n",
    "\n",
    "**üí° Key insight:** We can create different types of RAG documents:\n",
    "- **Single-field**: Use only `short_description` (simpler, faster)\n",
    "- **Multi-field**: Combine `short_description + content + close_notes` (richer context)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Set Up LlamaStack Client\n",
    "\n",
    "**What we're doing:** Connecting to LlamaStack and configuring our environment.\n",
    "\n",
    "**Why:** LlamaStack provides the vector database and ingestion APIs we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for LlamaStack\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "# Add root src directory to path to import shared config\n",
    "root_dir = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(root_dir / \"src\"))\n",
    "\n",
    "# Import centralized configuration\n",
    "from config import LLAMA_STACK_URL, MODEL, NAMESPACE\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "print(\"üîÑ Step 2: Connecting to LlamaStack...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üì° LlamaStack URL: {LLAMA_STACK_URL}\")\n",
    "print(f\"ü§ñ Model: {MODEL}\")\n",
    "\n",
    "client = LlamaStackClient(base_url=LLAMA_STACK_URL)\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    print(f\"\\n‚úÖ Connected to LlamaStack\")\n",
    "    print(f\"   Available models: {len(models.data) if hasattr(models, 'data') else 'N/A'}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Failed to connect to LlamaStack: {e}\")\n",
    "    print(f\"\\nüí° Make sure:\")\n",
    "    print(f\"   1. LlamaStack is deployed and running\")\n",
    "    print(f\"   2. LLAMA_STACK_URL is set correctly\")\n",
    "    print(f\"   3. You have network access to LlamaStack\")\n",
    "    raise\n",
    "\n",
    "# Configure inference parameters\n",
    "model = MODEL\n",
    "stream = True\n",
    "max_tokens = 4096\n",
    "temperature = 0.0\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Inference Parameters:\")\n",
    "print(f\"   Model: {model}\")\n",
    "print(f\"   Temperature: {temperature}\")\n",
    "print(f\"   Max Tokens: {max_tokens}\")\n",
    "print(f\"   Stream: {stream}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** We connected to LlamaStack successfully! ‚úÖ\n",
    "\n",
    "**What's next:** Now we'll create both vector stores (single-field and multi-field) and index documents into both.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Create Both Vector Stores\n",
    "\n",
    "**What we're doing:** Creating two vector stores - one for single-field RAG and one for multi-field RAG.\n",
    "\n",
    "**Why:** We'll create both so you can use:\n",
    "- **Single-field vector store** for notebook 01 (introduction to RAG)\n",
    "- **Multi-field vector store** for notebook 02 (advanced RAG)\n",
    "\n",
    "**üí° Benefits of creating both:**\n",
    "- **Single-field RAG**: Faster, simpler, good for basic search (uses only `short_description`)\n",
    "- **Multi-field RAG**: Richer context, better for complex queries (uses `short_description + content + close_notes`)\n",
    "- **Comparison**: You can compare both approaches side-by-side\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create both vector stores for this workshop\n",
    "print(\"=\" * 60)\n",
    "print(\"üìã Vector Store Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüîß Creating Both Vector Stores:\")\n",
    "\n",
    "print(\"\\nüìù Single-Field RAG Vector Store:\")\n",
    "print(\"   - Content field: short_description only\")\n",
    "print(\"   - Use case: Basic semantic search\")\n",
    "print(\"   - Best for: Notebook 01 - Introduction to RAG\")\n",
    "print(\"   - Vector store name: 'single-field-rag-tickets'\")\n",
    "\n",
    "print(\"\\nüìù Multi-Field RAG Vector Store:\")\n",
    "print(\"   - Content fields: short_description + content + close_notes\")\n",
    "print(\"   - Use case: Advanced semantic search with full context\")\n",
    "print(\"   - Best for: Notebook 02 - Advanced RAG\")\n",
    "print(\"   - Vector store name: 'multi-field-rag-tickets'\")\n",
    "\n",
    "print(\"\\nüí° Both vector stores will be created and populated in this notebook!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4: Create Both Vector Stores\n",
    "\n",
    "**What we're doing:** Creating two ChromaDB vector stores in LlamaStack - one for single-field and one for multi-field RAG.\n",
    "\n",
    "**Why:** We need separate vector stores because they'll contain different document structures (single-field vs multi-field), enabling comparison and use in different notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create both ChromaDB vector stores\n",
    "print(\"\\nüîÑ Step 4: Creating ChromaDB vector stores...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"   - Provider: ChromaDB (embedded in LlamaStack)\")\n",
    "print(\"   - Embedding model: sentence-transformers/nomic-ai/nomic-embed-text-v1.5\")\n",
    "print(\"   - Embedding dimension: 768\")\n",
    "\n",
    "# Create single-field vector store\n",
    "print(\"\\nüì¶ Creating single-field vector store...\")\n",
    "vs_single_field = client.vector_stores.create(\n",
    "    name=\"single-field-rag-tickets\",\n",
    "    extra_body={\n",
    "        \"provider_id\": \"chromadb\",\n",
    "        \"embedding_model\": \"sentence-transformers/nomic-ai/nomic-embed-text-v1.5\",\n",
    "        \"embedding_dimension\": 768\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Single-field vector store created!\")\n",
    "print(f\"   ID: {vs_single_field.id}\")\n",
    "print(f\"   Name: {vs_single_field.name if hasattr(vs_single_field, 'name') else 'N/A'}\")\n",
    "\n",
    "# Create multi-field vector store\n",
    "print(\"\\nüì¶ Creating multi-field vector store...\")\n",
    "vs_multi_field = client.vector_stores.create(\n",
    "    name=\"multi-field-rag-tickets\",\n",
    "    extra_body={\n",
    "        \"provider_id\": \"chromadb\",\n",
    "        \"embedding_model\": \"sentence-transformers/nomic-ai/nomic-embed-text-v1.5\",\n",
    "        \"embedding_dimension\": 768\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Multi-field vector store created!\")\n",
    "print(f\"   ID: {vs_multi_field.id}\")\n",
    "print(f\"   Name: {vs_multi_field.name if hasattr(vs_multi_field, 'name') else 'N/A'}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store vector store IDs for later use\n",
    "VECTOR_STORE_ID_SINGLE_FIELD = vs_single_field.id\n",
    "VECTOR_STORE_ID_MULTI_FIELD = vs_multi_field.id\n",
    "\n",
    "print(f\"\\nüí° Vector Store IDs saved:\")\n",
    "print(f\"   Single-field: {VECTOR_STORE_ID_SINGLE_FIELD} (for notebook 01)\")\n",
    "print(f\"   Multi-field: {VECTOR_STORE_ID_MULTI_FIELD} (for notebook 02)\")\n",
    "print(f\"\\n   You'll need these IDs in the query notebooks!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** We created both ChromaDB vector stores! ‚úÖ\n",
    "\n",
    "**üí° What is ChromaDB?** It's a vector database that stores embeddings. Think of it as a specialized database optimized for finding similar vectors (similar meanings).\n",
    "\n",
    "**Key point:** ChromaDB is embedded in LlamaStack - no separate deployment needed! This makes setup simple.\n",
    "\n",
    "**What's next:** Now we'll prepare our ticket data and convert it into both document formats (single-field and multi-field) for indexing.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Prepare Documents for Both Ingestion Modes\n",
    "\n",
    "**What we're doing:** Converting ticket data into RAG documents for both single-field and multi-field approaches.\n",
    "\n",
    "**Why:** We need to create two different document structures:\n",
    "- **Single-field documents**: Use only `short_description` (for notebook 01)\n",
    "- **Multi-field documents**: Combine `short_description + content + close_notes` (for notebook 02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "print(\"\\nüîÑ Step 5: Preparing data for indexing...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# Use all tickets (sample file already has 1000 rows)\n",
    "df_1000 = df  # Sample file already has 1000 rows\n",
    "print(f\"   Processing {len(df_1000)} tickets (out of {len(df)} total)\")\n",
    "\n",
    "# Create single-field RAG documents\n",
    "print(f\"\\nüîÑ Creating single-field RAG documents...\")\n",
    "print(\"   Using field: short_description (problem summary)\")\n",
    "print(\"   Storing other fields as metadata\")\n",
    "\n",
    "documents_single_field = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"ticket-{i}\",\n",
    "        content=df_1000.iloc[i][\"short_description\"],\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata=df_1000.iloc[i].drop(\"short_description\").to_dict(),\n",
    "    )\n",
    "    for i in range(len(df_1000))\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(documents_single_field)} single-field RAG documents\")\n",
    "print(f\"   - Content: short_description (what we'll search)\")\n",
    "print(f\"   - Metadata: All other fields (for filtering)\")\n",
    "\n",
    "# Create multi-field RAG documents\n",
    "print(f\"\\nüîÑ Creating multi-field RAG documents...\")\n",
    "print(\"   Combining fields: short_description + content + close_notes\")\n",
    "print(\"   Storing other fields as metadata\")\n",
    "\n",
    "documents_multi_field = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"ticket-{i}\",\n",
    "        content=f\"{df_1000.iloc[i]['short_description']}\\n\\n{df_1000.iloc[i]['content']}\\n\\n{df_1000.iloc[i]['close_notes']}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata=df_1000.iloc[i].drop([\"short_description\", \"content\", \"close_notes\"]).to_dict(),\n",
    "    )\n",
    "    for i in range(len(df_1000))\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(documents_multi_field)} multi-field RAG documents\")\n",
    "print(f\"   - Content: short_description + content + close_notes (full ticket story)\")\n",
    "print(f\"   - Metadata: All other fields (for filtering)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total: {len(documents_single_field)} single-field + {len(documents_multi_field)} multi-field documents ready for indexing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** We created RAG documents for both approaches! ‚úÖ\n",
    "\n",
    "**üí° What is a RAG Document?**\n",
    "- **Content:** The field(s) that will be searched semantically\n",
    "- **Metadata:** Additional fields stored for filtering and context\n",
    "\n",
    "**What's next:** Now we'll index both document sets into their respective vector stores. LlamaStack will automatically chunk them, generate embeddings, and store them for semantic search.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Index Documents into Both Vector Stores\n",
    "\n",
    "**What we're doing:** Ingesting documents into both vector stores in batches.\n",
    "\n",
    "**Why:** Batch processing prevents timeouts and allows progress tracking. LlamaStack will automatically:\n",
    "- Chunk long documents\n",
    "- Generate embeddings for each chunk\n",
    "- Store them in ChromaDB for semantic search\n",
    "\n",
    "**We'll index:**\n",
    "1. Single-field documents ‚Üí single-field vector store\n",
    "2. Multi-field documents ‚Üí multi-field vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index documents into both vector stores (in batches to avoid timeout)\n",
    "print(\"\\nüîÑ Step 6: Indexing documents into both vector stores...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Chunk size: 1024 tokens\")\n",
    "print(f\"   Single-field batch size: 100 documents\")\n",
    "print(f\"   Multi-field batch size: 10 documents (smaller due to larger document size)\")\n",
    "print(f\"   Processing in batches to avoid timeout...\")\n",
    "\n",
    "BATCH_SIZE_SINGLE_FIELD = 100  # Single-field documents are smaller, can use larger batches\n",
    "BATCH_SIZE_MULTI_FIELD = 10    # Multi-field documents are larger, use smaller batches to avoid timeout\n",
    "\n",
    "# ============================================================\n",
    "# Index single-field documents into single-field vector store\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üì¶ Indexing Single-Field Documents\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   Vector store: single-field-rag-tickets\")\n",
    "print(f\"   Total documents: {len(documents_single_field)}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE_SINGLE_FIELD} documents\")\n",
    "\n",
    "total_batches_single = (len(documents_single_field) + BATCH_SIZE_SINGLE_FIELD - 1) // BATCH_SIZE_SINGLE_FIELD\n",
    "inserted_count_single = 0\n",
    "\n",
    "for batch_num in range(total_batches_single):\n",
    "    start_idx = batch_num * BATCH_SIZE_SINGLE_FIELD\n",
    "    end_idx = min(start_idx + BATCH_SIZE_SINGLE_FIELD, len(documents_single_field))\n",
    "    batch = documents_single_field[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"\\n   Batch {batch_num + 1}/{total_batches_single}: Processing documents {start_idx} to {end_idx-1}...\")\n",
    "    \n",
    "    try:\n",
    "        insert_result = client.tool_runtime.rag_tool.insert( \n",
    "            chunk_size_in_tokens=1024,\n",
    "            documents=batch,\n",
    "            vector_db_id=str(VECTOR_STORE_ID_SINGLE_FIELD),\n",
    "            extra_body={\"vector_store_id\": str(VECTOR_STORE_ID_SINGLE_FIELD)},\n",
    "            extra_headers=None,\n",
    "            extra_query=None,\n",
    "            timeout=300  # 5 minute timeout per batch\n",
    "        )\n",
    "        inserted_count_single += len(batch)\n",
    "        print(f\"   ‚úÖ Batch {batch_num + 1} indexed successfully ({inserted_count_single}/{len(documents_single_field)} documents)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error indexing batch {batch_num + 1}: {e}\")\n",
    "        print(f\"   üí° Tip: You can continue with the documents already indexed, or reduce BATCH_SIZE_SINGLE_FIELD\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Single-field indexing complete!\")\n",
    "print(f\"   Successfully indexed: {inserted_count_single}/{len(documents_single_field)} documents\")\n",
    "\n",
    "# ============================================================\n",
    "# Index multi-field documents into multi-field vector store\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üì¶ Indexing Multi-Field Documents\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   Vector store: multi-field-rag-tickets\")\n",
    "print(f\"   Total documents: {len(documents_multi_field)}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE_MULTI_FIELD} documents (smaller batches due to larger document size)\")\n",
    "\n",
    "total_batches_multi = (len(documents_multi_field) + BATCH_SIZE_MULTI_FIELD - 1) // BATCH_SIZE_MULTI_FIELD\n",
    "inserted_count_multi = 0\n",
    "\n",
    "for batch_num in range(total_batches_multi):\n",
    "    start_idx = batch_num * BATCH_SIZE_MULTI_FIELD\n",
    "    end_idx = min(start_idx + BATCH_SIZE_MULTI_FIELD, len(documents_multi_field))\n",
    "    batch = documents_multi_field[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"\\n   Batch {batch_num + 1}/{total_batches_multi}: Processing documents {start_idx} to {end_idx-1}...\")\n",
    "    \n",
    "    try:\n",
    "        insert_result = client.tool_runtime.rag_tool.insert( \n",
    "            chunk_size_in_tokens=1024,\n",
    "            documents=batch,\n",
    "            vector_db_id=str(VECTOR_STORE_ID_MULTI_FIELD),\n",
    "            extra_body={\"vector_store_id\": str(VECTOR_STORE_ID_MULTI_FIELD)},\n",
    "            extra_headers=None,\n",
    "            extra_query=None,\n",
    "            timeout=300  # 5 minute timeout per batch\n",
    "        )\n",
    "        inserted_count_multi += len(batch)\n",
    "        print(f\"   ‚úÖ Batch {batch_num + 1} indexed successfully ({inserted_count_multi}/{len(documents_multi_field)} documents)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error indexing batch {batch_num + 1}: {e}\")\n",
    "        print(f\"   üí° Tip: You can continue with the documents already indexed, or reduce BATCH_SIZE_MULTI_FIELD\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Multi-field indexing complete!\")\n",
    "print(f\"   Successfully indexed: {inserted_count_multi}/{len(documents_multi_field)} documents\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä Indexing Summary\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"‚úÖ Single-field vector store: {inserted_count_single}/{len(documents_single_field)} documents\")\n",
    "print(f\"   Vector store ID: {VECTOR_STORE_ID_SINGLE_FIELD}\")\n",
    "print(f\"\\n‚úÖ Multi-field vector store: {inserted_count_multi}/{len(documents_multi_field)} documents\")\n",
    "print(f\"   Vector store ID: {VECTOR_STORE_ID_MULTI_FIELD}\")\n",
    "\n",
    "print(f\"\\nüí° LlamaStack automatically:\")\n",
    "print(f\"   - Chunked the documents\")\n",
    "print(f\"   - Generated embeddings for each chunk\")\n",
    "print(f\"   - Stored them in ChromaDB for semantic search\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** We indexed all documents into both ChromaDB vector stores! ‚úÖ\n",
    "\n",
    "**üéâ Success!** The tickets are now searchable using semantic similarity in both vector stores.\n",
    "\n",
    "**üí° What happened behind the scenes:**\n",
    "- LlamaStack automatically chunked the documents\n",
    "- Generated embeddings using the embedding model\n",
    "- Stored them in the vector databases\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7: Verify Both Vector Stores\n",
    "\n",
    "**What we're doing:** Checking that both vector stores contain our documents and are ready for queries.\n",
    "\n",
    "**Why:** Verification ensures the ingestion was successful before we start querying in notebooks 01 and 02.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display both vector stores with documents after indexing\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä Vector Store Status After Indexing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================\n",
    "# Verify Single-Field Vector Store\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üì¶ Single-Field Vector Store\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "vs_single_updated = client.vector_stores.retrieve(VECTOR_STORE_ID_SINGLE_FIELD)\n",
    "\n",
    "print(f\"\\nüì¶ Vector Store Details:\")\n",
    "print(f\"   ID: {vs_single_updated.id}\")\n",
    "print(f\"   Status: {vs_single_updated.status}\")\n",
    "if vs_single_updated.name:\n",
    "    print(f\"   Name: {vs_single_updated.name}\")\n",
    "if vs_single_updated.metadata:\n",
    "    provider = vs_single_updated.metadata.get('provider_id', 'N/A')\n",
    "    print(f\"   Provider: {provider}\")\n",
    "\n",
    "if hasattr(vs_single_updated, 'file_counts') and vs_single_updated.file_counts:\n",
    "    print(f\"\\nüìä Document Statistics:\")\n",
    "    print(f\"   Total files: {vs_single_updated.file_counts.total}\")\n",
    "    print(f\"   Completed: {vs_single_updated.file_counts.completed}\")\n",
    "    print(f\"   In progress: {vs_single_updated.file_counts.in_progress}\")\n",
    "    print(f\"   Failed: {vs_single_updated.file_counts.failed}\")\n",
    "\n",
    "# ============================================================\n",
    "# Verify Multi-Field Vector Store\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üì¶ Multi-Field Vector Store\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "vs_multi_updated = client.vector_stores.retrieve(VECTOR_STORE_ID_MULTI_FIELD)\n",
    "\n",
    "print(f\"\\nüì¶ Vector Store Details:\")\n",
    "print(f\"   ID: {vs_multi_updated.id}\")\n",
    "print(f\"   Status: {vs_multi_updated.status}\")\n",
    "if vs_multi_updated.name:\n",
    "    print(f\"   Name: {vs_multi_updated.name}\")\n",
    "if vs_multi_updated.metadata:\n",
    "    provider = vs_multi_updated.metadata.get('provider_id', 'N/A')\n",
    "    print(f\"   Provider: {provider}\")\n",
    "\n",
    "if hasattr(vs_multi_updated, 'file_counts') and vs_multi_updated.file_counts:\n",
    "    print(f\"\\nüìä Document Statistics:\")\n",
    "    print(f\"   Total files: {vs_multi_updated.file_counts.total}\")\n",
    "    print(f\"   Completed: {vs_multi_updated.file_counts.completed}\")\n",
    "    print(f\"   In progress: {vs_multi_updated.file_counts.in_progress}\")\n",
    "    print(f\"   Failed: {vs_multi_updated.file_counts.failed}\")\n",
    "\n",
    "# ============================================================\n",
    "# Test queries on both vector stores\n",
    "# ============================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üîç Testing Both Vector Stores\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "sample_query = \"IT support ticket\"\n",
    "\n",
    "# Test single-field vector store\n",
    "print(f\"\\nüìù Testing single-field vector store...\")\n",
    "try:\n",
    "    query_result_single = client.tool_runtime.rag_tool.query(\n",
    "        content=sample_query,\n",
    "        vector_db_ids=[str(VECTOR_STORE_ID_SINGLE_FIELD)],\n",
    "        extra_body={\"vector_store_ids\": [str(VECTOR_STORE_ID_SINGLE_FIELD)]},\n",
    "    )\n",
    "    print(f\"   ‚úÖ Single-field vector store is queryable!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Could not query single-field vector store: {e}\")\n",
    "\n",
    "# Test multi-field vector store\n",
    "print(f\"\\nüìù Testing multi-field vector store...\")\n",
    "try:\n",
    "    query_result_multi = client.tool_runtime.rag_tool.query(\n",
    "        content=sample_query,\n",
    "        vector_db_ids=[str(VECTOR_STORE_ID_MULTI_FIELD)],\n",
    "        extra_body={\"vector_store_ids\": [str(VECTOR_STORE_ID_MULTI_FIELD)]},\n",
    "    )\n",
    "    print(f\"   ‚úÖ Multi-field vector store is queryable!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Could not query multi-field vector store: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ Both vector stores are ready for semantic search!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** We verified both vector stores are working! ‚úÖ\n",
    "\n",
    "**üéâ Success!** Your data is now ingested into both vector stores and ready for semantic search.\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "**What you accomplished:**\n",
    "- ‚úÖ Created two vector stores in LlamaStack (single-field and multi-field)\n",
    "- ‚úÖ Prepared documents for both ingestion modes\n",
    "- ‚úÖ Indexed documents into both vector databases\n",
    "- ‚úÖ Verified both ingestions were successful\n",
    "\n",
    "**üí° Important Information to Save:**\n",
    "\n",
    "```python\n",
    "# Vector Store IDs (save these for query notebooks!)\n",
    "VECTOR_STORE_ID_SINGLE_FIELD = \"{VECTOR_STORE_ID_SINGLE_FIELD}\"\n",
    "VECTOR_STORE_ID_MULTI_FIELD = \"{VECTOR_STORE_ID_MULTI_FIELD}\"\n",
    "```\n",
    "\n",
    "**Next Steps:**\n",
    "- **For single-field RAG**: Use `01_introduction_to_rag.ipynb` to query the single-field vector store\n",
    "- **For multi-field RAG**: Use `02_advanced_rag_with_multiple_fields.ipynb` to query the multi-field vector store\n",
    "\n",
    "**üí° Tip:** Both vector stores are now ready! You can use notebook 01 and notebook 02 without re-running ingestion.\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ You Did It!\n",
    "\n",
    "You've successfully ingested IT tickets into both vector databases! The documents are now searchable using semantic similarity in both formats.\n",
    "\n",
    "**What's next:** \n",
    "- **Single-field RAG**: `01_introduction_to_rag.ipynb` - Learn basic semantic search\n",
    "- **Multi-field RAG**: `02_advanced_rag_with_multiple_fields.ipynb` - Learn advanced semantic search with full context\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
