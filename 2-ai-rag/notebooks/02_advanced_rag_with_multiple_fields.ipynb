{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 02: Advanced RAG with Multiple Fields\n",
    "\n",
    "## ðŸŽ¯ Your Mission\n",
    "\n",
    "You're an IT support engineer learning advanced semantic search with RAG. Your job today: query a multi-field vector store that combines multiple ticket fields (`short_description`, `content`, and `close_notes`) to find both problems AND solutions.\n",
    "\n",
    "**Why this matters:** This same multi-field RAG approach is how you could build intelligent search systems that understand the full context of incidents - from initial problem reports to diagnostic findings to final resolutions - enabling better pattern recognition and faster problem resolution.\n",
    "\n",
    "**ðŸ“‹ Prerequisites:** \n",
    "- Complete `00_data_ingestion.ipynb` first (it creates both vector stores)\n",
    "- The multi-field vector store will be automatically found by name\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## âš¡ Quick Win (First 2 Minutes)\n",
    "\n",
    "Let's see multi-field RAG in action! Run the cell below to see how combining multiple fields improves search results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "tags": []
   },
   "source": [
    "**What you'll see:** By combining multiple fields (problem description + diagnostic details + resolution steps), the RAG system can find tickets that match both the problem AND the solution, not just the problem description alone.\n",
    "\n",
    "Now let's build it step by step to understand how it works.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- âœ… Combine multiple ticket fields to create richer RAG documents\n",
    "- âœ… Understand why multi-field RAG outperforms single-field RAG\n",
    "- âœ… Build a search system that finds both problems and solutions\n",
    "\n",
    "**Time:** ~15-20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ The Journey\n",
    "\n",
    "We'll query the multi-field RAG system step by step:\n",
    "\n",
    "1. **Explore the Data** - Understand the ticket fields and how they're combined\n",
    "2. **Set Up LlamaStack** - Connect to our RAG platform\n",
    "3. **Load Vector Store** - Connect to the pre-indexed multi-field vector database\n",
    "4. **Query & Search** - Test queries that benefit from multi-field RAG\n",
    "5. **Compare Approaches** - See why multi-field RAG outperforms single-field RAG\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Step 1: Load and Explore the Dataset\n",
    "\n",
    "**What we're doing:** Loading IT call center tickets and examining their structure.\n",
    "\n",
    "**Why:** We need to understand what fields are available so we can combine them effectively for better search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "from llama_stack_client import RAGDocument\n",
    "\n",
    "# Load the CSV file from the data directory\n",
    "data_dir = Path(\"../data\")\n",
    "file_path = data_dir / \"synthetic-it-call-center-tickets-sample.csv\"\n",
    "\n",
    "print(\"ðŸ”„ Loading IT call center tickets dataset...\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"âœ… Loaded {len(df)} tickets\")\n",
    "print(f\"ðŸ“‹ Dataset shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"\\nðŸ” Let's examine the dataset structure:\")\n",
    "print(\"=\" * 60)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "**What we see:** Each ticket has multiple fields:\n",
    "- **`short_description`** - Brief problem summary\n",
    "- **`content`** - Detailed problem description\n",
    "- **`close_notes`** - Diagnostic findings and resolution steps\n",
    "- **Other fields** - Metadata like ticket number, priority, etc.\n",
    "\n",
    "**ðŸ’¡ Key insight:** By combining `short_description`, `content`, and `close_notes`, we create documents that contain the full ticket lifecycle - problem â†’ diagnosis â†’ solution. This enables better search!\n",
    "\n",
    "Let's see the field structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataset structure and key fields\n",
    "print(\"ðŸ“Š Dataset Structure:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nðŸ“ Key Fields for Multi-Field RAG:\")\n",
    "print(f\"   - short_description: Brief problem summary\")\n",
    "print(f\"   - content: Detailed problem description\")  \n",
    "print(f\"   - close_notes: Diagnostic findings and resolution steps\")\n",
    "print(f\"\\nðŸ’¡ Other fields will be stored as metadata for filtering\")\n",
    "\n",
    "# Show an example ticket to illustrate the multi-field concept\n",
    "print(\"\\nðŸ“‹ Example Ticket (showing multi-field structure):\")\n",
    "print(\"=\" * 60)\n",
    "if len(df) > 0:\n",
    "    example = df.iloc[0]\n",
    "    print(f\"\\nðŸŽ« Ticket #{example.get('number', 'N/A')}\")\n",
    "    print(f\"\\nðŸ“Œ Short Description:\")\n",
    "    print(f\"   {example.get('short_description', 'N/A')[:100]}...\")\n",
    "    print(f\"\\nðŸ“„ Content:\")\n",
    "    print(f\"   {str(example.get('content', 'N/A'))[:150]}...\")\n",
    "    print(f\"\\nâœ… Close Notes:\")\n",
    "    print(f\"   {str(example.get('close_notes', 'N/A'))[:150]}...\")\n",
    "    print(f\"\\nðŸ’¡ Notice: Combining all three fields gives us the complete ticket story!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: Set Up LlamaStack Client\n",
    "\n",
    "**What we're doing:** Connecting to LlamaStack and configuring our environment.\n",
    "\n",
    "**Why:** We need LlamaStack to handle vector database operations, embeddings, and RAG queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**What happened:** We explored the dataset and understand its structure. Now let's connect to LlamaStack.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for LlamaStack\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "# Add root src directory to path to import shared config\n",
    "root_dir = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(root_dir / \"src\"))\n",
    "\n",
    "# Import centralized configuration\n",
    "from config import LLAMA_STACK_URL, MODEL, CONFIG\n",
    "\n",
    "# Configuration values (automatically detected based on environment)\n",
    "llamastack_url = LLAMA_STACK_URL\n",
    "model = MODEL\n",
    "\n",
    "if not llamastack_url:\n",
    "    raise ValueError(\n",
    "        \"LLAMA_STACK_URL is not configured!\\n\"\n",
    "        \"Please run: ./scripts/setup-env.sh\\n\"\n",
    "        \"Or set LLAMA_STACK_URL environment variable:\\n\"\n",
    "        \"  export LLAMA_STACK_URL='https://llamastack-route-my-first-model.apps.ocp.example.com'\"\n",
    "    )\n",
    "\n",
    "print(\"ðŸ”„ Step 1: Connecting to LlamaStack...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ðŸ“¡ LlamaStack URL: {llamastack_url}\")\n",
    "print(f\"ðŸ¤– Model: {model}\")\n",
    "print(f\"ðŸ“ Environment: {'Inside OpenShift cluster' if CONFIG['inside_cluster'] else 'Outside OpenShift cluster'}\")\n",
    "print(f\"ðŸ“¦ Namespace: {CONFIG['namespace']}\")\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "client = LlamaStackClient(base_url=llamastack_url)\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    model_count = len(models.data) if hasattr(models, 'data') else len(models)\n",
    "    print(f\"\\nâœ… Connected to LlamaStack\")\n",
    "    print(f\"   Available models: {model_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Cannot connect to LlamaStack: {e}\")\n",
    "    print(\"\\nðŸ’¡ Troubleshooting:\")\n",
    "    print(\"   1. Check if route exists: oc get route llamastack-route -n my-first-model\")\n",
    "    print(\"   2. Run setup script: ./scripts/setup-env.sh\")\n",
    "    print(\"   3. Or set LLAMA_STACK_URL manually in .env file\")\n",
    "    raise\n",
    "\n",
    "# Configure inference parameters\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"\\nâš™ï¸  Inference Parameters:\")\n",
    "print(f\"   Model: {model}\")\n",
    "print(f\"   Temperature: {temperature}\")\n",
    "print(f\"   Max Tokens: {max_tokens}\")\n",
    "print(f\"   Stream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "**What happened:** We connected to LlamaStack and configured our inference parameters. Now we're ready to load the multi-field vector store.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Load Multi-Field Vector Store\n",
    "\n",
    "**What we're doing:** Connecting to the pre-indexed multi-field vector store created in `00_data_ingestion.ipynb`.\n",
    "\n",
    "**Why:** The vector store already contains indexed tickets with combined fields (`short_description + content + close_notes`). We just need to connect to it to start querying.\n",
    "\n",
    "**ðŸ“‹ Prerequisites:** \n",
    "- You must have completed `00_data_ingestion.ipynb` (it creates both vector stores)\n",
    "- The multi-field vector store will be automatically found by name\n",
    "\n",
    "**ðŸ’¡ Why use a separate vector store from notebook 01?**\n",
    "\n",
    "In notebook 01, we queried a vector store with only `short_description` field (problem summary). In this notebook, we're querying a vector store with `short_description + content + close_notes` (full ticket lifecycle). \n",
    "\n",
    "**We use separate vector stores because:**\n",
    "- **Different document structures**: Single-field vs multi-field documents have different content and embeddings\n",
    "- **Better separation**: Keeping them separate makes it easier to compare single-field vs multi-field RAG performance\n",
    "- **Pedagogical clarity**: Using separate vector stores helps demonstrate the multi-field RAG concept clearly\n",
    "\n",
    "**In production:** You could reuse a vector store and add different document types to it, or create separate vector stores for different document structures - the choice depends on your use case and whether you want to keep different document types separate or combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector store ID\n",
    "# Option 1: Use the vector store ID from the ingestion notebook (if you ran it in the same session)\n",
    "# Option 2: Manually set it here if you have a saved vector store ID\n",
    "\n",
    "print(\"\\nðŸ”„ Step 3: Loading multi-field vector store...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Try to use VECTOR_STORE_ID_MULTI_FIELD from ingestion notebook, or find by name\n",
    "try:\n",
    "    # Check if VECTOR_STORE_ID_MULTI_FIELD exists from ingestion notebook\n",
    "    if 'VECTOR_STORE_ID_MULTI_FIELD' in globals():\n",
    "        vector_store_id = VECTOR_STORE_ID_MULTI_FIELD\n",
    "        VECTOR_STORE_ID = VECTOR_STORE_ID_MULTI_FIELD  # For backward compatibility\n",
    "        print(f\"âœ… Using vector store ID from ingestion notebook\")\n",
    "    else:\n",
    "        raise NameError(\"VECTOR_STORE_ID_MULTI_FIELD not found\")\n",
    "except NameError:\n",
    "    # Try to find vector store by name\n",
    "    print(\"ðŸ”„ Attempting to find vector store by name 'multi-field-rag-tickets'...\")\n",
    "    try:\n",
    "        vector_stores = client.vector_stores.list()\n",
    "        found = False\n",
    "        for vs in vector_stores.data if hasattr(vector_stores, 'data') else []:\n",
    "            if hasattr(vs, 'name') and vs.name == 'multi-field-rag-tickets':\n",
    "                VECTOR_STORE_ID_MULTI_FIELD = vs.id\n",
    "                VECTOR_STORE_ID = vs.id  # For backward compatibility\n",
    "                vector_store_id = vs.id\n",
    "                found = True\n",
    "                print(f\"âœ… Found vector store: {vs.id}\")\n",
    "                break\n",
    "        if not found:\n",
    "            raise ValueError(\"Vector store not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Could not find vector store: {e}\")\n",
    "        print(f\"\\nðŸ’¡ Please run 00_data_ingestion.ipynb first!\")\n",
    "        print(f\"   It will create both vector stores (single-field and multi-field)\")\n",
    "        raise\n",
    "\n",
    "# Retrieve vector store information\n",
    "vs_chroma = client.vector_stores.retrieve(VECTOR_STORE_ID_MULTI_FIELD)\n",
    "\n",
    "print(f\"\\nâœ… Vector store loaded!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ðŸ“¦ Vector Store Details:\")\n",
    "print(f\"   ID: {vs_chroma.id}\")\n",
    "print(f\"   Status: {vs_chroma.status}\")\n",
    "if vs_chroma.name:\n",
    "    print(f\"   Name: {vs_chroma.name}\")\n",
    "if vs_chroma.metadata:\n",
    "    provider = vs_chroma.metadata.get('provider_id', 'N/A')\n",
    "    print(f\"   Provider: {provider}\")\n",
    "\n",
    "# Display document statistics\n",
    "if hasattr(vs_chroma, 'file_counts') and vs_chroma.file_counts:\n",
    "    print(f\"\\nðŸ“Š Document Statistics:\")\n",
    "    print(f\"   Total files: {vs_chroma.file_counts.total}\")\n",
    "    print(f\"   Completed: {vs_chroma.file_counts.completed}\")\n",
    "    print(f\"   In progress: {vs_chroma.file_counts.in_progress}\")\n",
    "    print(f\"   Failed: {vs_chroma.file_counts.failed}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nðŸ’¡ Vector store is ready for semantic search!\")\n",
    "print(f\"   Documents contain: short_description + content + close_notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "**What happened:** We loaded the multi-field vector store! âœ…\n",
    "\n",
    "**ðŸ’¡ What's in the Vector Store?**\n",
    "- **Content:** Combined `short_description + content + close_notes` (full ticket story)\n",
    "- **Metadata:** All other ticket fields (for filtering)\n",
    "\n",
    "**Key point:** The vector store was created and populated in `00_data_ingestion.ipynb`. It already contains indexed tickets ready for semantic search.\n",
    "\n",
    "**What's next:** Now we can query the vector store to see how multi-field RAG improves search results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents were already indexed in 00_data_ingestion.ipynb\n",
    "# No need to prepare or index here - the vector store is ready to query!\n",
    "\n",
    "print(\"âœ… Vector store is ready for querying!\")\n",
    "print(\"ðŸ’¡ Documents were indexed in 00_data_ingestion.ipynb\")\n",
    "print(\"   - Content fields: short_description + content + close_notes\")\n",
    "print(\"   - Metadata: All other ticket fields (for filtering)\")\n",
    "print(f\"\\nðŸ’¡ Each document contains:\")\n",
    "print(f\"   - Content: short_description + content + close_notes (full ticket story)\")\n",
    "print(f\"   - Metadata: All other fields (for filtering)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "**What happened:** We created RAG documents that combine multiple fields. Each document now contains the complete ticket story - from problem description to diagnostic findings to resolution steps.\n",
    "\n",
    "**ðŸ’¡ Key insight:** This multi-field approach enables the RAG system to match queries based on:\n",
    "- Problem descriptions (from `short_description` and `content`)\n",
    "- Diagnostic details (from `content` and `close_notes`)\n",
    "- Solution steps (from `close_notes`)\n",
    "\n",
    "This is much more powerful than single-field RAG!\n",
    "\n",
    "Now let's index these documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.3: Index documents into the vector store (in batches to avoid timeout)\n",
    "print(\"\\nðŸ”„ Step 3.3: Indexing documents into vector store...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Chunk size: 1024 tokens\")\n",
    "print(f\"   Total documents: {len(documents)}\")\n",
    "print(f\"   Processing in batches of 100 to avoid timeout...\")\n",
    "\n",
    "# Process in batches to avoid gateway timeout\n",
    "BATCH_SIZE = 10\n",
    "total_batches = (len(documents) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "inserted_count = 0\n",
    "\n",
    "for batch_num in range(total_batches):\n",
    "    start_idx = batch_num * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(documents))\n",
    "    batch = documents[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"\\n   Batch {batch_num + 1}/{total_batches}: Processing documents {start_idx} to {end_idx-1}...\")\n",
    "    \n",
    "    try:\n",
    "        insert_result = client.tool_runtime.rag_tool.insert( \n",
    "            chunk_size_in_tokens=1024,\n",
    "            documents=batch,\n",
    "            vector_db_id=str(vs_chroma.id),\n",
    "            extra_body={\"vector_store_id\": str(vs_chroma.id)},\n",
    "            extra_headers=None,\n",
    "            extra_query=None,\n",
    "            timeout=300  # 5 minute timeout per batch\n",
    "        )\n",
    "        inserted_count += len(batch)\n",
    "        print(f\"   âœ… Batch {batch_num + 1} indexed successfully ({inserted_count}/{len(documents)} documents)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Error indexing batch {batch_num + 1}: {e}\")\n",
    "        print(f\"   ðŸ’¡ Tip: You can continue with the documents already indexed, or reduce BATCH_SIZE\")\n",
    "        # Continue with next batch instead of failing completely\n",
    "        continue\n",
    "\n",
    "print(f\"\\nâœ… Indexing complete!\")\n",
    "print(f\"   Successfully indexed: {inserted_count}/{len(documents)} documents\")\n",
    "print(f\"   Vector store ID: {vs_chroma.id}\")\n",
    "print(f\"\\nðŸ’¡ LlamaStack automatically:\")\n",
    "print(f\"   - Chunked the documents\")\n",
    "print(f\"   - Generated embeddings for each chunk\")\n",
    "print(f\"   - Stored them in ChromaDB for semantic search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "**What happened:** The multi-field vector store is ready! âœ…\n",
    "\n",
    "**ðŸŽ‰ Success!** The multi-field tickets are already indexed and searchable. This was done in `00_data_ingestion.ipynb`. Each document contains:\n",
    "- âœ… Problem description (`short_description`)\n",
    "- âœ… Detailed context (`content`)\n",
    "- âœ… Diagnostic findings and solutions (`close_notes`)\n",
    "\n",
    "**ðŸ’¡ What happened in the ingestion notebook:**\n",
    "- Documents were created by combining `short_description + content + close_notes`\n",
    "- LlamaStack automatically chunked the combined field content\n",
    "- Generated embeddings using the embedding model\n",
    "- Stored them in the vector database for semantic search\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Query with Multi-Field RAG\n",
    "\n",
    "**What we're doing:** Testing our multi-field RAG system with queries that benefit from combined fields.\n",
    "\n",
    "**Why:** Multi-field RAG excels at queries that need both problem AND solution context, not just problem descriptions. This is where you'll see the power of combining multiple fields!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.vector_io.query(vector_db_id=vs_chroma.id,query=\"ZTrend crashes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Step 4.1: Execute Queries Using the RAG Tool\n",
    "\n",
    "**What we're doing:** Using the built-in RAG tool to query our multi-field vector database.\n",
    "\n",
    "**How it works:**\n",
    "1. Query the vector database to retrieve relevant document chunks\n",
    "2. Construct an extended prompt using the retrieved context\n",
    "3. Query the LLM with the extended prompt\n",
    "4. Get answers that combine retrieved context with LLM reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What was the root cause and resolution for application crashes related to memory issues?\",\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # RAG retrieval call\n",
    "    rag_response = client.tool_runtime.rag_tool.query(\n",
    "        content=prompt,\n",
    "        vector_db_ids=[str(vs_chroma.id)],   # o SDK exige isso\n",
    "        extra_body={\"vector_store_ids\": [str(vs_chroma.id)]},  # o backend exige isso\n",
    "    )\n",
    "\n",
    "    print(rag_response.content)\n",
    "    # the list of messages to be sent to the model must start with the system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "    # construct the actual prompt to be executed, incorporating the original query and the retrieved content\n",
    "    prompt_context = rag_response.content\n",
    "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{prompt_context}\\n\\nQUERY:\\n{prompt}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "    # use Llama Stack inference API to directly communicate with the desired model\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        stream=stream,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "if stream:\n",
    "    for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "    print()  # nova linha apÃ³s streaming\n",
    "else:\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Step 5: Why Multi-Field RAG is Better\n",
    "\n",
    "**What we're learning:** Understanding when and why multi-field RAG outperforms single-field RAG.\n",
    "\n",
    "**Why this matters:** Knowing the strengths of multi-field RAG helps you decide when to use it in production systems.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Queries: Multi-Field vs Single-Field RAG\n",
    "\n",
    "Using multiple fields (`short_description`, `content`, and `close_notes`) instead of just `short_description` significantly improves retrieval quality for certain types of queries. Here are examples where multi-field RAG outperforms single-field RAG:\n",
    "\n",
    "**Example 1: Troubleshooting Steps and Solutions**\n",
    "**Query**: \"How do I fix ZTrend crashes when saving files?\"\n",
    "\n",
    "- **Single-field (short_description only)**: May retrieve tickets about crashes, but won't have the solution steps\n",
    "- **Multi-field**: Retrieves tickets with both the problem description AND the detailed troubleshooting steps from `close_notes`, providing complete answers\n",
    "\n",
    "**Example 2: Historical Context and Resolution**\n",
    "**Query**: \"What was the root cause and resolution for application crashes related to memory issues?\"\n",
    "\n",
    "- **Single-field**: Only finds tickets mentioning \"crashes\" but misses the diagnostic details and resolution steps\n",
    "- **Multi-field**: Retrieves tickets with full context from `content` (initial problem description) and `close_notes` (diagnostic findings and resolution), enabling comprehensive answers\n",
    "\n",
    "**Example 3: Pattern Recognition Across Problem-Solution Pairs**\n",
    "**Query**: \"What are common solutions for software crashes that involve configuration files?\"\n",
    "\n",
    "- **Single-field**: Can identify crash-related tickets but can't see the solutions\n",
    "- **Multi-field**: Can match both problem patterns (from `short_description`/`content`) and solution patterns (from `close_notes`), enabling identification of recurring problem-solution patterns\n",
    "\n",
    "**Example 4: Detailed Technical Information**\n",
    "**Query**: \"Show me tickets where log file analysis revealed the issue\"\n",
    "\n",
    "- **Single-field**: May miss tickets where log analysis is only mentioned in `content` or `close_notes`\n",
    "- **Multi-field**: Captures technical details from all fields, ensuring comprehensive retrieval of relevant tickets\n",
    "\n",
    "**Example 5: End-to-End Ticket Understanding**\n",
    "**Query**: \"Find tickets where the customer reported a problem, diagnostics were performed, and the issue was resolved by reinstalling software\"\n",
    "\n",
    "- **Single-field**: Can't capture the full narrative flow from problem â†’ diagnosis â†’ solution\n",
    "- **Multi-field**: Preserves the complete ticket lifecycle, enabling retrieval based on complex multi-stage scenarios\n",
    "\n",
    "**Key Insight**: Multi-field RAG is especially powerful for queries that require understanding both the problem AND the solution, or queries that need to match patterns across different stages of the ticket lifecycle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ Key Takeaways\n",
    "This notebook demonstrated how to set up and use the built-in RAG tool for ingesting user-provided documents in a vector database and utilizing them during inference via direct retrieval. \n",
    "\n",
    "Key points:\n",
    "- **Multi-field content**: We combined `short_description`, `content`, and `close_notes` fields to create richer document representations, improving the quality of retrieval and context understanding.\n",
    "- **Metadata preservation**: Other fields from the dataset are stored as metadata, allowing for filtering and additional context during retrieval.\n",
    "- **Vector database integration**: The documents are chunked and indexed into ChromaDB using Llama Stack's RAG tool, enabling semantic search over the ticket data.\n",
    "- **Query advantages**: As shown in Section 4, multi-field RAG excels at queries requiring both problem and solution context, pattern recognition across ticket lifecycle stages, and comprehensive technical information retrieval.\n",
    "\n",
    "Now that we've seen how easy it is to implement RAG with Llama Stack, We'll move on to building a simple agent with Llama Stack next in our [Simple Agents](./Level2_simple_agent_with_websearch.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
