{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 02: Build Multi-Field RAG for IT Ticket Search\n",
    "\n",
    "## üéØ Your Mission\n",
    "\n",
    "You're an IT support engineer building a smarter ticket search system. Your job today: combine multiple ticket fields (`short_description`, `content`, and `close_notes`) to create richer document representations that help find both problems AND solutions.\n",
    "\n",
    "**Why this matters:** This same multi-field RAG approach is how you could build intelligent search systems that understand the full context of incidents - from initial problem reports to diagnostic findings to final resolutions - enabling better pattern recognition and faster problem resolution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## ‚ö° Quick Win (First 2 Minutes)\n",
    "\n",
    "Let's see multi-field RAG in action! Run the cell below to see how combining multiple fields improves search results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "tags": []
   },
   "source": [
    "**What you'll see:** By combining multiple fields (problem description + diagnostic details + resolution steps), the RAG system can find tickets that match both the problem AND the solution, not just the problem description alone.\n",
    "\n",
    "Now let's build it step by step to understand how it works.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Combine multiple ticket fields to create richer RAG documents\n",
    "- ‚úÖ Understand why multi-field RAG outperforms single-field RAG\n",
    "- ‚úÖ Build a search system that finds both problems and solutions\n",
    "\n",
    "**Time:** ~15-20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üìã The Journey\n",
    "\n",
    "We'll build this step by step:\n",
    "\n",
    "1. **Explore the Data** - Understand the ticket fields and how to combine them\n",
    "2. **Set Up LlamaStack** - Connect to our RAG platform\n",
    "3. **Create Multi-Field Documents** - Combine `short_description`, `content`, and `close_notes`\n",
    "4. **Index Documents** - Store multi-field documents in the vector database\n",
    "5. **Query & Search** - Test queries that benefit from multi-field RAG\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Step 1: Load and Explore the Dataset\n",
    "\n",
    "**What we're doing:** Loading IT call center tickets and examining their structure.\n",
    "\n",
    "**Why:** We need to understand what fields are available so we can combine them effectively for better search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "from llama_stack_client import RAGDocument\n",
    "\n",
    "# Load the CSV file from the data directory\n",
    "data_dir = Path(\"../data\")\n",
    "file_path = data_dir / \"synthetic-it-call-center-tickets-sample.csv\"\n",
    "\n",
    "print(\"üîÑ Loading IT call center tickets dataset...\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df)} tickets\")\n",
    "print(f\"üìã Dataset shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"\\nüîç Let's examine the dataset structure:\")\n",
    "print(\"=\" * 60)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "**What we see:** Each ticket has multiple fields:\n",
    "- **`short_description`** - Brief problem summary\n",
    "- **`content`** - Detailed problem description\n",
    "- **`close_notes`** - Diagnostic findings and resolution steps\n",
    "- **Other fields** - Metadata like ticket number, priority, etc.\n",
    "\n",
    "**üí° Key insight:** By combining `short_description`, `content`, and `close_notes`, we create documents that contain the full ticket lifecycle - problem ‚Üí diagnosis ‚Üí solution. This enables better search!\n",
    "\n",
    "Let's see the field structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataset structure and key fields\n",
    "print(\"üìä Dataset Structure:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nüìù Key Fields for Multi-Field RAG:\")\n",
    "print(f\"   - short_description: Brief problem summary\")\n",
    "print(f\"   - content: Detailed problem description\")  \n",
    "print(f\"   - close_notes: Diagnostic findings and resolution steps\")\n",
    "print(f\"\\nüí° Other fields will be stored as metadata for filtering\")\n",
    "\n",
    "# Show an example ticket to illustrate the multi-field concept\n",
    "print(\"\\nüìã Example Ticket (showing multi-field structure):\")\n",
    "print(\"=\" * 60)\n",
    "if len(df) > 0:\n",
    "    example = df.iloc[0]\n",
    "    print(f\"\\nüé´ Ticket #{example.get('number', 'N/A')}\")\n",
    "    print(f\"\\nüìå Short Description:\")\n",
    "    print(f\"   {example.get('short_description', 'N/A')[:100]}...\")\n",
    "    print(f\"\\nüìÑ Content:\")\n",
    "    print(f\"   {str(example.get('content', 'N/A'))[:150]}...\")\n",
    "    print(f\"\\n‚úÖ Close Notes:\")\n",
    "    print(f\"   {str(example.get('close_notes', 'N/A'))[:150]}...\")\n",
    "    print(f\"\\nüí° Notice: Combining all three fields gives us the complete ticket story!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: Set Up LlamaStack Client\n",
    "\n",
    "**What we're doing:** Connecting to LlamaStack and configuring our environment.\n",
    "\n",
    "**Why:** We need LlamaStack to handle vector database operations, embeddings, and RAG queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**What happened:** We explored the dataset and understand its structure. Now let's connect to LlamaStack.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for LlamaStack\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "# Add root src directory to path to import shared config\n",
    "root_dir = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(root_dir / \"src\"))\n",
    "\n",
    "# Import centralized configuration\n",
    "from config import LLAMA_STACK_URL, MODEL, CONFIG\n",
    "\n",
    "# Configuration values (automatically detected based on environment)\n",
    "llamastack_url = LLAMA_STACK_URL\n",
    "model = MODEL\n",
    "\n",
    "if not llamastack_url:\n",
    "    raise ValueError(\n",
    "        \"LLAMA_STACK_URL is not configured!\\n\"\n",
    "        \"Please run: ./scripts/setup-env.sh\\n\"\n",
    "        \"Or set LLAMA_STACK_URL environment variable:\\n\"\n",
    "        \"  export LLAMA_STACK_URL='https://llamastack-route-my-first-model.apps.ocp.example.com'\"\n",
    "    )\n",
    "\n",
    "print(\"üîÑ Step 1: Connecting to LlamaStack...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üì° LlamaStack URL: {llamastack_url}\")\n",
    "print(f\"ü§ñ Model: {model}\")\n",
    "print(f\"üìç Environment: {'Inside OpenShift cluster' if CONFIG['inside_cluster'] else 'Outside OpenShift cluster'}\")\n",
    "print(f\"üì¶ Namespace: {CONFIG['namespace']}\")\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "client = LlamaStackClient(base_url=llamastack_url)\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    model_count = len(models.data) if hasattr(models, 'data') else len(models)\n",
    "    print(f\"\\n‚úÖ Connected to LlamaStack\")\n",
    "    print(f\"   Available models: {model_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Cannot connect to LlamaStack: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   1. Check if route exists: oc get route llamastack-route -n my-first-model\")\n",
    "    print(\"   2. Run setup script: ./scripts/setup-env.sh\")\n",
    "    print(\"   3. Or set LLAMA_STACK_URL manually in .env file\")\n",
    "    raise\n",
    "\n",
    "# Configure inference parameters\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Inference Parameters:\")\n",
    "print(f\"   Model: {model}\")\n",
    "print(f\"   Temperature: {temperature}\")\n",
    "print(f\"   Max Tokens: {max_tokens}\")\n",
    "print(f\"   Stream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "**What happened:** We connected to LlamaStack and configured our inference parameters. Now we're ready to create the vector store and index documents.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Create Multi-Field Documents and Index Them\n",
    "\n",
    "**What we're doing:** Creating multi-field RAG documents by combining ticket fields, then indexing them into a vector store.\n",
    "\n",
    "**Why:** By combining `short_description`, `content`, and `close_notes`, we create richer document representations that enable better search results - finding both problems AND solutions.\n",
    "\n",
    "**This step includes:**\n",
    "1. Create a vector store\n",
    "2. Prepare and combine ticket fields\n",
    "3. Create multi-field RAG documents\n",
    "4. Index documents into the vector store\n",
    "\n",
    "**üí° Why create a new vector store instead of reusing notebook 01's?**\n",
    "\n",
    "In notebook 01, we indexed documents using only the `short_description` field (problem summary). In this notebook, we're indexing documents that combine `short_description + content + close_notes` (full ticket lifecycle). \n",
    "\n",
    "**We create a new vector store because:**\n",
    "- **Different document structures**: Single-field vs multi-field documents have different content and embeddings\n",
    "- **Better separation**: Keeping them separate makes it easier to compare single-field vs multi-field RAG performance\n",
    "- **Pedagogical clarity**: Creating a new vector store helps demonstrate the multi-field RAG concept clearly\n",
    "\n",
    "**In production:** You could reuse a vector store and add different document types to it, or create separate vector stores for different document structures - the choice depends on your use case and whether you want to keep different document types separate or combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new vector store for multi-field RAG\n",
    "# Note: We create a new vector store instead of reusing notebook 01's because:\n",
    "# - Notebook 01 indexes single-field documents (short_description only)\n",
    "# - This notebook indexes multi-field documents (short_description + content + close_notes)\n",
    "# - Different document structures benefit from separate vector stores for clarity and comparison\n",
    "#\n",
    "# To reuse notebook 01's vector store instead, you could:\n",
    "# 1. List existing vector stores: client.vector_stores.list()\n",
    "# 2. Retrieve a specific one: vs_chroma = client.vector_stores.retrieve(\"vs_<id-from-notebook-01>\")\n",
    "# 3. Then index your multi-field documents into that same vector store\n",
    "\n",
    "vs_chroma = client.vector_stores.create(\n",
    "    extra_body={\n",
    "        \"provider_id\": \"chromadb\",  # Optional: specify vector store provider\n",
    "        \"embedding_model\": \"sentence-transformers/nomic-ai/nomic-embed-text-v1.5\",\n",
    "        \"embedding_dimension\": 768  # Optional: will be auto-detected if not provided\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "**What happened:** We created a ChromaDB vector store. ChromaDB is embedded in LlamaStack (no separate deployment needed), unlike MongoDB which requires a separate MCP server.\n",
    "\n",
    "Now let's prepare and combine the ticket fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the data\n",
    "print(\"\\nüîÑ Step 2: Preparing data for indexing...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# Limit to first 1000 records for faster processing (you can use more for production)\n",
    "df_1000 = df  # Sample file already has 1000 rows\n",
    "print(f\"   Processing {len(df_1000)} tickets (out of {len(df)} total)\")\n",
    "\n",
    "# Step 3: Create multi-field RAG documents\n",
    "print(\"\\nüîÑ Step 3: Creating multi-field RAG documents...\")\n",
    "print(\"   Combining fields: short_description + content + close_notes\")\n",
    "print(\"   Storing other fields as metadata\")\n",
    "\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"ticket-{i}\",\n",
    "        content=f\"{df_1000.iloc[i]['short_description']}\\n\\n{df_1000.iloc[i]['content']}\\n\\n{df_1000.iloc[i]['close_notes']}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata=df_1000.iloc[i].drop([\"short_description\", \"content\", \"close_notes\"]).to_dict(),\n",
    "    )\n",
    "    for i in range(len(df_1000))\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(documents)} RAG documents\")\n",
    "print(f\"\\nüí° Each document contains:\")\n",
    "print(f\"   - Content: short_description + content + close_notes (full ticket story)\")\n",
    "print(f\"   - Metadata: All other fields (for filtering)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "**What happened:** We created RAG documents that combine multiple fields. Each document now contains the complete ticket story - from problem description to diagnostic findings to resolution steps.\n",
    "\n",
    "**üí° Key insight:** This multi-field approach enables the RAG system to match queries based on:\n",
    "- Problem descriptions (from `short_description` and `content`)\n",
    "- Diagnostic details (from `content` and `close_notes`)\n",
    "- Solution steps (from `close_notes`)\n",
    "\n",
    "This is much more powerful than single-field RAG!\n",
    "\n",
    "Now let's index these documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.3: Index documents into the vector store (in batches to avoid timeout)\n",
    "print(\"\\nüîÑ Step 3.3: Indexing documents into vector store...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Chunk size: 1024 tokens\")\n",
    "print(f\"   Total documents: {len(documents)}\")\n",
    "print(f\"   Processing in batches of 100 to avoid timeout...\")\n",
    "\n",
    "# Process in batches to avoid gateway timeout\n",
    "BATCH_SIZE = 10\n",
    "total_batches = (len(documents) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "inserted_count = 0\n",
    "\n",
    "for batch_num in range(total_batches):\n",
    "    start_idx = batch_num * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(documents))\n",
    "    batch = documents[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"\\n   Batch {batch_num + 1}/{total_batches}: Processing documents {start_idx} to {end_idx-1}...\")\n",
    "    \n",
    "    try:\n",
    "        insert_result = client.tool_runtime.rag_tool.insert( \n",
    "            chunk_size_in_tokens=1024,\n",
    "            documents=batch,\n",
    "            vector_db_id=str(vs_chroma.id),\n",
    "            extra_body={\"vector_store_id\": str(vs_chroma.id)},\n",
    "            extra_headers=None,\n",
    "            extra_query=None,\n",
    "            timeout=300  # 5 minute timeout per batch\n",
    "        )\n",
    "        inserted_count += len(batch)\n",
    "        print(f\"   ‚úÖ Batch {batch_num + 1} indexed successfully ({inserted_count}/{len(documents)} documents)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error indexing batch {batch_num + 1}: {e}\")\n",
    "        print(f\"   üí° Tip: You can continue with the documents already indexed, or reduce BATCH_SIZE\")\n",
    "        # Continue with next batch instead of failing completely\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Indexing complete!\")\n",
    "print(f\"   Successfully indexed: {inserted_count}/{len(documents)} documents\")\n",
    "print(f\"   Vector store ID: {vs_chroma.id}\")\n",
    "print(f\"\\nüí° LlamaStack automatically:\")\n",
    "print(f\"   - Chunked the documents\")\n",
    "print(f\"   - Generated embeddings for each chunk\")\n",
    "print(f\"   - Stored them in ChromaDB for semantic search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "**What happened:** We indexed all documents into ChromaDB! The documents are now searchable using semantic similarity.\n",
    "\n",
    "**üéâ Success!** The multi-field tickets are now searchable. Each document contains:\n",
    "- ‚úÖ Problem description (`short_description`)\n",
    "- ‚úÖ Detailed context (`content`)\n",
    "- ‚úÖ Diagnostic findings and solutions (`close_notes`)\n",
    "\n",
    "**üí° What happened behind the scenes:**\n",
    "- LlamaStack automatically chunked the combined field content\n",
    "- Generated embeddings using the embedding model\n",
    "- Stored them in the vector database for semantic search\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Query with Multi-Field RAG\n",
    "\n",
    "**What we're doing:** Testing our multi-field RAG system with queries that benefit from combined fields.\n",
    "\n",
    "**Why:** Multi-field RAG excels at queries that need both problem AND solution context, not just problem descriptions. This is where you'll see the power of combining multiple fields!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.vector_io.query(vector_db_id=vs_chroma.id,query=\"ZTrend crashes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Step 4.1: Execute Queries Using the RAG Tool\n",
    "\n",
    "**What we're doing:** Using the built-in RAG tool to query our multi-field vector database.\n",
    "\n",
    "**How it works:**\n",
    "1. Query the vector database to retrieve relevant document chunks\n",
    "2. Construct an extended prompt using the retrieved context\n",
    "3. Query the LLM with the extended prompt\n",
    "4. Get answers that combine retrieved context with LLM reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What was the root cause and resolution for application crashes related to memory issues?\",\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # RAG retrieval call\n",
    "    rag_response = client.tool_runtime.rag_tool.query(\n",
    "        content=prompt,\n",
    "        vector_db_ids=[str(vs_chroma.id)],   # o SDK exige isso\n",
    "        extra_body={\"vector_store_ids\": [str(vs_chroma.id)]},  # o backend exige isso\n",
    "    )\n",
    "\n",
    "    print(rag_response.content)\n",
    "    # the list of messages to be sent to the model must start with the system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "    # construct the actual prompt to be executed, incorporating the original query and the retrieved content\n",
    "    prompt_context = rag_response.content\n",
    "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{prompt_context}\\n\\nQUERY:\\n{prompt}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "    # use Llama Stack inference API to directly communicate with the desired model\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        stream=stream,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "if stream:\n",
    "    for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "    print()  # nova linha ap√≥s streaming\n",
    "else:\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Step 5: Why Multi-Field RAG is Better\n",
    "\n",
    "**What we're learning:** Understanding when and why multi-field RAG outperforms single-field RAG.\n",
    "\n",
    "**Why this matters:** Knowing the strengths of multi-field RAG helps you decide when to use it in production systems.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Queries: Multi-Field vs Single-Field RAG\n",
    "\n",
    "Using multiple fields (`short_description`, `content`, and `close_notes`) instead of just `short_description` significantly improves retrieval quality for certain types of queries. Here are examples where multi-field RAG outperforms single-field RAG:\n",
    "\n",
    "**Example 1: Troubleshooting Steps and Solutions**\n",
    "**Query**: \"How do I fix ZTrend crashes when saving files?\"\n",
    "\n",
    "- **Single-field (short_description only)**: May retrieve tickets about crashes, but won't have the solution steps\n",
    "- **Multi-field**: Retrieves tickets with both the problem description AND the detailed troubleshooting steps from `close_notes`, providing complete answers\n",
    "\n",
    "**Example 2: Historical Context and Resolution**\n",
    "**Query**: \"What was the root cause and resolution for application crashes related to memory issues?\"\n",
    "\n",
    "- **Single-field**: Only finds tickets mentioning \"crashes\" but misses the diagnostic details and resolution steps\n",
    "- **Multi-field**: Retrieves tickets with full context from `content` (initial problem description) and `close_notes` (diagnostic findings and resolution), enabling comprehensive answers\n",
    "\n",
    "**Example 3: Pattern Recognition Across Problem-Solution Pairs**\n",
    "**Query**: \"What are common solutions for software crashes that involve configuration files?\"\n",
    "\n",
    "- **Single-field**: Can identify crash-related tickets but can't see the solutions\n",
    "- **Multi-field**: Can match both problem patterns (from `short_description`/`content`) and solution patterns (from `close_notes`), enabling identification of recurring problem-solution patterns\n",
    "\n",
    "**Example 4: Detailed Technical Information**\n",
    "**Query**: \"Show me tickets where log file analysis revealed the issue\"\n",
    "\n",
    "- **Single-field**: May miss tickets where log analysis is only mentioned in `content` or `close_notes`\n",
    "- **Multi-field**: Captures technical details from all fields, ensuring comprehensive retrieval of relevant tickets\n",
    "\n",
    "**Example 5: End-to-End Ticket Understanding**\n",
    "**Query**: \"Find tickets where the customer reported a problem, diagnostics were performed, and the issue was resolved by reinstalling software\"\n",
    "\n",
    "- **Single-field**: Can't capture the full narrative flow from problem ‚Üí diagnosis ‚Üí solution\n",
    "- **Multi-field**: Preserves the complete ticket lifecycle, enabling retrieval based on complex multi-stage scenarios\n",
    "\n",
    "**Key Insight**: Multi-field RAG is especially powerful for queries that require understanding both the problem AND the solution, or queries that need to match patterns across different stages of the ticket lifecycle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "This notebook demonstrated how to set up and use the built-in RAG tool for ingesting user-provided documents in a vector database and utilizing them during inference via direct retrieval. \n",
    "\n",
    "Key points:\n",
    "- **Multi-field content**: We combined `short_description`, `content`, and `close_notes` fields to create richer document representations, improving the quality of retrieval and context understanding.\n",
    "- **Metadata preservation**: Other fields from the dataset are stored as metadata, allowing for filtering and additional context during retrieval.\n",
    "- **Vector database integration**: The documents are chunked and indexed into ChromaDB using Llama Stack's RAG tool, enabling semantic search over the ticket data.\n",
    "- **Query advantages**: As shown in Section 4, multi-field RAG excels at queries requiring both problem and solution context, pattern recognition across ticket lifecycle stages, and comprehensive technical information retrieval.\n",
    "\n",
    "Now that we've seen how easy it is to implement RAG with Llama Stack, We'll move on to building a simple agent with Llama Stack next in our [Simple Agents](./Level2_simple_agent_with_websearch.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
