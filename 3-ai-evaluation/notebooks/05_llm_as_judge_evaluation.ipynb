{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: LLM-as-a-Judge Evaluation\n",
    "\n",
    "## ðŸŽ¯ What is This Notebook About?\n",
    "\n",
    "This notebook evaluates close notes quality using **LLM-as-a-Judge** - an automated evaluation method that uses a Large Language Model (LLM) to assess close notes based on structured criteria.\n",
    "\n",
    "**Context:**\n",
    "1. We have **two datasets:**\n",
    "   - **Reference Dataset** (good close notes) - High-quality examples\n",
    "   - **Other Incidents Dataset** (bad/regular close notes) - Standard examples\n",
    "   \n",
    "2. We want to **evaluate close notes** using multiple quality criteria:\n",
    "   - Does it provide useful information?\n",
    "   - Is it specific and detailed?\n",
    "   - Is it complete?\n",
    "   - Does it avoid generic phrases?\n",
    "   - Is it clear and well-written?\n",
    "\n",
    "**This notebook's purpose:**\n",
    "- **Set up evaluation criteria** - Define what makes a good close note\n",
    "- **Evaluate close notes** - Use LLM to judge quality across multiple dimensions\n",
    "- **Compare results** - See how good vs bad close notes score differently\n",
    "- **Understand scoring** - Learn what the scores mean and how to interpret them\n",
    "\n",
    "**What we'll learn:**\n",
    "- LLM-as-a-Judge provides structured, explainable evaluation\n",
    "- Good close notes score higher across all criteria\n",
    "- Bad close notes score lower, especially on specificity and completeness\n",
    "- This evaluation method can be used to assess AI-generated close notes\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Key Concepts Explained\n",
    "\n",
    "### What is LLM-as-a-Judge?\n",
    "\n",
    "**LLM-as-a-Judge** is a method where we use a Large Language Model (like Llama) to evaluate text quality, similar to how a human judge would evaluate it.\n",
    "\n",
    "**Think of it like this:**\n",
    "- **Human judge:** Reads a close note and gives it a score based on criteria\n",
    "- **LLM judge:** Does the same thing, but uses AI to be consistent and scalable\n",
    "\n",
    "**How it works:**\n",
    "1. We define **evaluation criteria** (what to look for)\n",
    "2. We provide the **close note** and **incident context** to the LLM\n",
    "3. The LLM **assesses** the close note against each criterion\n",
    "4. The LLM **selects an option** (e.g., \"Excellent\", \"Acceptable\", \"Bad\")\n",
    "5. We get a **score** (0.0 to 1.0) and **reasoning** (why that score was given)\n",
    "\n",
    "**Why this matters:**\n",
    "- Provides **consistent evaluation** (same criteria applied to all notes)\n",
    "- Gives **explainable scores** (we know why a score was given)\n",
    "- Can **scale** to evaluate many close notes automatically\n",
    "- Helps **identify** what makes a close note good or bad\n",
    "\n",
    "### What are Evaluation Criteria?\n",
    "\n",
    "**Evaluation criteria** are specific questions we ask about a close note's quality.\n",
    "\n",
    "**Our 5 criteria:**\n",
    "1. **Informativeness** - Does it provide useful information?\n",
    "2. **Specificity** - Does it include specific details?\n",
    "3. **Completeness** - Does it cover all key aspects?\n",
    "4. **No Generic Statements** - Does it avoid generic phrases?\n",
    "5. **Clarity** - Is it well-written and clear?\n",
    "\n",
    "**Each criterion has options:**\n",
    "- **Excellent** (score: 1.0) - Meets the criterion perfectly\n",
    "- **Acceptable** (score: 0.75) - Good but could be better\n",
    "- **Could be Improved** (score: 0.4-0.6) - Needs improvement\n",
    "- **Bad** (score: 0.0-0.2) - Doesn't meet the criterion\n",
    "\n",
    "**Why multiple criteria?**\n",
    "- One score isn't enough - we need to understand **what** makes a close note good\n",
    "- Different close notes may be strong in different areas\n",
    "- Helps identify **specific improvements** needed\n",
    "\n",
    "### How Does Scoring Work?\n",
    "\n",
    "**Scoring process:**\n",
    "1. LLM reads the close note and incident context\n",
    "2. For each criterion, LLM evaluates the close note\n",
    "3. LLM selects an option (e.g., \"Excellent\")\n",
    "4. Option is converted to a numeric score (e.g., 1.0)\n",
    "5. We get scores for all 5 criteria\n",
    "6. We calculate an **average score** across all criteria\n",
    "\n",
    "**Score interpretation:**\n",
    "- **0.8 - 1.0** = Excellent close note (high quality)\n",
    "- **0.6 - 0.8** = Good close note (acceptable quality)\n",
    "- **0.4 - 0.6** = Needs improvement\n",
    "- **0.0 - 0.4** = Poor close note (low quality)\n",
    "\n",
    "**Example:**\n",
    "- Close note scores: Informativeness=1.0, Specificity=0.8, Completeness=1.0, No Generic=1.0, Clarity=0.9\n",
    "- Average: **0.94** = Excellent quality close note!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- âœ… Understand what LLM-as-a-Judge evaluation is and how it works\n",
    "- âœ… Set up evaluation criteria for close note quality assessment\n",
    "- âœ… Use LLM to evaluate close notes across multiple quality dimensions\n",
    "- âœ… Compare evaluation results between good and bad close notes\n",
    "- âœ… Interpret scores and understand what makes a close note high-quality\n",
    "- âœ… Apply this evaluation method to assess AI-generated close notes\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Prerequisites\n",
    "\n",
    "Before starting this notebook, make sure you have:\n",
    "- [ ] Completed Notebook 04: Embeddings and Semantics Analysis\n",
    "- [ ] Have `data/reference_close_notes.csv` and `data/other_incidents.csv` files ready\n",
    "- [ ] Python environment with unitxt, pandas, numpy, matplotlib, seaborn installed\n",
    "- [ ] Access to LLM endpoint (Ollama or vLLM) for evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Step-by-Step Guide\n",
    "\n",
    "**Datasets:**\n",
    "- **Reference Dataset** (`reference_close_notes.csv`) - Good close notes (ground truth)\n",
    "- **Other Incidents Dataset** (`other_incidents.csv`) - Bad/regular close notes\n",
    "\n",
    "**What we'll evaluate:**\n",
    "- Close notes from both datasets\n",
    "- Using 5 evaluation criteria\n",
    "- With incident context (`content` field) for better evaluation\n",
    "- Get scores and reasoning for each evaluation\n",
    "\n",
    "**Expected results:**\n",
    "- Reference close notes should score **higher** (0.7-1.0 average)\n",
    "- Other incidents should score **lower** (0.3-0.6 average)\n",
    "- Differences should be most obvious in **Specificity** and **No Generic Statements**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Getting Started\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our evaluation environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries and Setup\n",
    "\n",
    "**What we're doing:** Importing the libraries we need for LLM-as-a-Judge evaluation.\n",
    "\n",
    "**Libraries:**\n",
    "- `pandas` - For working with datasets\n",
    "- `unitxt` - For LLM-as-a-Judge evaluation framework\n",
    "- `matplotlib` and `seaborn` - For visualizations\n",
    "- `numpy` - For numerical operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Unitxt imports for LLM-as-a-Judge\n",
    "from unitxt.api import create_dataset, evaluate\n",
    "from unitxt.inference import CrossProviderInferenceEngine\n",
    "from unitxt.llm_as_judge import LLMJudgeDirect\n",
    "from unitxt.llm_as_judge_constants import CriteriaWithOptions\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ðŸ“Š Working directory: {Path.cwd()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Datasets\n",
    "\n",
    "**What we're doing:** Loading the reference dataset (good close notes) and other incidents dataset (bad/regular close notes) that we created in Notebook 02.\n",
    "\n",
    "**Files:**\n",
    "- `data/reference_close_notes.csv` - High-quality close notes (ground truth)\n",
    "- `data/other_incidents.csv` - Standard close notes (for comparison)\n",
    "\n",
    "**Key fields:**\n",
    "- `content` - Incident description (context for evaluation)\n",
    "- `close_notes_ref` or `close_notes` - The close note to evaluate\n",
    "- `category` - Incident category (for filtering/grouping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "data_dir = Path(\"../data\")\n",
    "\n",
    "reference_df = pd.read_csv(data_dir / \"reference_close_notes.csv\")\n",
    "other_incidents_df = pd.read_csv(data_dir / \"other_incidents.csv\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASETS LOADED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“Š Reference Dataset (Good Close Notes):\")\n",
    "print(f\"   - Total records: {len(reference_df)}\")\n",
    "print(f\"   - Columns: {list(reference_df.columns)}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Other Incidents Dataset (Bad/Regular Close Notes):\")\n",
    "print(f\"   - Total records: {len(other_incidents_df)}\")\n",
    "print(f\"   - Columns: {list(other_incidents_df.columns)}\")\n",
    "\n",
    "# Check for required fields\n",
    "print(f\"\\nðŸ” Checking required fields...\")\n",
    "required_ref_fields = ['content', 'close_notes_ref']\n",
    "required_other_fields = ['content', 'close_notes']\n",
    "\n",
    "missing_ref = [f for f in required_ref_fields if f not in reference_df.columns]\n",
    "missing_other = [f for f in required_other_fields if f not in other_incidents_df.columns]\n",
    "\n",
    "if missing_ref:\n",
    "    print(f\"   âš ï¸  Missing in reference dataset: {missing_ref}\")\n",
    "else:\n",
    "    print(f\"   âœ… Reference dataset has all required fields\")\n",
    "\n",
    "if missing_other:\n",
    "    print(f\"   âš ï¸  Missing in other incidents dataset: {missing_other}\")\n",
    "else:\n",
    "    print(f\"   âœ… Other incidents dataset has all required fields\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare Sample Data for Evaluation\n",
    "\n",
    "**What we're doing:** Selecting a sample of close notes from both datasets to evaluate. We'll start with a small sample to test the evaluation, then can expand.\n",
    "\n",
    "**Why sample?**\n",
    "- LLM evaluation takes time and resources\n",
    "- Starting small helps us verify everything works\n",
    "- We can evaluate more later if needed\n",
    "\n",
    "**Selection strategy:**\n",
    "- Take a diverse sample (different categories)\n",
    "- Ensure we have incident context (`content` field)\n",
    "- Filter out empty or very short close notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for Jupyter notebook event loop issue\n",
    "# Jupyter notebooks already run an event loop, so we need nest_asyncio\n",
    "# to allow nested event loops (required by Unitxt's async inference)\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"âœ… Event loop fix applied (nest_asyncio enabled)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sample data for evaluation\n",
    "# Start with a small sample (e.g., 5-10 from each dataset) for testing\n",
    "SAMPLE_SIZE = 5\n",
    "\n",
    "# Filter reference dataset: ensure we have content and close_notes_ref\n",
    "reference_sample = reference_df[\n",
    "    (reference_df['content'].notna()) & \n",
    "    (reference_df['content'].astype(str).str.strip() != '') &\n",
    "    (reference_df['close_notes_ref'].notna()) & \n",
    "    (reference_df['close_notes_ref'].astype(str).str.strip() != '') &\n",
    "    (reference_df['close_notes_ref'].astype(str).str.len() > 20)  # Minimum length\n",
    "].head(SAMPLE_SIZE).copy()\n",
    "\n",
    "# Filter other incidents dataset: ensure we have content and close_notes\n",
    "other_sample = other_incidents_df[\n",
    "    (other_incidents_df['content'].notna()) & \n",
    "    (other_incidents_df['content'].astype(str).str.strip() != '') &\n",
    "    (other_incidents_df['close_notes'].notna()) & \n",
    "    (other_incidents_df['close_notes'].astype(str).str.strip() != '') &\n",
    "    (other_incidents_df['close_notes'].astype(str).str.len() > 10)  # Minimum length\n",
    "].head(SAMPLE_SIZE).copy()\n",
    "\n",
    "print(f\"Sample: {len(reference_sample)} reference + {len(other_sample)} other\")\n",
    "\n",
    "# Show example\n",
    "print(f\"\\nðŸ“ Example Reference Close Note:\")\n",
    "if len(reference_sample) > 0:\n",
    "    example_idx = 0\n",
    "    print(f\"   Content (first 150 chars): {reference_sample.iloc[example_idx]['content'][:150]}...\")\n",
    "    print(f\"   Close Note: {reference_sample.iloc[example_idx]['close_notes_ref'][:200]}...\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define Evaluation Criteria\n",
    "\n",
    "**What we're doing:** Defining the 5 evaluation criteria that will be used to judge close note quality.\n",
    "\n",
    "**Each criterion:**\n",
    "- Has a **name** and **description** (what we're looking for)\n",
    "- Has **options** (e.g., \"Excellent\", \"Acceptable\", \"Bad\")\n",
    "- Has an **option_map** (converts options to numeric scores 0.0-1.0)\n",
    "\n",
    "**Why these criteria?**\n",
    "- They cover the key aspects of a good close note\n",
    "- They're specific enough to be evaluated consistently\n",
    "- They help identify what makes a close note good or bad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 5 evaluation criteria for close notes quality\n",
    "\n",
    "informativeness = CriteriaWithOptions.from_obj({\n",
    "    \"name\": \"Informativeness\",\n",
    "    \"description\": \"Does the close note provide useful, specific information about what happened and how it was resolved?\",\n",
    "    \"options\": [\n",
    "        {\"name\": \"Excellent\", \"description\": \"Highly informative with specific details about problem, cause, and resolution.\"},\n",
    "        {\"name\": \"Acceptable\", \"description\": \"Provides useful information but could include more specific details.\"},\n",
    "        {\"name\": \"Could be Improved\", \"description\": \"Some information present but vague or incomplete.\"},\n",
    "        {\"name\": \"Bad\", \"description\": \"Little or no useful information (e.g., just 'Issue resolved').\"},\n",
    "    ],\n",
    "    \"option_map\": {\"Excellent\": 1.0, \"Acceptable\": 0.75, \"Could be Improved\": 0.4, \"Bad\": 0.0},\n",
    "})\n",
    "\n",
    "specificity = CriteriaWithOptions.from_obj({\n",
    "    \"name\": \"Specificity\",\n",
    "    \"description\": \"Does the close note include specific details such as error messages, specific actions taken, browser versions, or exact resolutions?\",\n",
    "    \"options\": [\n",
    "        {\"name\": \"Highly Specific\", \"description\": \"Includes concrete details like error codes, specific steps taken, browser versions, or exact outcomes.\"},\n",
    "        {\"name\": \"Somewhat Specific\", \"description\": \"Includes some details but could be more precise.\"},\n",
    "        {\"name\": \"Vague\", \"description\": \"Lacks specific details; too general.\"},\n",
    "    ],\n",
    "    \"option_map\": {\"Highly Specific\": 1.0, \"Somewhat Specific\": 0.6, \"Vague\": 0.2},\n",
    "})\n",
    "\n",
    "completeness = CriteriaWithOptions.from_obj({\n",
    "    \"name\": \"Completeness\",\n",
    "    \"description\": \"Does the close note cover the key aspects: what the problem was, what was done to resolve it, and the outcome?\",\n",
    "    \"options\": [\n",
    "        {\"name\": \"Complete\", \"description\": \"Covers problem, actions taken, and outcome clearly.\"},\n",
    "        {\"name\": \"Partially Complete\", \"description\": \"Covers some aspects but missing important details.\"},\n",
    "        {\"name\": \"Incomplete\", \"description\": \"Significant gaps in information; missing key aspects.\"},\n",
    "    ],\n",
    "    \"option_map\": {\"Complete\": 1.0, \"Partially Complete\": 0.5, \"Incomplete\": 0.0},\n",
    "})\n",
    "\n",
    "no_generic_statements = CriteriaWithOptions.from_obj({\n",
    "    \"name\": \"No Generic Statements\",\n",
    "    \"description\": \"Does the close note avoid generic, unhelpful phrases like 'Issue resolved', 'No changes noted', or 'Resolved per user' without explanation?\",\n",
    "    \"options\": [\n",
    "        {\"name\": \"No Generic Phrases\", \"description\": \"No generic statements; all content is specific and informative.\"},\n",
    "        {\"name\": \"Few Generic Phrases\", \"description\": \"Mostly specific but includes some generic statements.\"},\n",
    "        {\"name\": \"Too Generic\", \"description\": \"Primarily or entirely generic statements without explanation.\"},\n",
    "    ],\n",
    "    \"option_map\": {\"No Generic Phrases\": 1.0, \"Few Generic Phrases\": 0.4, \"Too Generic\": 0.0},\n",
    "})\n",
    "\n",
    "clarity = CriteriaWithOptions.from_obj({\n",
    "    \"name\": \"Clarity\",\n",
    "    \"description\": \"Is the close note well-written, clear, and easy to understand?\",\n",
    "    \"options\": [\n",
    "        {\"name\": \"Clear\", \"description\": \"Well-structured, easy to follow, and professional.\"},\n",
    "        {\"name\": \"Somewhat Clear\", \"description\": \"Understandable but could be better organized or more concise.\"},\n",
    "        {\"name\": \"Unclear\", \"description\": \"Difficult to understand or poorly structured.\"},\n",
    "    ],\n",
    "    \"option_map\": {\"Clear\": 1.0, \"Somewhat Clear\": 0.6, \"Unclear\": 0.0},\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATION CRITERIA DEFINED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nâœ… Created {5} evaluation criteria:\")\n",
    "print(\"   1. Informativeness - Does it provide useful information?\")\n",
    "print(\"   2. Specificity - Does it include specific details?\")\n",
    "print(\"   3. Completeness - Does it cover all key aspects?\")\n",
    "print(\"   4. No Generic Statements - Does it avoid generic phrases?\")\n",
    "print(\"   5. Clarity - Is it well-written and clear?\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Configure LLM-as-a-Judge\n",
    "\n",
    "**What we're doing:** Setting up the LLM judge using vLLM (OpenAI-compatible endpoint) and Unitxt framework.\n",
    "\n",
    "**Configuration:**\n",
    "- **Model:** Meta-Llama-3.1-8B-Instruct-FP8-dynamic (via vLLM)\n",
    "- **Provider:** `open-ai` (vLLM OpenAI-compatible endpoint)\n",
    "- **Endpoint:** Red Hat AI Infrastructure vLLM server\n",
    "- **Context:** We'll pass the incident description (`content`) as context\n",
    "- **Metrics:** One metric per criterion (5 metrics total)\n",
    "\n",
    "**Important:** Make sure environment variables are set before running this notebook:\n",
    "- `VLLM_API_BASE` - vLLM endpoint URL (default: http://rhaiis.bgskk.sandbox5288.opentlc.com:8000/v1)\n",
    "- `OPENAI_MODEL` - Model name (default: vllm-inference/llama-32-3b-instruct)\n",
    "- `OPENAI_API_KEY` - API key (default: dummy-key)\n",
    "\n",
    "**Note:** The provider name is `open-ai` (with hyphen) as required by Unitxt. Configuration is passed via `provider_specific_args` parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics for each criterion\n",
    "# Each criterion gets its own LLMJudgeDirect metric instance\n",
    "# Using vLLM endpoint (OpenAI-compatible) via OpenAI provider\n",
    "\n",
    "# Get vLLM configuration from shared config (with fallback to environment variables)\n",
    "vllm_api_base = VLLM_API_BASE or os.getenv(\"VLLM_API_BASE\", \"\")\n",
    "openai_model_raw = OPENAI_MODEL or os.getenv(\"OPENAI_MODEL\", \"vllm-inference/llama-32-3b-instruct\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"dummy-key\")\n",
    "\n",
    "# Validate configuration\n",
    "if not vllm_api_base:\n",
    "    raise ValueError(\n",
    "        \"VLLM_API_BASE is not configured!\\n\"\n",
    "        \"Please run: ./scripts/setup-env.sh\\n\"\n",
    "        \"Or set VLLM_API_BASE environment variable:\\n\"\n",
    "        \"  export VLLM_API_BASE='https://model-predictor-route-my-first-model.apps.ocp.example.com/v1'\\n\"\n",
    "        \"Or if inside cluster:\\n\"\n",
    "        \"  export VLLM_API_BASE='http://model-predictor.my-first-model.svc.cluster.local:8080/v1'\"\n",
    "    )\n",
    "\n",
    "print(f\"ðŸ“¡ vLLM API Base: {vllm_api_base}\")\n",
    "print(f\"ðŸ¤– OpenAI Model: {openai_model_raw}\")\n",
    "\n",
    "# LiteLLM requires provider prefix for custom models: openai/<model-name>\n",
    "openai_model = f\"openai/{openai_model_raw}\"  # Add provider prefix for LiteLLM\n",
    "\n",
    "# Set environment variables for OpenAI client (Unitxt will use these)\n",
    "os.environ[\"OPENAI_API_BASE\"] = vllm_api_base\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# Configure provider-specific arguments for OpenAI provider\n",
    "# Note: provider must be 'open-ai' (with hyphen) as per Unitxt specification\n",
    "# LiteLLMInferenceEngine (used internally) expects 'credentials' dict, not 'api_base'/'api_key'\n",
    "# The base URL is read from OPENAI_API_BASE environment variable (set above)\n",
    "provider_specific_args = {\n",
    "    \"open-ai\": {\n",
    "        \"credentials\": {\n",
    "            \"api_key\": openai_api_key,\n",
    "        },\n",
    "        \"extra_headers\": {},\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics = [\n",
    "    LLMJudgeDirect(\n",
    "        inference_engine=CrossProviderInferenceEngine(\n",
    "            model=openai_model,  # vLLM model name\n",
    "            max_tokens=1024,\n",
    "            data_classification_policy=[\"private\"],\n",
    "            provider=\"open-ai\",  # Note: 'open-ai' with hyphen, not 'openai'\n",
    "            provider_specific_args=provider_specific_args,\n",
    "        ),\n",
    "        criteria=informativeness,\n",
    "        context_fields=[\"question\"],  # Will contain incident context\n",
    "        criteria_field=\"criteria\",\n",
    "    ),\n",
    "    LLMJudgeDirect(\n",
    "        inference_engine=CrossProviderInferenceEngine(\n",
    "            model=openai_model,\n",
    "            max_tokens=1024,\n",
    "            data_classification_policy=[\"private\"],\n",
    "            provider=\"open-ai\",\n",
    "            provider_specific_args=provider_specific_args,\n",
    "        ),\n",
    "        criteria=specificity,\n",
    "        context_fields=[\"question\"],\n",
    "        criteria_field=\"criteria\",\n",
    "    ),\n",
    "    LLMJudgeDirect(\n",
    "        inference_engine=CrossProviderInferenceEngine(\n",
    "            model=openai_model,\n",
    "            max_tokens=1024,\n",
    "            data_classification_policy=[\"private\"],\n",
    "            provider=\"open-ai\",\n",
    "            provider_specific_args=provider_specific_args,\n",
    "        ),\n",
    "        criteria=completeness,\n",
    "        context_fields=[\"question\"],\n",
    "        criteria_field=\"criteria\",\n",
    "    ),\n",
    "    LLMJudgeDirect(\n",
    "        inference_engine=CrossProviderInferenceEngine(\n",
    "            model=openai_model,\n",
    "            max_tokens=1024,\n",
    "            data_classification_policy=[\"private\"],\n",
    "            provider=\"open-ai\",\n",
    "            provider_specific_args=provider_specific_args,\n",
    "        ),\n",
    "        criteria=no_generic_statements,\n",
    "        context_fields=[\"question\"],\n",
    "        criteria_field=\"criteria\",\n",
    "    ),\n",
    "    LLMJudgeDirect(\n",
    "        inference_engine=CrossProviderInferenceEngine(\n",
    "            model=openai_model,\n",
    "            max_tokens=1024,\n",
    "            data_classification_policy=[\"private\"],\n",
    "            provider=\"open-ai\",\n",
    "            provider_specific_args=provider_specific_args,\n",
    "        ),\n",
    "        criteria=clarity,\n",
    "        context_fields=[\"question\"],\n",
    "        criteria_field=\"criteria\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"âœ… Configured {len(metrics)} evaluation metrics using {openai_model_raw}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Prepare Data for Evaluation\n",
    "\n",
    "**What we're doing:** Formatting the close notes and incident context into the format expected by Unitxt.\n",
    "\n",
    "**Format:**\n",
    "- Each item needs a `question` field containing:\n",
    "  - Instruction to write a close note\n",
    "  - Full incident context (from `content` field)\n",
    "  - This gives the LLM judge all the information it needs\n",
    "\n",
    "**We'll prepare:**\n",
    "- Reference dataset close notes (good examples)\n",
    "- Other incidents close notes (bad/regular examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for evaluation\n",
    "# Format: question field contains instruction + incident context\n",
    "\n",
    "def prepare_evaluation_data(df, close_notes_col='close_notes_ref', dataset_type='reference'):\n",
    "    \"\"\"Prepare data in format expected by Unitxt.\"\"\"\n",
    "    data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        content = str(row['content']) if pd.notna(row['content']) else \"\"\n",
    "        close_note = str(row[close_notes_col]) if pd.notna(row[close_notes_col]) else \"\"\n",
    "        \n",
    "        # Create question with incident context\n",
    "        question = f\"\"\"Write a close note for this incident:\n",
    "\n",
    "{content}\n",
    "\n",
    "The close note being evaluated is:\n",
    "{close_note}\"\"\"\n",
    "        \n",
    "        data.append({\n",
    "            \"question\": question,\n",
    "            \"dataset_type\": dataset_type,\n",
    "            \"incident_id\": row.get('number', f\"INC-{idx}\"),\n",
    "            \"category\": row.get('category', 'Unknown'),\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Prepare reference dataset\n",
    "reference_data = prepare_evaluation_data(\n",
    "    reference_sample, \n",
    "    close_notes_col='close_notes_ref',\n",
    "    dataset_type='reference'\n",
    ")\n",
    "\n",
    "# Prepare other incidents dataset\n",
    "other_data = prepare_evaluation_data(\n",
    "    other_sample,\n",
    "    close_notes_col='close_notes',\n",
    "    dataset_type='other'\n",
    ")\n",
    "\n",
    "print(f\"Prepared {len(reference_data)} reference + {len(other_data)} other records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Evaluate Close Notes\n",
    "\n",
    "**What we're doing:** Running the LLM-as-a-Judge evaluation on our close notes.\n",
    "\n",
    "**Process:**\n",
    "1. Create a dataset with our close notes\n",
    "2. For each close note, the LLM evaluates it against all 5 criteria\n",
    "3. We get scores (0.0-1.0) and reasoning for each criterion\n",
    "4. Results are stored for analysis\n",
    "\n",
    "**Note:** This may take a few minutes as the LLM processes each close note for each criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data for evaluation\n",
    "all_data = reference_data + other_data\n",
    "\n",
    "# Create dataset\n",
    "print(\"Creating dataset...\")\n",
    "dataset = create_dataset(\n",
    "    task=\"tasks.qa.open\",\n",
    "    test_set=all_data,\n",
    "    metrics=metrics,\n",
    "    split=\"test\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dataset created with {len(dataset)} examples\")\n",
    "\n",
    "# Prepare predictions (the close notes to evaluate)\n",
    "# For each item in data, extract the close note from the question\n",
    "predictions = []\n",
    "for item in all_data:\n",
    "    # Extract close note from question (it's after \"The close note being evaluated is:\")\n",
    "    question_parts = item['question'].split(\"The close note being evaluated is:\")\n",
    "    if len(question_parts) > 1:\n",
    "        close_note = question_parts[1].strip()\n",
    "    else:\n",
    "        # Fallback: extract from original data\n",
    "        if item['dataset_type'] == 'reference':\n",
    "            idx = reference_data.index(item) if item in reference_data else 0\n",
    "            close_note = reference_sample.iloc[idx]['close_notes_ref']\n",
    "        else:\n",
    "            idx = other_data.index(item) if item in other_data else 0\n",
    "            close_note = other_sample.iloc[idx]['close_notes']\n",
    "    predictions.append(close_note)\n",
    "\n",
    "print(f\"âœ… Prepared {len(predictions)} predictions for evaluation\")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running evaluation...\")\n",
    "results = evaluate(predictions=predictions, data=dataset)\n",
    "print(\"âœ… Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Extract and Analyze Results\n",
    "\n",
    "**What we're doing:** Extracting scores from the evaluation results and organizing them for analysis.\n",
    "\n",
    "**We'll extract:**\n",
    "- Score for each criterion (0.0-1.0)\n",
    "- Selected option for each criterion (e.g., \"Excellent\", \"Acceptable\")\n",
    "- Reasoning for each evaluation (why that score was given)\n",
    "- Overall average score across all criteria\n",
    "\n",
    "**Then we'll:**\n",
    "- Compare scores between reference (good) and other (bad) close notes\n",
    "- Identify which criteria show the biggest differences\n",
    "- Visualize the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verify all 5 criteria are present in results\n",
    "try:\n",
    "    # Find criterion columns (exclude positional bias and average_score)\n",
    "    all_criterion_cols = [col for col in results_df.columns if col.endswith('_score') and col != 'average_score']\n",
    "    criterion_cols = [col for col in all_criterion_cols if 'Positional Bias' not in col]\n",
    "    \n",
    "    # Expected criteria (normalized to lowercase for comparison)\n",
    "    expected_criteria_normalized = ['informativeness', 'specificity', 'completeness', 'no generic statements', 'clarity']\n",
    "    expected_criteria_display = ['Informativeness', 'Specificity', 'Completeness', 'No Generic Statements', 'Clarity']\n",
    "    \n",
    "    # Normalize found criteria to lowercase for comparison\n",
    "    found_criteria_normalized = [col.replace('_score', '').replace('_', ' ').lower() for col in criterion_cols]\n",
    "    found_criteria_display = [col.replace('_score', '').replace('_', ' ') for col in criterion_cols]\n",
    "    \n",
    "    missing_normalized = set(expected_criteria_normalized) - set(found_criteria_normalized)\n",
    "    \n",
    "    if missing_normalized:\n",
    "        # Map back to display names\n",
    "        missing_display = [expected_criteria_display[expected_criteria_normalized.index(m)] for m in missing_normalized]\n",
    "        print(f\"âš ï¸  Missing criteria: {missing_display}\")\n",
    "    else:\n",
    "        print(f\"âœ… All 5 criteria found: {', '.join(sorted(found_criteria_display))}\")\n",
    "except NameError:\n",
    "    print(\"âš ï¸  Run the 'Extract Results' cell first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results and organize into a DataFrame\n",
    "results_list = []\n",
    "\n",
    "if hasattr(results, 'instance_scores') and isinstance(results.instance_scores, list):\n",
    "    for i, instance in enumerate(results.instance_scores):\n",
    "        if isinstance(instance, dict):\n",
    "            # Get metadata from original data\n",
    "            metadata = all_data[i]\n",
    "            \n",
    "            # Extract scores for each criterion\n",
    "            result_row = {\n",
    "                'dataset_type': metadata['dataset_type'],\n",
    "                'incident_id': metadata['incident_id'],\n",
    "                'category': metadata.get('category', 'Unknown'),\n",
    "            }\n",
    "            \n",
    "            # Find all criteria scores\n",
    "            score_keys = [k for k in instance.keys() if k.endswith('_selected_option')]\n",
    "            all_scores = []\n",
    "            \n",
    "            for score_key in score_keys:\n",
    "                base_name = score_key.replace('_selected_option', '')\n",
    "                score = instance.get(base_name, None)\n",
    "                selected_option = instance.get(f'{base_name}_selected_option', 'N/A')\n",
    "                \n",
    "                if score is not None:\n",
    "                    # Use base_name directly for column names (keeps underscores, consistent)\n",
    "                    # This ensures column names are consistent: informativeness_score, no_generic_statements_score, etc.\n",
    "                    # We'll format display names separately when needed for charts\n",
    "                    result_row[f'{base_name}_score'] = score\n",
    "                    result_row[f'{base_name}_option'] = selected_option\n",
    "                    all_scores.append(score)\n",
    "            \n",
    "            # Calculate average score\n",
    "            if all_scores:\n",
    "                result_row['average_score'] = sum(all_scores) / len(all_scores)\n",
    "            else:\n",
    "                result_row['average_score'] = None\n",
    "            \n",
    "            results_list.append(result_row)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "print(f\"Extracted {len(results_df)} results\")\n",
    "if len(results_df) > 0:\n",
    "    ref_count = len(results_df[results_df['dataset_type'] == 'reference'])\n",
    "    other_count = len(results_df[results_df['dataset_type'] == 'other'])\n",
    "    print(f\"  Reference: {ref_count}, Other: {other_count}\")\n",
    "    print(f\"  Avg scores - Reference: {results_df[results_df['dataset_type'] == 'reference']['average_score'].mean():.2f}, \"\n",
    "          f\"Other: {results_df[results_df['dataset_type'] == 'other']['average_score'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Visualize Results\n",
    "\n",
    "**What we're doing:** Creating visualizations to compare scores between good and bad close notes.\n",
    "\n",
    "**Charts we'll create:**\n",
    "1. **Average Score Comparison** - Box plot showing score distributions\n",
    "2. **Criterion-by-Criterion Comparison** - See which criteria show biggest differences\n",
    "3. **Score Distribution** - Histogram showing how scores are distributed\n",
    "\n",
    "**What to look for:**\n",
    "- Reference (good) close notes should score **higher** overall\n",
    "- Biggest differences likely in **Specificity** and **No Generic Statements**\n",
    "- Scores should cluster: good notes 0.7-1.0, bad notes 0.3-0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('LLM-as-a-Judge Evaluation Results: Good vs Bad Close Notes', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Average Score Comparison\n",
    "ax1 = axes[0, 0]\n",
    "results_df.boxplot(column='average_score', by='dataset_type', ax=ax1)\n",
    "ax1.set_title('Average Score Distribution')\n",
    "ax1.set_xlabel('Dataset Type')\n",
    "ax1.set_ylabel('Average Score (0.0 - 1.0)')\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Criterion Scores Comparison\n",
    "ax2 = axes[0, 1]\n",
    "# Get criterion columns, excluding average_score and positional bias columns\n",
    "all_criterion_cols = [col for col in results_df.columns if col.endswith('_score') and col != 'average_score']\n",
    "criterion_cols = [col for col in all_criterion_cols if 'Positional Bias' not in col]\n",
    "if criterion_cols:\n",
    "    comparison_data = []\n",
    "    for criterion in criterion_cols:\n",
    "        # Format criterion name for display (replace underscores with spaces)\n",
    "        criterion_name = criterion.replace('_score', '').replace('_', ' ')\n",
    "        for dataset_type in ['reference', 'other']:\n",
    "            subset = results_df[results_df['dataset_type'] == dataset_type]\n",
    "            if len(subset) > 0:\n",
    "                comparison_data.append({\n",
    "                    'Criterion': criterion_name,\n",
    "                    'Dataset': 'Good (Reference)' if dataset_type == 'reference' else 'Bad/Regular (Other)',\n",
    "                    'Score': subset[criterion].mean()\n",
    "                })\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_pivot = comparison_df.pivot(index='Criterion', columns='Dataset', values='Score')\n",
    "        comparison_pivot.plot(kind='bar', ax=ax2, color=['#2ecc71', '#e74c3c'])\n",
    "        ax2.set_title('Average Scores by Criterion')\n",
    "        ax2.set_xlabel('Criterion')\n",
    "        ax2.set_ylabel('Average Score')\n",
    "        ax2.legend(title='Dataset Type')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Score Distribution Histogram\n",
    "ax3 = axes[1, 0]\n",
    "reference_scores = results_df[results_df['dataset_type'] == 'reference']['average_score'].dropna()\n",
    "other_scores = results_df[results_df['dataset_type'] == 'other']['average_score'].dropna()\n",
    "\n",
    "if len(reference_scores) > 0 and len(other_scores) > 0:\n",
    "    ax3.hist(reference_scores, bins=10, alpha=0.6, label='Good (Reference)', color='#2ecc71')\n",
    "    ax3.hist(other_scores, bins=10, alpha=0.6, label='Bad/Regular (Other)', color='#e74c3c')\n",
    "    ax3.set_title('Score Distribution')\n",
    "    ax3.set_xlabel('Average Score')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Summary Statistics Table\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for dataset_type in ['reference', 'other']:\n",
    "    subset = results_df[results_df['dataset_type'] == dataset_type]\n",
    "    if len(subset) > 0 and 'average_score' in subset.columns:\n",
    "        summary_data.append({\n",
    "            'Dataset': 'Good (Reference)' if dataset_type == 'reference' else 'Bad/Regular (Other)',\n",
    "            'Count': len(subset),\n",
    "            'Mean': subset['average_score'].mean(),\n",
    "            'Std': subset['average_score'].std(),\n",
    "            'Min': subset['average_score'].min(),\n",
    "            'Max': subset['average_score'].max()\n",
    "        })\n",
    "\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    table = ax4.table(cellText=summary_df.values,\n",
    "                     colLabels=summary_df.columns,\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     bbox=[0, 0, 1, 1])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    ax4.set_title('Summary Statistics', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualizations created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9b. Additional Visualizations: Heatmap and Radar Chart\n",
    "\n",
    "**What we're doing:** Creating two additional charts that provide deeper insights into the evaluation results.\n",
    "\n",
    "**New charts:**\n",
    "1. **Heatmap** - Visual comparison of scores across all criteria (quick overview)\n",
    "2. **Radar Chart** - Shows the \"profile\" of good vs bad notes (strengths/weaknesses)\n",
    "\n",
    "**Why these charts help:**\n",
    "- **Heatmap:** See all scores at once - which criteria show the biggest gaps?\n",
    "- **Radar Chart:** Understand the \"shape\" of quality - are good notes strong across all criteria or just some?\n",
    "\n",
    "**Think of it like:** \n",
    "- Heatmap = A color-coded report card showing where good notes excel\n",
    "- Radar Chart = A \"spider web\" showing the quality profile of each dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional visualizations: Heatmap and Radar Chart\n",
    "# These charts provide deeper insights into the evaluation results\n",
    "\n",
    "# Extract criterion columns (exclude average_score)\n",
    "criterion_cols = [\n",
    "    col\n",
    "    for col in results_df.columns\n",
    "    if col.endswith(\"_score\") and col != \"average_score\"\n",
    "]\n",
    "criterion_names = [\n",
    "    col.replace(\"_score\", \"\").replace(\"_\", \" \").title() for col in criterion_cols\n",
    "]\n",
    "\n",
    "if len(criterion_cols) > 0:\n",
    "    # Create figure with 2 subplots\n",
    "    fig2, (ax5, ax6) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    fig2.suptitle(\n",
    "        \"Additional Insights: Heatmap and Criterion Profile\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.02,\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # CHART 1: HEATMAP - Visual comparison across all criteria\n",
    "    # ========================================================================\n",
    "    # Prepare data for heatmap\n",
    "    heatmap_data = []\n",
    "    for dataset_type in [\"reference\", \"other\"]:\n",
    "        subset = results_df[results_df[\"dataset_type\"] == dataset_type]\n",
    "        if len(subset) > 0:\n",
    "            row_data = []\n",
    "            for criterion in criterion_cols:\n",
    "                row_data.append(subset[criterion].mean())\n",
    "            heatmap_data.append(row_data)\n",
    "\n",
    "    if heatmap_data:\n",
    "        heatmap_df = pd.DataFrame(\n",
    "            heatmap_data,\n",
    "            index=[\"Good (Reference)\", \"Bad/Regular (Other)\"],\n",
    "            columns=criterion_names,\n",
    "        )\n",
    "\n",
    "        # Create heatmap with better colormap\n",
    "        im = ax5.imshow(heatmap_df.values, cmap=\"RdYlGn\", aspect=\"auto\", vmin=0, vmax=1)\n",
    "\n",
    "        # Set ticks and labels\n",
    "        ax5.set_xticks(np.arange(len(heatmap_df.columns)))\n",
    "        ax5.set_yticks(np.arange(len(heatmap_df.index)))\n",
    "        ax5.set_xticklabels(heatmap_df.columns, rotation=45, ha=\"right\", fontsize=11)\n",
    "        ax5.set_yticklabels(heatmap_df.index, fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "        # Add text annotations with better formatting\n",
    "        for i in range(len(heatmap_df.index)):\n",
    "            for j in range(len(heatmap_df.columns)):\n",
    "                score = heatmap_df.iloc[i, j]\n",
    "                # Use white text for low scores, black for high scores\n",
    "                text_color = \"white\" if score < 0.5 else \"black\"\n",
    "                ax5.text(\n",
    "                    j,\n",
    "                    i,\n",
    "                    f\"{score:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=text_color,\n",
    "                    fontsize=11,\n",
    "                    fontweight=\"bold\",\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.7),\n",
    "                )\n",
    "\n",
    "        ax5.set_title(\n",
    "            \"Score Heatmap Across All Criteria\\n(Darker Green = Higher Score)\",\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "            pad=15,\n",
    "        )\n",
    "\n",
    "        # Add colorbar with better styling\n",
    "        cbar = plt.colorbar(im, ax=ax5, fraction=0.046, pad=0.04)\n",
    "        cbar.set_label(\n",
    "            \"Score (0.0 = Poor, 1.0 = Excellent)\", fontsize=11, fontweight=\"bold\"\n",
    "        )\n",
    "        cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "    # ========================================================================\n",
    "    # CHART 2: RADAR CHART - Criterion Profile Comparison\n",
    "    # ========================================================================\n",
    "    if len(criterion_cols) >= 3:\n",
    "        try:\n",
    "            # Calculate average scores for each criterion\n",
    "            ref_means = [\n",
    "                results_df[results_df[\"dataset_type\"] == \"reference\"][col].mean()\n",
    "                for col in criterion_cols\n",
    "            ]\n",
    "            other_means = [\n",
    "                results_df[results_df[\"dataset_type\"] == \"other\"][col].mean()\n",
    "                for col in criterion_cols\n",
    "            ]\n",
    "\n",
    "            # Number of criteria\n",
    "            N = len(criterion_cols)\n",
    "\n",
    "            # Compute angle for each criterion\n",
    "            angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "            angles += angles[:1]  # Complete the circle\n",
    "\n",
    "            # Add values to complete the circle\n",
    "            ref_means += ref_means[:1]\n",
    "            other_means += other_means[:1]\n",
    "\n",
    "            # Create radar chart\n",
    "            ax6 = plt.subplot(1, 2, 2, projection=\"polar\")\n",
    "            ax6.plot(\n",
    "                angles,\n",
    "                ref_means,\n",
    "                \"o-\",\n",
    "                linewidth=3,\n",
    "                label=\"Good (Reference)\",\n",
    "                color=\"#2ecc71\",\n",
    "                markersize=8,\n",
    "            )\n",
    "            ax6.fill(angles, ref_means, alpha=0.25, color=\"#2ecc71\")\n",
    "            ax6.plot(\n",
    "                angles,\n",
    "                other_means,\n",
    "                \"o-\",\n",
    "                linewidth=3,\n",
    "                label=\"Bad/Regular (Other)\",\n",
    "                color=\"#e74c3c\",\n",
    "                markersize=8,\n",
    "            )\n",
    "            ax6.fill(angles, other_means, alpha=0.25, color=\"#e74c3c\")\n",
    "\n",
    "            # Add criterion labels with better positioning\n",
    "            ax6.set_xticks(angles[:-1])\n",
    "            ax6.set_xticklabels(criterion_names, fontsize=10)\n",
    "            ax6.set_ylim(0, 1)\n",
    "            ax6.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "            ax6.set_yticklabels([\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], fontsize=9)\n",
    "            ax6.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "            ax6.set_title(\n",
    "                \"Criterion Profile Comparison\\n(Radar Chart - Shows Strengths/Weaknesses)\",\n",
    "                fontsize=14,\n",
    "                fontweight=\"bold\",\n",
    "                pad=20,\n",
    "            )\n",
    "            ax6.legend(\n",
    "                loc=\"upper right\",\n",
    "                bbox_to_anchor=(1.3, 1.1),\n",
    "                fontsize=11,\n",
    "                framealpha=0.9,\n",
    "            )\n",
    "\n",
    "            # Add score annotations at each point\n",
    "            for angle, ref_val, other_val in zip(\n",
    "                angles[:-1], ref_means[:-1], other_means[:-1]\n",
    "            ):\n",
    "                ax6.text(\n",
    "                    angle,\n",
    "                    ref_val + 0.1,\n",
    "                    f\"{ref_val:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=8,\n",
    "                    color=\"#2ecc71\",\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "                ax6.text(\n",
    "                    angle,\n",
    "                    other_val - 0.1,\n",
    "                    f\"{other_val:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=8,\n",
    "                    color=\"#e74c3c\",\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "        except Exception as e:\n",
    "            ax6.text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                f\"Radar chart unavailable\\n({str(e)[:40]})\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax6.transAxes,\n",
    "                fontsize=11,\n",
    "            )\n",
    "            ax6.set_title(\n",
    "                \"Criterion Profile (Radar Chart)\", fontsize=14, fontweight=\"bold\"\n",
    "            )\n",
    "    else:\n",
    "        ax6.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Radar chart requires at least 3 criteria\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=ax6.transAxes,\n",
    "            fontsize=11,\n",
    "        )\n",
    "        ax6.set_title(\"Criterion Profile (Radar Chart)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nâœ… Additional visualizations created!\")\n",
    "    print(\"   ðŸ“Š Charts: Heatmap (all criteria at once), Radar Chart (quality profile)\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"âš ï¸  No criterion columns found for additional visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Detailed Results Per Close Note\n",
    "\n",
    "**What we're doing:** Showing detailed evaluation results for each close note, including scores and reasoning for each criterion.\n",
    "\n",
    "**This helps us:**\n",
    "- Understand why each close note scored the way it did\n",
    "- See which criteria are strengths/weaknesses for each note\n",
    "- Learn what makes a close note good or bad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary results for each close note\n",
    "if hasattr(results, 'instance_scores') and isinstance(results.instance_scores, list):\n",
    "    for i, instance in enumerate(results.instance_scores):\n",
    "        metadata = all_data[i]\n",
    "        \n",
    "        if isinstance(instance, dict):\n",
    "            # Extract main criteria scores (exclude positional bias)\n",
    "            score_keys = [k for k in instance.keys() if k.endswith('_selected_option') and 'Positional Bias' not in k]\n",
    "            all_scores = [instance.get(k.replace('_selected_option', ''), 0) for k in score_keys]\n",
    "            \n",
    "            if all_scores:\n",
    "                avg_score = sum(all_scores) / len(all_scores)\n",
    "                print(f\"Note {i+1} ({metadata['dataset_type']}): {avg_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Comparison and Interpretation\n",
    "\n",
    "**What we're doing:** Comparing results between good (reference) and bad (other) close notes to understand the differences.\n",
    "\n",
    "**Key questions:**\n",
    "- Do good close notes score higher? (Expected: Yes)\n",
    "- Which criteria show the biggest differences?\n",
    "- What can we learn about what makes a close note good?\n",
    "\n",
    "**Interpretation guide:**\n",
    "- **Large difference (0.3+)** = This criterion strongly distinguishes good from bad\n",
    "- **Small difference (<0.2)** = This criterion doesn't distinguish well\n",
    "- **Consistent pattern** = Good notes score higher across all criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results between reference and other datasets\n",
    "if len(results_df) > 0:\n",
    "    reference_scores = results_df[results_df['dataset_type'] == 'reference']\n",
    "    other_scores = results_df[results_df['dataset_type'] == 'other']\n",
    "    \n",
    "    if len(reference_scores) > 0 and len(other_scores) > 0:\n",
    "        ref_avg = reference_scores['average_score'].mean()\n",
    "        other_avg = other_scores['average_score'].mean()\n",
    "        print(f\"Comparison: Reference={ref_avg:.2f}, Other={other_avg:.2f}, Diff={ref_avg-other_avg:.2f}\")\n",
    "        \n",
    "        # Show criterion differences\n",
    "        all_criterion_cols = [col for col in results_df.columns if col.endswith('_score') and col != 'average_score']\n",
    "        criterion_cols = [col for col in all_criterion_cols if 'Positional Bias' not in col]\n",
    "        if criterion_cols:\n",
    "            print(\"\\nCriterion differences:\")\n",
    "            for criterion_col in criterion_cols:\n",
    "                criterion_name = criterion_col.replace('_score', '').replace('_', ' ')\n",
    "                diff = reference_scores[criterion_col].mean() - other_scores[criterion_col].mean()\n",
    "                print(f\"  {criterion_name}: {diff:+.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¼ How This Applies to IT Operations\n",
    "\n",
    "What you just built is a production-ready quality evaluation system for IT operations:\n",
    "\n",
    "**Real-World Applications:**\n",
    "- **Automated Quality Scoring:** Use LLM-as-a-Judge to automatically score all new close notes\n",
    "- **Agent Feedback:** Show agents exactly where their documentation needs improvement (specific criteria)\n",
    "- **Quality Monitoring:** Set up automated alerts for close notes that score below threshold\n",
    "- **Training Data:** Use evaluation results to train AI models to generate better close notes\n",
    "\n",
    "**The Pattern:**\n",
    "1. **Define criteria** - What makes good documentation? (You did this!)\n",
    "2. **Evaluate automatically** - Use LLM-as-a-Judge to score all close notes\n",
    "3. **Provide feedback** - Show agents specific areas for improvement\n",
    "4. **Measure improvement** - Track quality scores over time\n",
    "\n",
    "**Why this matters:** In IT operations, consistent documentation quality reduces MTTD. When agents write good close notes, future incidents are resolved faster. LLM-as-a-Judge provides automated, consistent, explainable quality assessment at scale.\n",
    "\n",
    "**Practical Use Cases:**\n",
    "- **Real-time Quality Checks:** Evaluate close notes as they're written, provide instant feedback\n",
    "- **Quality Dashboards:** Track documentation quality metrics across teams\n",
    "- **Agent Coaching:** Use evaluation results to coach agents on specific criteria\n",
    "- **AI Training:** Use high-scoring examples to train AI models\n",
    "- **Compliance:** Ensure documentation meets organizational standards automatically\n",
    "\n",
    "**Integration Points:**\n",
    "- **Ticketing Systems:** Integrate evaluation into ServiceNow, Jira, etc.\n",
    "- **Workflow Automation:** Auto-flag low-quality close notes for review\n",
    "- **Reporting:** Generate quality reports for management\n",
    "- **Continuous Improvement:** Use scores to identify training needs\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ Key Takeaways\n",
    "\n",
    "- **LLM-as-a-Judge works!** - It provides structured, explainable evaluation that actually distinguishes good from bad\n",
    "- **Multiple criteria matter** - One score isn't enough; you need to understand what makes documentation good\n",
    "- **Explainable scores** - Unlike simple metrics, you get reasoning for each score\n",
    "- **Scalable evaluation** - Can evaluate thousands of close notes automatically\n",
    "- **Better than n-grams** - Understands meaning and quality, not just word overlap\n",
    "- **Better than semantic similarity alone** - Provides structured evaluation across multiple dimensions\n",
    "\n",
    "**Key insights from this notebook:**\n",
    "- Good close notes score higher across ALL criteria (not just one)\n",
    "- Specificity and \"No Generic Statements\" are the strongest differentiators\n",
    "- LLM-as-a-Judge provides consistent, explainable evaluation\n",
    "- This method can evaluate both human-written and AI-generated close notes\n",
    "- Evaluation results can guide prompt engineering for better AI outputs\n",
    "\n",
    "**What you accomplished:**\n",
    "- âœ… Set up LLM-as-a-Judge evaluation framework\n",
    "- âœ… Evaluated close notes across 5 quality criteria\n",
    "- âœ… Compared good vs bad close notes systematically\n",
    "- âœ… Validated that structured evaluation works for quality assessment\n",
    "- âœ… Built a production-ready evaluation system\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Next Steps\n",
    "\n",
    "**Congratulations!** You've completed the evaluation workflow! ðŸŽ‰\n",
    "\n",
    "**What you've built:**\n",
    "- âœ… A complete evaluation pipeline: Ground Truth â†’ N-gram Baseline â†’ Semantic Analysis â†’ LLM-as-a-Judge\n",
    "- âœ… Multiple evaluation methods tested and validated\n",
    "- âœ… Production-ready quality assessment system\n",
    "\n",
    "**Potential Next Steps:**\n",
    "- **Notebook 06 (if created):** Generate close notes using LLM and evaluate them\n",
    "- **Apply to your data:** Use this workflow with your own incident data\n",
    "- **Build automation:** Integrate evaluation into your ticketing workflow\n",
    "- **Monitor quality:** Set up dashboards to track documentation quality over time\n",
    "\n",
    "**How to use this in production:**\n",
    "1. **Set up evaluation** - Use the same criteria and framework\n",
    "2. **Evaluate new close notes** - Run LLM-as-a-Judge on all new documentation\n",
    "3. **Provide feedback** - Show agents where they can improve\n",
    "4. **Track metrics** - Monitor quality scores over time\n",
    "5. **Iterate** - Refine criteria based on your organization's needs\n",
    "\n",
    "**Related concepts:**\n",
    "- LLM-as-a-Judge evaluation\n",
    "- Automated quality assessment\n",
    "- Production deployment\n",
    "- Quality monitoring and dashboards\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ Congratulations!** You've built a complete AI-powered quality evaluation system! You now have the tools to automatically assess and improve IT incident documentation quality. This directly reduces MTTD and improves operational efficiency.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
