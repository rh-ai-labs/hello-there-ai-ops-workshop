{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: LlamaStack Core Features\n",
    "\n",
    "## üéØ What is This Notebook About?\n",
    "\n",
    "Welcome to Notebook 03! This notebook explores **LlamaStack's core capabilities** - Chat and RAG (Retrieval Augmented Generation) - the building blocks that make powerful agents possible.\n",
    "\n",
    "**What we'll do:**\n",
    "- Understand Simple Chat - basic LLM interactions for Q&A and text generation\n",
    "- Explore RAG - giving agents access to your documentation and knowledge bases\n",
    "- Learn when to use each feature and how they work together\n",
    "- See practical examples for IT operations use cases\n",
    "\n",
    "**Why this matters:**\n",
    "- Chat is the foundation - everything else builds on this\n",
    "- RAG gives agents access to YOUR documentation (runbooks, procedures, internal docs)\n",
    "- Understanding these features helps you build better agents\n",
    "- This knowledge prepares you for advanced agent development (MCP tools, safety, evaluation)\n",
    "\n",
    "**The big picture:**\n",
    "- **Chat** = General Q&A using the LLM's training data\n",
    "- **RAG** = Domain-specific Q&A using YOUR documents\n",
    "- **Together** = Agents that can answer general questions AND questions about your specific infrastructure\n",
    "\n",
    "**Real-world impact:**\n",
    "- **Chat:** Answer general IT questions (\"What is a load balancer?\")\n",
    "- **RAG:** Answer questions about YOUR infrastructure (\"How do we restart services in our setup?\")\n",
    "- **Combined:** Agents that understand both general IT concepts and your specific procedures\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Key Concepts Explained\n",
    "\n",
    "### Simple Chat\n",
    "\n",
    "**Chat** is the most basic way to interact with an LLM - a conversation where you send messages and receive responses.\n",
    "\n",
    "**What it is:** Direct interaction with an LLM - send a message, get a response. That's it!\n",
    "\n",
    "**Why it matters:** Chat is the foundation. Everything else (RAG, tools, agents) builds on this basic capability.\n",
    "\n",
    "**Think of it like:** Talking to a knowledgeable colleague. You ask questions, they answer based on what they know.\n",
    "\n",
    "**How it works:**\n",
    "1. You send a message (user message)\n",
    "2. Optionally set context (system prompt - defines the assistant's role)\n",
    "3. LLM generates a response based on its training data\n",
    "4. You can continue the conversation (multi-turn) - the LLM remembers context\n",
    "\n",
    "**When to use:**\n",
    "- ‚úÖ General Q&A (\"What is a load balancer?\")\n",
    "- ‚úÖ Text generation (summaries, explanations)\n",
    "- ‚úÖ Basic reasoning tasks\n",
    "- ‚ùå Don't use when you need external knowledge or tools (use RAG or MCP instead)\n",
    "\n",
    "### RAG (Retrieval Augmented Generation)\n",
    "\n",
    "**RAG** enhances LLMs with external knowledge by retrieving relevant documents and using them as context.\n",
    "\n",
    "**What it is:** A technique that combines document retrieval with text generation. Store your docs in a vector store, search for relevant context, and use that context to generate better answers.\n",
    "\n",
    "**Why it matters:** LLMs have training data cutoff dates and can't access private/internal documents. RAG gives agents access to YOUR documentation - runbooks, procedures, internal knowledge bases.\n",
    "\n",
    "**Think of it like:** A librarian who can search your internal wiki and use those documents to answer questions. The LLM doesn't just rely on its training - it uses YOUR docs!\n",
    "\n",
    "**How it works:**\n",
    "1. **Store:** Documents are stored in a vector database (vector store) with embeddings\n",
    "2. **Search:** When you ask a question, the system searches for semantically similar documents\n",
    "3. **Retrieve:** Relevant documents are retrieved based on meaning (not just keywords)\n",
    "4. **Augment:** Retrieved documents are added as context to the LLM prompt\n",
    "5. **Generate:** The LLM generates an answer using both its training data AND your documents\n",
    "\n",
    "**When to use:**\n",
    "- ‚úÖ Need access to specific documents (runbooks, procedures)\n",
    "- ‚úÖ Domain-specific knowledge required (your infrastructure)\n",
    "- ‚úÖ Private/internal information (internal docs, configurations)\n",
    "- ‚úÖ Up-to-date information needed (current procedures, recent changes)\n",
    "\n",
    "### Vector Stores\n",
    "\n",
    "A **vector store** is a database that stores documents as embeddings (vector representations) for semantic search.\n",
    "\n",
    "**What it is:** A specialized database that stores documents and their embeddings, enabling semantic search (finding documents by meaning, not just keywords).\n",
    "\n",
    "**Why it matters:** Vector stores enable RAG - they're where your documents live and how the system finds relevant context.\n",
    "\n",
    "**Think of it like:** A smart filing cabinet. Instead of searching by filename or keywords, you search by meaning - \"find documents about restarting services\" finds relevant docs even if they don't contain those exact words.\n",
    "\n",
    "### System Prompts\n",
    "\n",
    "**System prompts** are instructions that guide the LLM's behavior - they set the \"personality\" and \"role\" of the assistant.\n",
    "\n",
    "**What they are:** Special messages that define how the assistant should behave, what role it plays, and what context it should consider.\n",
    "\n",
    "**Why they matter:** System prompts ensure consistent, domain-appropriate responses. They tell the LLM \"you are an IT operations assistant\" so it responds accordingly.\n",
    "\n",
    "**Think of it like:** A job description. The system prompt tells the LLM what job it's doing and how to do it.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand what Chat is and when to use it\n",
    "- ‚úÖ Understand what RAG is and how it works\n",
    "- ‚úÖ Know when to use Chat vs. RAG\n",
    "- ‚úÖ Create vector stores and add documents\n",
    "- ‚úÖ Use RAG to answer questions with your documentation\n",
    "- ‚úÖ See how Chat and RAG work together in agents\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Prerequisites\n",
    "\n",
    "Before starting this notebook, make sure you have:\n",
    "- ‚úÖ Completed Notebook 02: Building a Simple Agent (understanding of agents and tools)\n",
    "- ‚úÖ LlamaStack server running (see Module README)\n",
    "- ‚úÖ Python environment with dependencies installed (`llama-stack-client`)\n",
    "- ‚úÖ Basic understanding of how LLMs work (from Notebook 01)\n",
    "\n",
    "**The fun part:** No MCP servers needed yet! We're exploring LlamaStack's built-in features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Step-by-Step Guide\n",
    "\n",
    "### Step 1: Setup and Configuration\n",
    "\n",
    "**What we're doing:** Setting up the environment and connecting to LlamaStack.\n",
    "\n",
    "**Why:** We need to establish connections before we can explore Chat and RAG features.\n",
    "\n",
    "**What to expect:**\n",
    "- Import required libraries\n",
    "- Load configuration from shared config system\n",
    "- Connect to LlamaStack server\n",
    "- Verify everything is working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "# Add src directory to path for shared configuration\n",
    "root_dir = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(root_dir / \"src\"))\n",
    "\n",
    "# Import centralized configuration\n",
    "from config import LLAMA_STACK_URL, MODEL, CONFIG\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üì° LlamaStack URL: {LLAMA_STACK_URL}\")\n",
    "print(f\"ü§ñ Model: {MODEL}\")\n",
    "\n",
    "# Verify configuration\n",
    "if not LLAMA_STACK_URL:\n",
    "    raise ValueError(\n",
    "        \"LLAMA_STACK_URL is not configured!\\n\"\n",
    "        \"Please run: ./scripts/setup-env.sh\"\n",
    "    )\n",
    "\n",
    "# Initialize LlamaStack client\n",
    "client = LlamaStackClient(base_url=LLAMA_STACK_URL)\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    model_count = len(models.data) if hasattr(models, 'data') else len(models)\n",
    "    print(f\"\\n‚úÖ Connected to LlamaStack\")\n",
    "    print(f\"   Available models: {model_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Cannot connect to LlamaStack: {e}\")\n",
    "    raise\n",
    "\n",
    "# Use MODEL from config\n",
    "model = MODEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** After running the code, you should see successful connections to LlamaStack. The configuration is loaded from the shared `src/config.py` system, which auto-detects your environment.\n",
    "\n",
    "**Key takeaway:** The shared configuration system makes it easy to switch between environments (local, OpenShift, etc.) without changing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Step-by-Step Guide\n",
    "\n",
    "### Step 2: Basic Chat Completion\n",
    "\n",
    "**What we're doing:** Learning the basics of LLM interactions - sending messages and getting responses.\n",
    "\n",
    "**Why:** Chat is the foundation. Everything else builds on this. It's like learning to walk before you run!\n",
    "\n",
    "**What to expect:**\n",
    "- Send a simple question to the LLM\n",
    "- Receive a response\n",
    "- See how basic chat works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** After running the code, you should see successful connections to LlamaStack. The configuration is loaded from the shared `src/config.py` system, which auto-detects your environment.\n",
    "\n",
    "**Key takeaway:** The shared configuration system makes it easy to switch between environments (local, OpenShift, etc.) without changing code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic chat completion - simplest example\n",
    "print(\"=\" * 60)\n",
    "print(\"Basic Chat Completion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is artificial intelligence in one sentence?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract and display the response\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"\\nüìù Question: What is artificial intelligence in one sentence?\")\n",
    "print(f\"\\nü§ñ Answer:\\n{answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** After running the code, you sent a message to the LLM and received a response. This is the most basic form of interaction - just a question and an answer.\n",
    "\n",
    "**Key takeaway:** Chat is simple - send a message, get a response. The LLM uses its training data to generate the answer. This is the foundation that everything else builds on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Chat with System Prompt\n",
    "\n",
    "**What we're doing:** Using system prompts to guide the LLM's behavior and set its role.\n",
    "\n",
    "**Why:** System prompts define the assistant's role, personality, and domain expertise. They ensure consistent, domain-appropriate responses.\n",
    "\n",
    "**What to expect:**\n",
    "- Set a system prompt that defines the assistant as an IT operations expert\n",
    "- Ask an IT operations question\n",
    "- See how the system prompt influences the response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with system prompt\n",
    "print(\"=\" * 60)\n",
    "print(\"Chat with System Prompt\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful IT operations assistant. You provide clear, concise answers about IT infrastructure and operations.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What should I check if a web server is not responding?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"\\nüìù Question: What should I check if a web server is not responding?\")\n",
    "print(f\"\\nü§ñ Answer (with IT operations context):\\n{answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** After running the code, you set a system prompt that defines the assistant as an IT operations expert. Notice how the response is tailored to IT operations - the system prompt influenced the LLM's behavior.\n",
    "\n",
    "**Key takeaway:** System prompts are powerful - they define the assistant's role and ensure consistent, domain-appropriate responses. Think of them as a job description for the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Multi-turn Conversations\n",
    "\n",
    "**What we're doing:** Maintaining context across multiple exchanges in a conversation.\n",
    "\n",
    "**Why:** Multi-turn conversations let the LLM remember previous messages, enabling natural conversation flow and building on previous context.\n",
    "\n",
    "**What to expect:**\n",
    "- Start a conversation with one question\n",
    "- Continue the conversation with a follow-up question\n",
    "- See how the LLM remembers the context from earlier messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation\n",
    "print(\"=\" * 60)\n",
    "print(\"Multi-turn Conversation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First turn\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm setting up a new database server. What should I consider?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer1 = response1.choices[0].message.content\n",
    "print(f\"\\nüìù Turn 1 - Question: I'm setting up a new database server. What should I consider?\")\n",
    "print(f\"\\nü§ñ Answer:\\n{answer1[:200]}...\\n\")\n",
    "\n",
    "# Second turn - add previous messages to maintain context\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": answer1\n",
    "})\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What about security specifically?\"\n",
    "})\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer2 = response2.choices[0].message.content\n",
    "print(f\"\\nüìù Turn 2 - Question: What about security specifically?\")\n",
    "print(f\"   (Note: The assistant knows we're talking about database servers)\\n\")\n",
    "print(f\"ü§ñ Answer:\\n{answer2[:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** After running the code, you had a multi-turn conversation. Notice how in Turn 2, the assistant knew you were asking about database server security - it remembered the context from Turn 1!\n",
    "\n",
    "**Key takeaway:** Multi-turn conversations maintain context by including previous messages. The LLM remembers what you talked about, enabling natural conversation flow. This is how agents maintain context across multiple interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Streaming Responses\n",
    "\n",
    "**What we're doing:** Receiving responses as they're generated, token by token.\n",
    "\n",
    "**Why:** Streaming provides faster perceived response time and real-time feedback, creating a better user experience.\n",
    "\n",
    "**What to expect:**\n",
    "- Enable streaming in the chat completion\n",
    "- See the response appear token by token as it's generated\n",
    "- Understand when to use streaming vs. non-streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming response\n",
    "print(\"=\" * 60)\n",
    "print(\"Streaming Response\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìù Question: Explain what RAG (Retrieval Augmented Generation) is.\\n\")\n",
    "print(\"ü§ñ Answer (streaming):\\n\")\n",
    "\n",
    "# Create streaming completion\n",
    "stream = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain what RAG (Retrieval Augmented Generation) is in 2-3 sentences.\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True  # Enable streaming\n",
    ")\n",
    "\n",
    "# Process stream chunk by chunk\n",
    "full_response = \"\"\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        print(content, end=\"\", flush=True)\n",
    "        full_response += content\n",
    "\n",
    "print(\"\\n\\n‚úÖ Streaming complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** After running the code, you saw the response appear token by token as it was generated, rather than waiting for the complete response. This provides immediate feedback and feels more interactive.\n",
    "\n",
    "**Key takeaway:** Streaming is great for long responses and interactive applications. It provides faster perceived response time - users see results immediately rather than waiting for the complete response. Use streaming when you want real-time feedback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Creating a Vector Store for RAG\n",
    "\n",
    "**What we're doing:** Creating a vector store and adding IT operations documentation.\n",
    "\n",
    "**Why:** Vector stores enable RAG - they store your documents as embeddings, allowing semantic search to find relevant context when answering questions.\n",
    "\n",
    "**What to expect:**\n",
    "- Create sample IT operations documentation\n",
    "- Upload documents to LlamaStack\n",
    "- Create a vector store with those documents\n",
    "- Wait for documents to be processed and indexed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample IT operations documentation\n",
    "print(\"=\" * 60)\n",
    "print(\"Creating Vector Store for RAG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample IT operations documentation\n",
    "it_docs = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"content\": \"To restart a web server, use: systemctl restart nginx. Check status with: systemctl status nginx.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"content\": \"High CPU usage troubleshooting: 1) Check top processes with 'top' or 'htop', 2) Identify CPU-intensive processes, 3) Check for runaway processes or infinite loops.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"content\": \"Database connection issues: Check firewall rules, verify credentials, ensure database service is running, check network connectivity with 'telnet hostname port'.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc4\",\n",
    "        \"content\": \"Disk space issues: Use 'df -h' to check disk usage, find large files with 'du -sh /*', clean logs with 'journalctl --vacuum-time=7d'.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc5\",\n",
    "        \"content\": \"Service monitoring: Use 'systemctl list-units --type=service' to list all services, 'systemctl is-active servicename' to check status, set up monitoring with Prometheus or Nagios.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\nüìö Sample IT Operations Documentation:\")\n",
    "for doc in it_docs:\n",
    "    print(f\"   - {doc['id']}: {doc['content'][:60]}...\")\n",
    "\n",
    "print(\"\\nüí° These documents will be stored in a vector store for retrieval.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Uploading Documents and Creating Vector Store\n",
    "\n",
    "**What we're doing:** Uploading documents to LlamaStack and creating a vector store.\n",
    "\n",
    "**Why:** Documents need to be uploaded and indexed before they can be searched. The vector store organizes documents for semantic search.\n",
    "\n",
    "**What to expect:**\n",
    "- Upload each document as a file to LlamaStack\n",
    "- Create a vector store containing all the files\n",
    "- Wait for documents to be processed and indexed\n",
    "- Verify the vector store is ready for search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** After running the code, you created sample IT operations documentation. These documents represent the kind of knowledge you'd store in a vector store - runbooks, troubleshooting guides, procedures.\n",
    "\n",
    "**Key takeaway:** Vector stores hold YOUR documentation. Instead of relying only on the LLM's training data, you can store your specific procedures, runbooks, and knowledge bases in a vector store for RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store using LlamaStack\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Creating Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "# Step 1: Create files from text content\n",
    "print(f\"\\nüìù Creating files from {len(it_docs)} documents...\")\n",
    "file_ids = []\n",
    "\n",
    "for i, doc in enumerate(it_docs, 1):\n",
    "    # Create a file-like object from the document content\n",
    "    file_content = BytesIO(doc[\"content\"].encode('utf-8'))\n",
    "    file_name = f\"doc_{i}.txt\"\n",
    "    \n",
    "    # Upload file to LlamaStack\n",
    "    # The API expects a tuple: (filename, file_content, content_type)\n",
    "    file_obj = (file_name, file_content, 'text/plain')\n",
    "    \n",
    "    uploaded_file = client.files.create(\n",
    "        file=file_obj,\n",
    "        purpose=\"assistants\"\n",
    "    )\n",
    "    file_ids.append(uploaded_file.id)\n",
    "    print(f\"   ‚úÖ Uploaded {file_name} (ID: {uploaded_file.id})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(file_ids)} files\")\n",
    "\n",
    "# Step 2: Create vector store with files\n",
    "print(f\"\\nüì¶ Creating vector store...\")\n",
    "vector_store = client.vector_stores.create(\n",
    "    name=\"it-operations-docs\",\n",
    "    file_ids=file_ids,\n",
    "    metadata={\"description\": \"IT operations documentation and troubleshooting guides\"}\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Vector store created!\")\n",
    "print(f\"   Name: {vector_store.name}\")\n",
    "print(f\"   ID: {vector_store.id}\")\n",
    "print(f\"   Files: {len(file_ids)}\")\n",
    "\n",
    "# Step 3: Wait for files to be processed (vector stores need time to index files)\n",
    "print(f\"\\n‚è≥ Waiting for files to be processed and indexed...\")\n",
    "import time\n",
    "\n",
    "max_wait = 30  # Maximum wait time in seconds\n",
    "wait_interval = 2  # Check every 2 seconds\n",
    "elapsed = 0\n",
    "\n",
    "while elapsed < max_wait:\n",
    "    # Check vector store status\n",
    "    vs_status = client.vector_stores.retrieve(vector_store.id)\n",
    "    \n",
    "    # Check if files are processed (status might be in file_counts or similar)\n",
    "    if hasattr(vs_status, 'file_counts'):\n",
    "        file_counts = vs_status.file_counts\n",
    "        if hasattr(file_counts, 'in_progress') and file_counts.in_progress == 0:\n",
    "            print(f\"   ‚úÖ All files processed!\")\n",
    "            break\n",
    "    elif hasattr(vs_status, 'status'):\n",
    "        if vs_status.status == 'completed':\n",
    "            print(f\"   ‚úÖ Vector store ready!\")\n",
    "            break\n",
    "    \n",
    "    # Check file status directly\n",
    "    vs_files = client.vector_stores.files.list(vector_store.id)\n",
    "    if hasattr(vs_files, 'data'):\n",
    "        processed = sum(1 for f in vs_files.data if hasattr(f, 'status') and f.status == 'completed')\n",
    "        if processed == len(file_ids):\n",
    "            print(f\"   ‚úÖ All {processed} files processed!\")\n",
    "            break\n",
    "    \n",
    "    print(f\"   ‚è≥ Waiting... ({elapsed}s/{max_wait}s)\", end='\\r')\n",
    "    time.sleep(wait_interval)\n",
    "    elapsed += wait_interval\n",
    "\n",
    "if elapsed >= max_wait:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Timeout waiting for processing. Files may still be indexing.\")\n",
    "    print(f\"   üí° You can proceed, but search results may be incomplete initially.\")\n",
    "\n",
    "print(f\"\\nüí° The vector store is ready for semantic search!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** After running the code, you created sample IT operations documentation. These documents represent the kind of knowledge you'd store in a vector store - runbooks, troubleshooting guides, procedures.\n",
    "\n",
    "**Key takeaway:** Vector stores hold YOUR documentation. Instead of relying only on the LLM's training data, you can store your specific procedures, runbooks, and knowledge bases in a vector store for RAG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Searching the Vector Store\n",
    "\n",
    "**What we're doing:** Searching the vector store for relevant documents using semantic search.\n",
    "\n",
    "**Why:** Semantic search finds documents by meaning, not just keywords. This is how RAG retrieves relevant context for answering questions.\n",
    "\n",
    "**What to expect:**\n",
    "- Search the vector store with a query\n",
    "- See relevant documents retrieved based on semantic similarity\n",
    "- Understand how semantic search works differently from keyword search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the vector store\n",
    "print(\"=\" * 60)\n",
    "print(\"Searching Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"How do I restart a web server?\"\n",
    "print(f\"\\nüîç Query: {query}\\n\")\n",
    "\n",
    "# Search the vector store using LlamaStack API\n",
    "search_results = client.vector_stores.search(\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    max_num_results=2\n",
    ")\n",
    "\n",
    "print(\"üìö Retrieved Documents (from vector store):\")\n",
    "print(f\"   Found {len(search_results.data)} results\\n\")\n",
    "\n",
    "if len(search_results.data) == 0:\n",
    "    print(\"   ‚ö†Ô∏è  No results found. This might mean:\")\n",
    "    print(\"      - Files are still being processed/indexed\")\n",
    "    print(\"      - Try waiting a few seconds and searching again\")\n",
    "    print(\"      - Or check if files were added correctly to the vector store\")\n",
    "    print(\"\\n   üí° For demonstration, we'll use the original documents:\")\n",
    "    # Fallback to original documents for demonstration\n",
    "    for i, doc in enumerate(it_docs[:2], 1):\n",
    "        if \"restart\" in doc[\"content\"].lower() or \"web server\" in doc[\"content\"].lower():\n",
    "            print(f\"\\n   {i}. {doc['id']}:\")\n",
    "            print(f\"      {doc['content']}\")\n",
    "else:\n",
    "    for i, result in enumerate(search_results.data, 1):\n",
    "        print(f\"   {i}. \", end=\"\")\n",
    "        # The result contains the document content and score\n",
    "        if hasattr(result, 'score'):\n",
    "            print(f\"Score: {result.score:.3f}\")\n",
    "        if hasattr(result, 'content') and result.content:\n",
    "            print(f\"      Content: {result.content[:150]}...\")\n",
    "        elif hasattr(result, 'text') and result.text:\n",
    "            print(f\"      Text: {result.text[:150]}...\")\n",
    "        elif hasattr(result, 'document') and result.document:\n",
    "            print(f\"      Document: {str(result.document)[:150]}...\")\n",
    "        else:\n",
    "            # Try to get any text-like attribute\n",
    "            result_str = str(result)\n",
    "            print(f\"      Result: {result_str[:150]}...\")\n",
    "        print()\n",
    "\n",
    "print(\"\\nüí° These documents were retrieved using semantic search (embeddings).\")\n",
    "print(\"   They will be used as context for the LLM.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** After running the code, you uploaded documents to LlamaStack and created a vector store. The documents are being processed and indexed - converted to embeddings that enable semantic search.\n",
    "\n",
    "**Key takeaway:** Vector stores need time to process documents. Once processed, documents are stored as embeddings, enabling semantic search (finding documents by meaning, not just keywords). This is what makes RAG possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the vector store\n",
    "print(\"=\" * 60)\n",
    "print(\"Searching Vector Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"How do I restart a web server?\"\n",
    "print(f\"\\nüîç Query: {query}\\n\")\n",
    "\n",
    "# Search the vector store using LlamaStack API\n",
    "search_results = client.vector_stores.search(\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    max_num_results=2\n",
    ")\n",
    "\n",
    "print(\"üìö Retrieved Documents (from vector store):\")\n",
    "print(f\"   Found {len(search_results.data)} results\\n\")\n",
    "\n",
    "if len(search_results.data) == 0:\n",
    "    print(\"   ‚ö†Ô∏è  No results found. This might mean:\")\n",
    "    print(\"      - Files are still being processed/indexed\")\n",
    "    print(\"      - Try waiting a few seconds and searching again\")\n",
    "    print(\"      - Or check if files were added correctly to the vector store\")\n",
    "    print(\"\\n   üí° For demonstration, we'll use the original documents:\")\n",
    "    # Fallback to original documents for demonstration\n",
    "    for i, doc in enumerate(it_docs[:2], 1):\n",
    "        if \"restart\" in doc[\"content\"].lower() or \"web server\" in doc[\"content\"].lower():\n",
    "            print(f\"\\n   {i}. {doc['id']}:\")\n",
    "            print(f\"      {doc['content']}\")\n",
    "else:\n",
    "    for i, result in enumerate(search_results.data, 1):\n",
    "        print(f\"   {i}. \", end=\"\")\n",
    "        # The result contains the document content and score\n",
    "        if hasattr(result, 'score'):\n",
    "            print(f\"Score: {result.score:.3f}\")\n",
    "        if hasattr(result, 'content') and result.content:\n",
    "            print(f\"      Content: {result.content[:150]}...\")\n",
    "        elif hasattr(result, 'text') and result.text:\n",
    "            print(f\"      Text: {result.text[:150]}...\")\n",
    "        elif hasattr(result, 'document') and result.document:\n",
    "            print(f\"      Document: {str(result.document)[:150]}...\")\n",
    "        else:\n",
    "            # Try to get any text-like attribute\n",
    "            result_str = str(result)\n",
    "            print(f\"      Result: {result_str[:150]}...\")\n",
    "        print()\n",
    "\n",
    "print(\"\\nüí° These documents were retrieved using semantic search (embeddings).\")\n",
    "print(\"   They will be used as context for the LLM.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** After running the code, you searched the vector store and retrieved relevant documents. Notice how the search found documents about restarting web servers even though the query might not match exact keywords - this is semantic search!\n",
    "\n",
    "**Key takeaway:** Semantic search finds documents by meaning, not just keywords. The query \"How do I restart a web server?\" found documents about restarting nginx, even if they don't contain those exact words. This is the power of embeddings and semantic search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Using RAG - Chat with Retrieved Context\n",
    "\n",
    "**What we're doing:** Using retrieved documents as context for the LLM. This is the \"Augmented Generation\" part of RAG.\n",
    "\n",
    "**Why:** RAG combines retrieval (finding relevant docs) with generation (using those docs to answer). The LLM uses YOUR documentation to answer questions, not just its training data.\n",
    "\n",
    "**What to expect:**\n",
    "- Search the vector store for relevant context\n",
    "- Build a prompt that includes the retrieved documents\n",
    "- Get a response that uses YOUR documentation to answer the question\n",
    "- See how RAG provides domain-specific answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG - Chat with retrieved context\n",
    "print(\"=\" * 60)\n",
    "print(\"RAG - Chat with Retrieved Context\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"How do I restart a web server?\"\n",
    "print(f\"\\nüìù Question: {query}\\n\")\n",
    "\n",
    "# Search the vector store for relevant context\n",
    "search_results = client.vector_stores.search(\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    max_num_results=2\n",
    ")\n",
    "\n",
    "# Build context from retrieved documents\n",
    "context_parts = []\n",
    "for i, result in enumerate(search_results.data, 1):\n",
    "    # Extract content from result\n",
    "    if hasattr(result, 'content') and result.content:\n",
    "        content = result.content\n",
    "    elif hasattr(result, 'text') and result.text:\n",
    "        content = result.text\n",
    "    else:\n",
    "        # Try to get content from file if available\n",
    "        content = f\"Document {i} (score: {result.score:.3f})\"\n",
    "    \n",
    "    context_parts.append(f\"Document {i}:\\n{content}\")\n",
    "\n",
    "context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "# Create prompt with context\n",
    "prompt = f\"\"\"Use the following IT operations documentation to answer the question.\n",
    "\n",
    "Documentation:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the documentation provided:\"\"\"\n",
    "\n",
    "print(f\"üìö Context Retrieved from Vector Store:\\n{context[:300]}...\\n\")\n",
    "\n",
    "# Get response with context\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful IT operations assistant. Answer questions based on the provided documentation.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"ü§ñ Answer (with RAG context):\\n{answer}\\n\")\n",
    "print(\"‚úÖ Notice how the answer uses the specific documentation retrieved from the vector store!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:** After running the code, you completed a full RAG cycle: searched the vector store, retrieved relevant documents, and used them as context for the LLM. Notice how the answer uses YOUR documentation (the specific commands from your docs) rather than just general knowledge!\n",
    "\n",
    "**Key takeaway:** This is RAG in action! The LLM didn't just use its training data - it used YOUR documentation to provide a specific, accurate answer. This is how you give agents access to your internal knowledge bases, runbooks, and procedures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. **Simple Chat** is the foundation - basic LLM interactions for Q&A and text generation\n",
    "2. **RAG (Retrieval Augmented Generation)** gives agents access to YOUR documentation - store docs, retrieve context, answer questions!\n",
    "3. **System prompts** guide the LLM's behavior - set the role, personality, and domain\n",
    "4. **Multi-turn conversations** maintain context - agents remember what you talked about\n",
    "5. **Streaming** provides real-time feedback - see responses as they're generated\n",
    "6. **Vector stores** enable RAG - store documents as embeddings for semantic search\n",
    "\n",
    "**The big picture:**\n",
    "- **Chat** for general Q&A - when you need general knowledge\n",
    "- **RAG** for domain-specific knowledge - when you need YOUR docs\n",
    "- **Combine both** - agents can answer general questions AND questions about your specific setup\n",
    "\n",
    "**For IT operations:**\n",
    "- Use **Chat** for general IT questions (\"What is a load balancer?\")\n",
    "- Use **RAG** for your specific procedures (\"How do we restart services in our infrastructure?\")\n",
    "- Store your runbooks, troubleshooting guides, and documentation in vector stores\n",
    "- Give agents access to your internal knowledge base\n",
    "\n",
    "**When to use each:**\n",
    "- **Chat:** General Q&A, text generation, basic reasoning\n",
    "- **RAG:** Domain-specific knowledge, private/internal docs, up-to-date information\n",
    "- **Together:** Agents that understand both general concepts and your specific infrastructure\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Next Steps\n",
    "\n",
    "**Ready for more?** In **Notebook 04**, we'll explore:\n",
    "- **MCP (Model Context Protocol)** - External tool integration (give agents access to APIs, databases, commands!)\n",
    "- **How to integrate tools** with agents (connect to your monitoring systems, ticketing systems, etc.)\n",
    "- **Building production-ready agents** that can both answer questions AND take actions\n",
    "\n",
    "**The fun part:** You'll learn how to give agents access to your IT infrastructure tools - monitoring APIs, service management, databases, anything!\n",
    "\n",
    "**Next notebook:** `04_mcp_tools.ipynb` - MCP Tools and External Integrations\n",
    "\n",
    "**Related concepts:**\n",
    "- Client-side tools (covered in Notebook 02)\n",
    "- Agent safety and evaluation (covered in Notebooks 05-06)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
