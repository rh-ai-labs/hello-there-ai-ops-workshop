{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Fine-tuning de Modelo com LoRA\n",
    "\n",
    "Este notebook demonstra como realizar fine-tuning de um modelo de linguagem (Qwen2.5-3B-Instruct) usando a técnica LoRA (Low-Rank Adaptation) para uma tarefa específica de extração de campos estruturados a partir de tickets de call center.\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Treinar o modelo para extrair automaticamente campos estruturados (categoria, subcategoria, grupo de atribuição, etc.) a partir de descrições de tickets de TI, retornando os dados em formato JSON.\n",
    "\n",
    "## Tecnologias Utilizadas\n",
    "\n",
    "- **Modelo Base**: Qwen/Qwen2.5-3B-Instruct (modelo de linguagem instrucional)\n",
    "- **Técnica de Fine-tuning**: LoRA (Low-Rank Adaptation) - permite treinar apenas uma pequena parte dos parâmetros\n",
    "- **Bibliotecas**: \n",
    "  - `transformers` - Hugging Face Transformers\n",
    "  - `peft` - Parameter-Efficient Fine-Tuning\n",
    "  - `trl` - Transformer Reinforcement Learning (SFTTrainer)\n",
    "  - `datasets` - Hugging Face Datasets\n",
    "\n",
    "## Estrutura do Notebook\n",
    "\n",
    "1. **Configuração e Carregamento**: Configuração inicial e carregamento do dataset\n",
    "2. **Preparação do Modelo**: Carregamento do tokenizer e modelo base\n",
    "3. **Configuração LoRA**: Aplicação da técnica LoRA para treinamento eficiente\n",
    "4. **Preparação dos Dados**: Formatação e tokenização do dataset\n",
    "5. **Treinamento**: Execução do fine-tuning\n",
    "6. **Salvamento**: Salvamento do adaptador LoRA treinado\n",
    "\n",
    "## Formato do Dataset\n",
    "\n",
    "O dataset deve estar em formato JSONL com o seguinte formato:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"Prompt com a descrição do ticket...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"{\\\"campo1\\\": \\\"valor1\\\", ...}\"}\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Configuração e Carregamento do Dataset\n",
    "\n",
    "Primeiro, vamos configurar os parâmetros principais e carregar o dataset de treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "DATA_FILE = \"finetune_multioutput_small.jsonl\"  # seu arquivo JSONL com formato de mensagens\n",
    "OUTPUT_DIR = \"./qwen2.5-lora\"\n",
    "\n",
    "# Carregar o dataset de treinamento\n",
    "# O dataset deve estar em formato JSONL com campo \"messages\"\n",
    "print(\"Carregando dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=DATA_FILE, split=\"train\")\n",
    "print(f\"Dataset carregado: {len(dataset)} exemplos\")\n",
    "print(f\"Estrutura do dataset: {dataset.features}\")\n",
    "\n",
    "# Verificar formato do primeiro exemplo para validação\n",
    "if len(dataset) > 0:\n",
    "    print(\"\\nPrimeiro exemplo do dataset:\")\n",
    "    print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Verificação de Autenticação\n",
    "\n",
    "Verifica se você está autenticado no Hugging Face Hub (necessário para baixar modelos privados ou fazer upload).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3. Carregamento do Tokenizer\n",
    "\n",
    "O tokenizer é responsável por converter texto em tokens (IDs numéricos) que o modelo pode processar. Configuramos o token de padding caso não exista.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "print(whoami())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 4. Carregamento do Modelo Base\n",
    "\n",
    "Carregamos o modelo Qwen2.5-3B-Instruct. O modelo detecta automaticamente se há GPU disponível e ajusta o tipo de dados (float16 para GPU, float32 para CPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Load tokenizer\n",
    "# -------------------------------\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, use_auth_token=True)\n",
    "\n",
    "# Configurar padding token se não existir\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 5. Configuração LoRA\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** é uma técnica de fine-tuning eficiente que:\n",
    "\n",
    "- Treina apenas uma pequena fração dos parâmetros do modelo (geralmente < 1%)\n",
    "- Adiciona matrizes de baixo rank às camadas de atenção\n",
    "- Reduz drasticamente o uso de memória e tempo de treinamento\n",
    "- Mantém a qualidade do modelo original\n",
    "\n",
    "**Parâmetros LoRA**:\n",
    "- `r=32`: Rank das matrizes LoRA (maior = mais parâmetros treináveis)\n",
    "- `lora_alpha=32`: Fator de escala para os pesos LoRA\n",
    "- `target_modules`: Módulos onde LoRA será aplicado (q_proj, v_proj são camadas de atenção)\n",
    "- `lora_dropout=0.05`: Taxa de dropout para regularização\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Load base model\n",
    "# -------------------------------\n",
    "print(\"Loading base model...\")\n",
    "# Usar float16 apenas se tiver GPU, caso contrário usar float32\n",
    "model_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"Loading model with dtype: {model_dtype}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    use_auth_token=True, \n",
    "    torch_dtype=model_dtype,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Model loaded on device: {next(model.parameters()).device}\")\n",
    "else:\n",
    "    print(\"Model loaded on CPU\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 6. Configuração dos Argumentos de Treinamento\n",
    "\n",
    "Configuramos os hiperparâmetros do treinamento:\n",
    "\n",
    "- **Batch size**: Tamanho do lote (ajustado para 1 com gradient accumulation)\n",
    "- **Learning rate**: Taxa de aprendizado (2e-4 é um bom valor inicial)\n",
    "- **Epochs**: Número de épocas de treinamento\n",
    "- **Gradient accumulation**: Acumula gradientes de múltiplos batches antes de atualizar pesos\n",
    "- **FP16**: Usa precisão de 16 bits se GPU disponível (economiza memória)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. LoRA configuration\n",
    "# -------------------------------\n",
    "# Para Qwen2.5, os módulos de atenção geralmente são: q_proj, k_proj, v_proj, o_proj\n",
    "# Vamos usar q_proj e v_proj para começar (mais eficiente)\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # módulos de atenção do Qwen2.5\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"Aplicando LoRA ao modelo...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Associar tokenizer ao modelo (necessário para algumas versões do TRL)\n",
    "if not hasattr(model, 'tokenizer') or model.tokenizer is None:\n",
    "    model.tokenizer = tokenizer\n",
    "    print(\"✓ Tokenizer associado ao modelo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 7. Validação do Formato do Dataset\n",
    "\n",
    "Verificamos se o dataset está no formato correto esperado pelo SFTTrainer (campo \"messages\" com formato de chat).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 8. Preparação e Tokenização do Dataset\n",
    "\n",
    "Esta é uma etapa crítica do processo:\n",
    "\n",
    "1. **Formatação**: Converte as mensagens do formato chat para texto formatado no padrão do Qwen (`<|im_start|>user`, `<|im_end|>`, etc.)\n",
    "\n",
    "2. **Tokenização**: Converte o texto em tokens (IDs numéricos) que o modelo pode processar\n",
    "   - **Importante**: O DataCollator precisa de dados já tokenizados, não texto bruto\n",
    "   - Usamos `truncation=True` e `max_length=1024` para limitar o tamanho\n",
    "   - `padding=False` porque o DataCollator fará padding dinâmico durante o treinamento\n",
    "\n",
    "3. **DataCollator**: Responsável por criar batches com padding adequado durante o treinamento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5. Training arguments\n",
    "# -------------------------------\n",
    "# Verificar se há GPU disponível\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "print(f\"Using FP16: {use_fp16} (GPU available: {torch.cuda.is_available()})\")\n",
    "\n",
    "# Usar TrainingArguments do transformers (compatível com SFTTrainer)\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=100,\n",
    "    fp16=use_fp16,  # Use fp16 apenas se tiver GPU\n",
    "    bf16=False,  # Use bf16 se tiver GPU moderna (A100, H100)\n",
    "    dataloader_num_workers=0,  # Evitar problemas de multiprocessing\n",
    "    remove_unused_columns=False,  # Manter todas as colunas do dataset\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 9. Treinamento\n",
    "\n",
    "Inicia o processo de fine-tuning. O treinamento pode levar algum tempo dependendo do tamanho do dataset e dos recursos disponíveis.\n",
    "\n",
    "**Monitoramento**:\n",
    "- Os logs aparecem a cada `logging_steps` (10 steps)\n",
    "- Checkpoints são salvos a cada `save_steps` (500 steps)\n",
    "- O progresso mostra loss, learning rate e outras métricas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 10. Salvamento do Modelo\n",
    "\n",
    "Salva o adaptador LoRA treinado e o tokenizer. O adaptador LoRA contém apenas os pesos treinados (muito menor que o modelo completo) e pode ser carregado junto com o modelo base para inferência.\n",
    "\n",
    "**Estrutura salva**:\n",
    "- `adapter_config.json`: Configuração do LoRA\n",
    "- `adapter_model.bin`: Pesos do adaptador LoRA\n",
    "- `tokenizer files`: Arquivos do tokenizer\n",
    "\n",
    "**Para usar o modelo treinado**:\n",
    "```python\n",
    "from peft import PeftModel\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model = PeftModel.from_pretrained(model, OUTPUT_DIR)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se o dataset tem o formato correto de mensagens\n",
    "# O SFTTrainer espera um campo \"messages\" com formato de chat\n",
    "sample = dataset[0]\n",
    "if \"messages\" in sample:\n",
    "    print(\"✓ Dataset tem formato de mensagens correto\")\n",
    "    print(f\"  Exemplo de mensagens: {sample['messages']}\")\n",
    "else:\n",
    "    print(\"✗ Dataset não tem formato de mensagens\")\n",
    "    print(f\"  Campos disponíveis: {sample.keys()}\")\n",
    "    raise ValueError(\"O dataset precisa ter o campo 'messages' no formato de chat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 6. Trainer (SFT)\n",
    "# -------------------------------\n",
    "# Esta versão do TRL usa uma API diferente - precisa usar processing_class para o tokenizer\n",
    "# e formatar as mensagens em texto antes\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def format_messages_manual(messages):\n",
    "    \"\"\"Formata mensagens manualmente no formato do Qwen\"\"\"\n",
    "    formatted = \"\"\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        content = msg[\"content\"]\n",
    "        if role == \"user\":\n",
    "            formatted += f\"<|im_start|>user\\n{content}<|im_end|>\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            formatted += f\"<|im_start|>assistant\\n{content}<|im_end|>\\n\"\n",
    "    return formatted\n",
    "\n",
    "# Função para formatar as mensagens do dataset em texto\n",
    "def format_messages(examples):\n",
    "    \"\"\"Converte mensagens do formato chat para texto formatado\"\"\"\n",
    "    texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        # Verificar se o tokenizer tem apply_chat_template\n",
    "        if hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template is not None:\n",
    "            try:\n",
    "                formatted_text = tokenizer.apply_chat_template(\n",
    "                    messages, \n",
    "                    tokenize=False, \n",
    "                    add_generation_prompt=False\n",
    "                )\n",
    "            except:\n",
    "                # Fallback: formatar manualmente\n",
    "                formatted_text = format_messages_manual(messages)\n",
    "        else:\n",
    "            # Formatar manualmente se não tiver chat_template\n",
    "            formatted_text = format_messages_manual(messages)\n",
    "        texts.append(formatted_text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Aplicar formatação ao dataset\n",
    "print(\"Formatando dataset...\")\n",
    "try:\n",
    "    dataset_formatted = dataset.map(\n",
    "        format_messages,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,  # Remove colunas originais\n",
    "    )\n",
    "    print(f\"✓ Dataset formatado. Exemplo (primeiros 200 chars): {str(dataset_formatted[0])[:200]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao formatar dataset: {e}\")\n",
    "    # Tentar sem remover colunas\n",
    "    dataset_formatted = dataset.map(\n",
    "        format_messages,\n",
    "        batched=True,\n",
    "    )\n",
    "    print(f\"✓ Dataset formatado (mantendo colunas originais)\")\n",
    "\n",
    "# Tokenizar o dataset - CRÍTICO: o DataCollator precisa de dados tokenizados, não texto bruto\n",
    "print(\"\\nTokenizando dataset...\")\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokeniza os textos do dataset\"\"\"\n",
    "    # Tokenizar com padding e truncation\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # O DataCollator fará o padding\n",
    "        max_length=1024,\n",
    "        return_tensors=None,  # Retornar como listas, não tensors\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "dataset_tokenized = dataset_formatted.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],  # Remover texto bruto, manter apenas tokens\n",
    "    desc=\"Tokenizando dataset\"\n",
    ")\n",
    "print(f\"✓ Dataset tokenizado. Exemplo de campos: {list(dataset_tokenized[0].keys())}\")\n",
    "\n",
    "# Criar data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Não é masked language modeling\n",
    "    pad_to_multiple_of=8,  # Otimização para GPUs modernas\n",
    ")\n",
    "\n",
    "# Criar o trainer com os parâmetros corretos para esta versão do TRL\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset_tokenized,  # Usar dataset tokenizado, não formatado\n",
    "    processing_class=tokenizer,  # Tokenizer via processing_class\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"✓ SFTTrainer criado com sucesso!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 7. Train!\n",
    "# -------------------------------\n",
    "print(\"Starting training...\")\n",
    "print(f\"Trainer type: {type(trainer)}\")\n",
    "print(f\"Trainer tem tokenizer: {hasattr(trainer, 'tokenizer')}\")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"Erro durante o treinamento: {e}\")\n",
    "    print(f\"Tipo do erro: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 8. Save LoRA adapter\n",
    "# -------------------------------\n",
    "print(\"Saving model...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
